"""
Multi-Agent Causal Scaffolding Module
å¤šæ™ºèƒ½ä½“å› æœè„šæ‰‹æ¶æ¨¡å—

This module implements a multi-agent system for causal graph generation:
- 3 parallel LLM agents generate causal graphs independently
- 1 critic agent reviews, merges, and refines the results

æ­¤æ¨¡å—å®ç°äº†ç”¨äºå› æœå›¾ç”Ÿæˆçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼š
- 3ä¸ªå¹¶è¡ŒLLMæ™ºèƒ½ä½“ç‹¬ç«‹ç”Ÿæˆå› æœå›¾
- 1ä¸ªæ‰¹åˆ¤è€…æ™ºèƒ½ä½“å®¡æŸ¥ã€èåˆå’Œç²¾ç‚¼ç»“æœ
"""

import json
import os
import time
from typing import Dict, List, Optional, Any
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv

# Import base scaffolder
from engine.scaffolder import LLMClient, CausalScaffolder


class MultiAgentScaffolder:
    """
    Multi-Agent Causal Scaffolder with parallel generation and critic fusion.
    å…·æœ‰å¹¶è¡Œç”Ÿæˆå’Œæ‰¹åˆ¤èåˆçš„å¤šæ™ºèƒ½ä½“å› æœè„šæ‰‹æ¶å™¨

    Architecture:
    1. Three generator agents run in parallel to produce diverse causal graphs
    2. One critic agent evaluates, merges, and refines the outputs

    æ¶æ„ï¼š
    1. ä¸‰ä¸ªç”Ÿæˆå™¨æ™ºèƒ½ä½“å¹¶è¡Œè¿è¡Œä»¥äº§ç”Ÿå¤šæ ·åŒ–çš„å› æœå›¾
    2. ä¸€ä¸ªæ‰¹åˆ¤è€…æ™ºèƒ½ä½“è¯„ä¼°ã€èåˆå’Œç²¾ç‚¼è¾“å‡º
    """

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        num_generators: int = 3,
        generator_temperature: float = 0.3,  # Slightly higher for diversity
        critic_temperature: float = 0.0,  # Deterministic for stability
        max_retries: int = 3,
        retry_delay: float = 2.0,
        experience_manager=None,  # æ–°å¢ï¼šGRPOç»éªŒç®¡ç†å™¨ / Added: GRPO experience manager
        rollouts_per_generator: int = 1,  # æ–°å¢ï¼šæ¯ä¸ªgeneratorç”Ÿæˆçš„rolloutæ•°é‡ / Added: Number of rollouts per generator (for GRPO training)
        use_separate_apis: bool = True  # æ–°å¢ï¼šæ˜¯å¦ä¸ºæ¯ä¸ªagentä½¿ç”¨ç‹¬ç«‹API / Added: Use separate API for each agent
    ):
        """
        Initialize multi-agent scaffolder.
        åˆå§‹åŒ–å¤šæ™ºèƒ½ä½“è„šæ‰‹æ¶å™¨

        Args:
            llm_client: Shared LLM client (used only if use_separate_apis=False)
                        å…±äº«çš„LLMå®¢æˆ·ç«¯ï¼ˆä»…åœ¨use_separate_apis=Falseæ—¶ä½¿ç”¨ï¼‰
            num_generators: Number of parallel generator agents (default: 3)
                           å¹¶è¡Œç”Ÿæˆå™¨æ™ºèƒ½ä½“çš„æ•°é‡ï¼ˆé»˜è®¤ï¼š3ï¼‰
            generator_temperature: Temperature for generator agents (for diversity)
                                  ç”Ÿæˆå™¨æ™ºèƒ½ä½“çš„æ¸©åº¦ï¼ˆç”¨äºå¤šæ ·æ€§ï¼‰
            critic_temperature: Temperature for critic agent (deterministic)
                               æ‰¹åˆ¤è€…æ™ºèƒ½ä½“çš„æ¸©åº¦ï¼ˆç¡®å®šæ€§ï¼‰
            max_retries: Maximum retry attempts
                        æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: Delay between retries
                        é‡è¯•ä¹‹é—´çš„å»¶è¿Ÿ
            experience_manager: GRPOExperienceManager instance for injecting learned experiences
                               GRPOç»éªŒç®¡ç†å™¨å®ä¾‹ï¼Œç”¨äºæ³¨å…¥å­¦åˆ°çš„ç»éªŒ
            rollouts_per_generator: Number of rollouts each generator produces (for GRPO training)
                                   æ¯ä¸ªç”Ÿæˆå™¨äº§ç”Ÿçš„rolloutæ•°é‡ï¼ˆç”¨äºGRPOè®­ç»ƒï¼Œé»˜è®¤1ï¼‰
            use_separate_apis: Use separate API for each generator and critic
                              ä¸ºæ¯ä¸ªç”Ÿæˆå™¨å’Œæ‰¹åˆ¤è€…ä½¿ç”¨ç‹¬ç«‹çš„API
        """
        self.num_generators = num_generators
        self.generator_temperature = generator_temperature
        self.critic_temperature = critic_temperature
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.use_separate_apis = use_separate_apis
        
        # GRPO Experience Manager / GRPOç»éªŒç®¡ç†å™¨
        self.experience_manager = experience_manager
        
        # GRPO: Number of rollouts per generator / GRPOï¼šæ¯ä¸ªç”Ÿæˆå™¨çš„rolloutæ•°é‡
        self.rollouts_per_generator = rollouts_per_generator

        # Initialize LLM clients for each agent / ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if use_separate_apis:
            self._init_separate_clients()
        else:
            # Use shared client / ä½¿ç”¨å…±äº«å®¢æˆ·ç«¯
            self.llm_client = llm_client or LLMClient()
            self.generator_clients = {i: self.llm_client for i in range(1, num_generators + 1)}
            self.critic_client = self.llm_client

        # Load prompts
        self.generator_prompt = self._load_prompt("prompts/scaffolding_prompt_v3.txt")
        self.critic_prompt = self._load_prompt("prompts/critic_fusion_prompt.txt")

        # Logs
        self.generation_log = []
        self.fusion_log = []

        print(f"ğŸ¤– Multi-Agent Scaffolder initialized:")
        print(f"   - {num_generators} parallel generators (T={generator_temperature})")
        if use_separate_apis:
            print(f"   - Using separate API for each generator âœ“")
        if rollouts_per_generator > 1:
            print(f"   - {rollouts_per_generator} rollouts per generator (GRPO mode)")
        print(f"   - 1 critic agent (T={critic_temperature})")
        if experience_manager:
            print(f"   - Training-Free GRPO enabled âœ“")
        print(f"ğŸ¤– å¤šæ™ºèƒ½ä½“è„šæ‰‹æ¶å™¨å·²åˆå§‹åŒ–ï¼š")
        print(f"   - {num_generators}ä¸ªå¹¶è¡Œç”Ÿæˆå™¨ï¼ˆæ¸©åº¦={generator_temperature}ï¼‰")
        if use_separate_apis:
            print(f"   - æ¯ä¸ªç”Ÿæˆå™¨ä½¿ç”¨ç‹¬ç«‹API âœ“")
        if rollouts_per_generator > 1:
            print(f"   - æ¯ä¸ªç”Ÿæˆå™¨{rollouts_per_generator}ä¸ªrolloutsï¼ˆGRPOæ¨¡å¼ï¼‰")
        print(f"   - 1ä¸ªæ‰¹åˆ¤è€…æ™ºèƒ½ä½“ï¼ˆæ¸©åº¦={critic_temperature}ï¼‰")
        if experience_manager:
            print(f"   - è®­ç»ƒè‡ªç”±GRPOå·²å¯ç”¨ âœ“")
    
    def _init_separate_clients(self) -> None:
        """
        Initialize separate LLM clients for each generator and critic.
        ä¸ºæ¯ä¸ªç”Ÿæˆå™¨å’Œæ‰¹åˆ¤è€…åˆå§‹åŒ–ç‹¬ç«‹çš„LLMå®¢æˆ·ç«¯
        """
        from engine.api_manager import APIKeyManager
        from engine.scaffolder import LLMClient
        
        try:
            api_manager = APIKeyManager()
            
            # Initialize generator clients / åˆå§‹åŒ–ç”Ÿæˆå™¨å®¢æˆ·ç«¯
            self.generator_clients = {}
            for i in range(1, self.num_generators + 1):
                role = f'generator_{i}'
                try:
                    api_key = api_manager.get_api_key(role)
                    # Create client with API key
                    client = LLMClient()
                    # Override API key
                    if hasattr(client, 'client'):
                        client.client.api_key = api_key
                    self.generator_clients[i] = client
                    print(f"   âœ“ Generator {i} API configured")
                except Exception as e:
                    print(f"   âš ï¸  Generator {i} API config failed: {e}, using default")
                    self.generator_clients[i] = LLMClient()
            
            # Initialize critic client / åˆå§‹åŒ–æ‰¹åˆ¤è€…å®¢æˆ·ç«¯
            try:
                critic_key = api_manager.get_api_key('critic')
                self.critic_client = LLMClient()
                if hasattr(self.critic_client, 'client'):
                    self.critic_client.client.api_key = critic_key
                print(f"   âœ“ Critic API configured")
            except Exception as e:
                print(f"   âš ï¸  Critic API config failed: {e}, using default")
                self.critic_client = LLMClient()
                
        except Exception as e:
            print(f"   âš ï¸  API Manager initialization failed: {e}")
            print(f"   Using default LLM client for all agents")
            # Fallback to shared client
            default_client = LLMClient()
            self.generator_clients = {i: default_client for i in range(1, self.num_generators + 1)}
            self.critic_client = default_client
    
    def _load_agent_experiences(self, agent_id: str) -> str:
        """
        Load agent's own experiences from its experience file.
        ä»agentè‡ªå·±çš„ç»éªŒæ–‡ä»¶åŠ è½½ç»éªŒ
        
        Args:
            agent_id: Agent identifier (e.g., 'generator_1', 'generator_2', 'critic')
            
        Returns:
            Formatted experiences string for prompt
        """
        import json
        from pathlib import Path
        
        # Get absolute path to experience file
        project_root = Path(__file__).parent.parent
        exp_file = project_root / "data" / "grpo_experiences" / f"{agent_id}_experiences.json"
        
        if not exp_file.exists():
            return "No prior experiences available."
        
        try:
            with open(exp_file, 'r', encoding='utf-8') as f:
                experiences = json.load(f)
            
            if not experiences:
                return "No prior experiences available."
            
            # Format experiences as numbered list
            experiences_str = "\n".join(
                f"{i}. {exp['content']}" for i, exp in enumerate(experiences, 1)
            )
            
            return experiences_str
            
        except Exception as e:
            print(f"  âš ï¸  Failed to load experiences for {agent_id}: {e}")
            return "No prior experiences available."

    def _load_prompt(self, path: str) -> str:
        """Load prompt template from file."""
        prompt_path = Path(path)
        
        # Try relative path first
        if prompt_path.exists():
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        # Try absolute path from project root
        project_root = Path(__file__).parent.parent
        absolute_path = project_root / prompt_path
        
        if absolute_path.exists():
            with open(absolute_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        # Fallback to default only for critic prompt
        if "critic" in path:
            return self._get_default_critic_prompt()
        
        # For generator prompt, raise error (must use file)
        raise FileNotFoundError(
            f"Generator prompt template not found at:\n"
            f"  - Relative path: {prompt_path}\n"
            f"  - Absolute path: {absolute_path}\n"
            f"Please ensure '{path}' exists in project root."
        )

    def _get_default_critic_prompt(self) -> str:
        """Get default critic fusion prompt."""
        return """**ROLE:**
You are a Meta-Critic for Causal Reasoning. You receive multiple causal graph proposals from different agents and your task is to:
1. Identify strengths and weaknesses in each proposal
2. Detect inconsistencies, errors, or missing elements
3. Merge the best ideas from all proposals into one coherent, correct solution
4. Ensure the final output is logically sound and complete

**INPUT:**
You will receive:
- The original problem
- Retrieved knowledge (formulas, rules)
- THREE causal graph proposals (JSON format) from different agents

**YOUR TASK:**
Analyze all three proposals critically and generate a SINGLE REFINED JSON that:
- Preserves correct elements from all proposals
- Fixes errors or inconsistencies
- Adds missing causal links if needed
- Ensures computational plan is complete and correct

---
**ORIGINAL PROBLEM:**
{problem_text}

**RETRIEVED KNOWLEDGE (from knowledge base):**
{retrieved_knowledge}

**PRIOR EXPERIENCES (learned from previous problems):**
{prior_experiences}

**PROPOSAL 1 (Agent 1):**
```json
{proposal_1}
```

**PROPOSAL 2 (Agent 2):**
```json
{proposal_2}
```

**PROPOSAL 3 (Agent 3):**
```json
{proposal_3}
```

---

**CRITICAL ANALYSIS PROTOCOL:**

1. **Constraint Adherence Check:**
   - Do all proposals correctly identify the problem constraints?
   - Are there any violations of stated conditions?

2. **Causal Graph Comparison:**
   - Which proposal has the most complete causal graph?
   - Are there missing causal links in any proposal?
   - Are there incorrect or redundant links?

3. **Computation Plan Evaluation:**
   - Which plan is most logically ordered?
   - Are all necessary steps included?
   - Are there any computational errors?

4. **Consistency Check:**
   - Do the knowns match across proposals?
   - Is the target_variable correctly identified?
   - Are variable names consistent?

5. **Fusion Strategy:**
   - Take the most accurate constraints_and_premises
   - Merge causal graphs to include all valid causal links
   - Create a refined computation_plan with correct ordering
   - Ensure all variables are properly defined

**OUTPUT:**
Generate a SINGLE refined JSON object following the same schema as the proposals. This should be the best possible synthesis of all three inputs.

**REFINED JSON OUTPUT:**
"""

    def generate_scaffold_parallel(
        self,
        problem_text: str,
        retrieved_knowledge: List[str]
    ) -> Optional[Dict[str, Any]]:
        """
        Generate causal scaffold using multi-agent parallel system.
        ä½¿ç”¨å¤šæ™ºèƒ½ä½“å¹¶è¡Œç³»ç»Ÿç”Ÿæˆå› æœè„šæ‰‹æ¶

        Process:
        1. Launch 3 generator agents in parallel (each loads its own experiences)
        2. Collect all proposals
        3. Send to critic for fusion (critic loads its own experiences)
        4. Return refined result

        æµç¨‹ï¼š
        1. å¹¶è¡Œå¯åŠ¨3ä¸ªç”Ÿæˆå™¨æ™ºèƒ½ä½“ï¼ˆå„è‡ªåŠ è½½è‡ªå·±çš„ç»éªŒï¼‰
        2. æ”¶é›†æ‰€æœ‰ææ¡ˆ
        3. å‘é€ç»™æ‰¹åˆ¤è€…è¿›è¡Œèåˆï¼ˆæ‰¹åˆ¤è€…åŠ è½½è‡ªå·±çš„ç»éªŒï¼‰
        4. è¿”å›ç²¾ç‚¼ç»“æœ

        Args:
            problem_text: Problem statement
            retrieved_knowledge: List of relevant formulas/rules (from RAG)

        Returns:
            Refined causal scaffold JSON
        """
        print("\n" + "="*80)
        print("ğŸš€ MULTI-AGENT CAUSAL SCAFFOLDING STARTED")
        print("ğŸš€ å¤šæ™ºèƒ½ä½“å› æœè„šæ‰‹æ¶ç”Ÿæˆå¼€å§‹")
        print("="*80 + "\n")

        # Step 1: Parallel generation by 3 agents
        print(f"ğŸ“Š Phase 1: Parallel Generation ({self.num_generators} agents)")
        print(f"ğŸ“Š é˜¶æ®µ1ï¼šå¹¶è¡Œç”Ÿæˆï¼ˆ{self.num_generators}ä¸ªæ™ºèƒ½ä½“ï¼‰")
        print("-" * 80)

        proposals = self._parallel_generate(problem_text, retrieved_knowledge)

        if len(proposals) == 0:
            print("\nâŒ No valid proposals generated by any agent.")
            print("âŒ æ²¡æœ‰æ™ºèƒ½ä½“ç”Ÿæˆæœ‰æ•ˆææ¡ˆ")
            return None

        print(f"\nâœ“ Generated {len(proposals)}/{self.num_generators} valid proposals")
        print(f"âœ“ ç”Ÿæˆäº† {len(proposals)}/{self.num_generators} ä¸ªæœ‰æ•ˆææ¡ˆ")

        # Step 2: Critic fusion
        print(f"\nğŸ“Š Phase 2: Critic Fusion & Refinement")
        print(f"ğŸ“Š é˜¶æ®µ2ï¼šæ‰¹åˆ¤è€…èåˆä¸ç²¾ç‚¼")
        print("-" * 80)

        refined_scaffold = self._critic_fusion(
            problem_text,
            retrieved_knowledge,
            proposals
        )

        if refined_scaffold:
            print("\n" + "="*80)
            print("âœ… MULTI-AGENT SCAFFOLDING COMPLETED")
            print("âœ… å¤šæ™ºèƒ½ä½“è„šæ‰‹æ¶ç”Ÿæˆå®Œæˆ")
            print("="*80 + "\n")
        else:
            print("\n" + "="*80)
            print("âŒ MULTI-AGENT SCAFFOLDING FAILED")
            print("âŒ å¤šæ™ºèƒ½ä½“è„šæ‰‹æ¶ç”Ÿæˆå¤±è´¥")
            print("="*80 + "\n")

        return refined_scaffold

    def generate_scaffold_for_grpo_training(
        self,
        problem_text: str,
        retrieved_knowledge: List[str]
    ) -> List[Dict[str, Any]]:
        """
        Generate scaffolds for GRPO training with multiple rollouts per generator.
        ä¸ºGRPOè®­ç»ƒç”Ÿæˆè„šæ‰‹æ¶ï¼Œæ¯ä¸ªç”Ÿæˆå™¨äº§ç”Ÿå¤šä¸ªrollouts
        
        Architecture (ç”¨æˆ·æ¶æ„):
        Question â†’ Generator 1 â†’ [R1.1, R1.2, R1.3] â†’ Critic fusion â†’ Scaffold 1
        Question â†’ Generator 2 â†’ [R2.1, R2.2, R2.3] â†’ Critic fusion â†’ Scaffold 2
        Question â†’ Generator 3 â†’ [R3.1, R3.2, R3.3] â†’ Critic fusion â†’ Scaffold 3
        
        Returns:
            List of 3 final scaffolds (one per generator after critic fusion)
            è¿”å›3ä¸ªæœ€ç»ˆè„šæ‰‹æ¶ï¼ˆæ¯ä¸ªç”Ÿæˆå™¨ç»è¿‡criticèåˆåä¸€ä¸ªï¼‰
        """
        print("\n" + "="*80)
        print("ğŸ“ GRPO TRAINING MODE: Multiple Rollouts Per Generator")
        print("ğŸ“ GRPOè®­ç»ƒæ¨¡å¼ï¼šæ¯ä¸ªç”Ÿæˆå™¨å¤šä¸ªRollouts")
        print("="*80)
        print(f"   - {self.num_generators} generators")
        print(f"   - {self.rollouts_per_generator} rollouts per generator")
        print(f"   - Total rollouts: {self.num_generators * self.rollouts_per_generator}")
        print(f"   - {self.num_generators}ä¸ªç”Ÿæˆå™¨")
        print(f"   - æ¯ä¸ªç”Ÿæˆå™¨{self.rollouts_per_generator}ä¸ªrollouts")
        print(f"   - æ€»rollouts: {self.num_generators * self.rollouts_per_generator}\n")
        
        knowledge_str = "\n".join(
            f"{i}. {rule}" for i, rule in enumerate(retrieved_knowledge, 1)
        )
        
        results = []

        # Optional: parallelize across generators if configured
        try:
            if getattr(self, 'parallel_generators', False) and self.num_generators > 1:
                def _process_generator(agent_id: int):
                    # Build rollouts for this generator (optionally in parallel)
                    rollouts_local = []
                    def _gen_one(idx: int):
                        sc = self._single_agent_generate(
                            agent_id=agent_id,
                            problem_text=problem_text,
                            knowledge_str=knowledge_str
                        )
                        if sc:
                            return {'agent_id': agent_id, 'rollout_id': idx, 'scaffold': sc}
                        return None
                    if getattr(self, 'parallel_rollouts', False) and self.rollouts_per_generator > 1:
                        with ThreadPoolExecutor(max_workers=self.rollouts_per_generator) as ex:
                            futs = {ex.submit(_gen_one, i): i for i in range(1, self.rollouts_per_generator + 1)}
                            for fut in as_completed(futs):
                                v = fut.result()
                                if v:
                                    rollouts_local.append(v)
                    else:
                        for i in range(1, self.rollouts_per_generator + 1):
                            v = _gen_one(i)
                            if v:
                                rollouts_local.append(v)
                    if not rollouts_local:
                        return None
                    fused = self._critic_fusion(
                        problem_text=problem_text,
                        retrieved_knowledge=retrieved_knowledge,
                        proposals=rollouts_local
                    )
                    if fused:
                        return {
                            'agent_id': agent_id,
                            'num_rollouts': len(rollouts_local),
                            'scaffold': fused,
                            'rollouts': rollouts_local
                        }
                    return None

                with ThreadPoolExecutor(max_workers=self.num_generators) as ex:
                    futs = {ex.submit(_process_generator, g): g for g in range(1, self.num_generators + 1)}
                    for fut in as_completed(futs):
                        res = fut.result()
                        if res:
                            results.append(res)
                # Return early if parallel branch was used
                print(f"\nParallel generators mode produced {len(results)} fused scaffolds")
                return results
        except Exception:
            # Fallback to serial loop below on any error
            pass
        
        # For each generator, generate multiple rollouts and fuse them
        # å¯¹æ¯ä¸ªç”Ÿæˆå™¨ï¼Œç”Ÿæˆå¤šä¸ªrolloutså¹¶èåˆ
        for agent_id in range(1, self.num_generators + 1):
            print(f"\n{'â”€'*80}")
            print(f"ğŸ¤– Generator {agent_id}: Generating {self.rollouts_per_generator} rollouts")
            print(f"ğŸ¤– ç”Ÿæˆå™¨ {agent_id}ï¼šç”Ÿæˆ {self.rollouts_per_generator} ä¸ªrollouts")
            print(f"{'â”€'*80}")
            
            # Step 1: Generate multiple rollouts for this generator
            # æ­¥éª¤1ï¼šä¸ºè¿™ä¸ªç”Ÿæˆå™¨ç”Ÿæˆå¤šä¸ªrollouts
            rollouts = []
            for rollout_idx in range(1, self.rollouts_per_generator + 1):
                print(f"\n  ğŸ“ Rollout {rollout_idx}/{self.rollouts_per_generator}...")
                
                scaffold = self._single_agent_generate(
                    agent_id=agent_id,
                    problem_text=problem_text,
                    knowledge_str=knowledge_str
                )
                
                if scaffold:
                    rollouts.append({
                        'agent_id': agent_id,
                        'rollout_id': rollout_idx,
                        'scaffold': scaffold
                    })
                    print(f"    âœ“ Rollout {rollout_idx} generated successfully")
                else:
                    print(f"    âœ— Rollout {rollout_idx} failed")
            
            print(f"\n  ğŸ“Š Generator {agent_id} produced {len(rollouts)}/{self.rollouts_per_generator} valid rollouts")
            
            # Step 2: Critic fuses this generator's rollouts
            # æ­¥éª¤2ï¼šCriticèåˆè¿™ä¸ªç”Ÿæˆå™¨çš„rollouts
            if len(rollouts) > 0:
                print(f"\n  ğŸ§  Critic fusing Generator {agent_id}'s rollouts...")
                print(f"  ğŸ§  Criticæ­£åœ¨èåˆç”Ÿæˆå™¨ {agent_id} çš„rollouts...")
                
                fused_scaffold = self._critic_fusion(
                    problem_text=problem_text,
                    retrieved_knowledge=retrieved_knowledge,
                    proposals=rollouts
                )
                
                if fused_scaffold:
                    results.append({
                        'agent_id': agent_id,
                        'num_rollouts': len(rollouts),
                        'scaffold': fused_scaffold,
                        'rollouts': rollouts  # Keep rollouts for analysis
                    })
                    print(f"    âœ… Generator {agent_id}: Fusion successful")
                else:
                    print(f"    âŒ Generator {agent_id}: Fusion failed")
            else:
                print(f"  âš  Generator {agent_id}: No valid rollouts, skipping fusion")
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š GRPO Training Rollout Summary")
        print(f"ğŸ“Š GRPOè®­ç»ƒRolloutæ€»ç»“")
        print(f"{'='*80}")
        print(f"âœ“ Successful fusions: {len(results)}/{self.num_generators}")
        print(f"âœ“ æˆåŠŸèåˆ: {len(results)}/{self.num_generators}")
        
        for result in results:
            print(f"  - Generator {result['agent_id']}: {result['num_rollouts']} rollouts â†’ 1 fused scaffold")
        
        print(f"{'='*80}\n")
        
        return results

    def _parallel_generate(
        self,
        problem_text: str,
        retrieved_knowledge: List[str]
    ) -> List[Dict[str, Any]]:
        """
        Generate proposals in parallel using ThreadPoolExecutor.
        ä½¿ç”¨ThreadPoolExecutorå¹¶è¡Œç”Ÿæˆææ¡ˆ
        
        Note: Each generator loads its own experiences internally.
        æ³¨æ„ï¼šæ¯ä¸ªç”Ÿæˆå™¨åœ¨å†…éƒ¨åŠ è½½è‡ªå·±çš„ç»éªŒã€‚

        Returns:
            List of valid proposals
        """
        knowledge_str = "\n".join(
            f"{i}. {rule}" for i, rule in enumerate(retrieved_knowledge, 1)
        ) if retrieved_knowledge else "No additional knowledge provided."

        proposals = []

        # Use ThreadPoolExecutor for parallel execution
        with ThreadPoolExecutor(max_workers=self.num_generators) as executor:
            # Submit all generator tasks
            # Each agent will load its own experiences based on agent_id
            # æ¯ä¸ªagentå°†æ ¹æ®agent_idåŠ è½½è‡ªå·±çš„ç»éªŒ
            future_to_agent = {
                executor.submit(
                    self._single_agent_generate,
                    agent_id,
                    problem_text,
                    knowledge_str
                ): agent_id
                for agent_id in range(1, self.num_generators + 1)
            }

            # Collect results as they complete
            for future in as_completed(future_to_agent):
                agent_id = future_to_agent[future]
                try:
                    result = future.result()
                    if result:
                        proposals.append({
                            'agent_id': agent_id,
                            'scaffold': result,
                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
                        })
                        print(f"  âœ“ Agent {agent_id} completed successfully")
                        print(f"  âœ“ æ™ºèƒ½ä½“ {agent_id} æˆåŠŸå®Œæˆ")
                    else:
                        print(f"  âœ— Agent {agent_id} failed to generate valid scaffold")
                        print(f"  âœ— æ™ºèƒ½ä½“ {agent_id} ç”Ÿæˆæ— æ•ˆè„šæ‰‹æ¶")
                except Exception as e:
                    print(f"  âœ— Agent {agent_id} encountered error: {e}")
                    print(f"  âœ— æ™ºèƒ½ä½“ {agent_id} é‡åˆ°é”™è¯¯: {e}")

        return proposals

    def _single_agent_generate(
        self,
        agent_id: int,
        problem_text: str,
        knowledge_str: str
    ) -> Optional[Dict[str, Any]]:
        """
        Single agent generation with retry logic.
        å•ä¸ªæ™ºèƒ½ä½“ç”Ÿæˆï¼ˆå¸¦é‡è¯•é€»è¾‘ï¼‰

        Args:
            agent_id: Agent identifier (1, 2, or 3)
            problem_text: Problem statement
            knowledge_str: Formatted knowledge string (RAG)

        Returns:
            Generated scaffold or None
        """
        print(f"\nğŸ¤– Agent {agent_id} starting generation...")
        print(f"ğŸ¤– æ™ºèƒ½ä½“ {agent_id} å¼€å§‹ç”Ÿæˆ...")

        # Load this agent's own experiences from its experience file
        # ä»è¯¥agentè‡ªå·±çš„ç»éªŒæ–‡ä»¶åŠ è½½ç»éªŒ
        experiences_str = self._load_agent_experiences(f'generator_{agent_id}')

        # Construct prompt with both knowledge and experiences
        # ä½¿ç”¨çŸ¥è¯†å’Œç»éªŒæ„é€ æç¤º
        prompt = self.generator_prompt.format(
            retrieved_knowledge=knowledge_str,
            prior_experiences=experiences_str,
            problem_text=problem_text
        )

        # Retry loop
        for attempt in range(1, self.max_retries + 1):
            try:
                if attempt > 1:
                    print(f"  ğŸ”„ Agent {agent_id} retry {attempt}/{self.max_retries}")
                    time.sleep(self.retry_delay)

                # Call LLM with agent-specific client / ä½¿ç”¨è¯¥æ™ºèƒ½ä½“ç‰¹å®šçš„å®¢æˆ·ç«¯è°ƒç”¨LLM
                agent_client = self.generator_clients.get(agent_id, self.generator_clients[1])
                response = agent_client.complete(
                    prompt,
                    temperature=self.generator_temperature
                )

                # Extract JSON
                scaffold = self._extract_json(response)

                if scaffold:
                    # Validate
                    if self._validate_scaffold(scaffold):
                        print(f"  âœ“ Agent {agent_id} generated valid scaffold")

                        # Log generation
                        self.generation_log.append({
                            'agent_id': agent_id,
                            'attempt': attempt,
                            'success': True,
                            'target_variable': scaffold.get('target_variable')
                        })

                        return scaffold

                # Failed to parse or validate
                if attempt < self.max_retries:
                    print(f"  âš  Agent {agent_id} failed attempt {attempt}, retrying...")

            except Exception as e:
                print(f"  âœ— Agent {agent_id} error on attempt {attempt}: {e}")
                if attempt >= self.max_retries:
                    self.generation_log.append({
                        'agent_id': agent_id,
                        'attempt': attempt,
                        'success': False,
                        'error': str(e)
                    })

        return None

    def _critic_fusion(
        self,
        problem_text: str,
        retrieved_knowledge: List[str],
        proposals: List[Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """
        Critic agent fuses multiple proposals into refined output.
        æ‰¹åˆ¤è€…æ™ºèƒ½ä½“å°†å¤šä¸ªææ¡ˆèåˆä¸ºç²¾ç‚¼è¾“å‡º

        Args:
            problem_text: Original problem
            retrieved_knowledge: Knowledge base (from RAG)
            proposals: List of proposals from generator agents

        Returns:
            Refined scaffold or None
        """
        if len(proposals) == 0:
            print("âš  No proposals to fuse")
            return None

        # If only one proposal, validate and return it
        if len(proposals) == 1:
            print("â„¹ Only one proposal available, using it directly")
            return proposals[0]['scaffold']

        print(f"\nğŸ§  Critic analyzing {len(proposals)} proposals...")
        print(f"ğŸ§  æ‰¹åˆ¤è€…æ­£åœ¨åˆ†æ {len(proposals)} ä¸ªææ¡ˆ...")

        # Format knowledge
        knowledge_str = "\n".join(
            f"{i}. {rule}" for i, rule in enumerate(retrieved_knowledge, 1)
        ) if retrieved_knowledge else "No additional knowledge provided."
        
        # Load critic's own experiences from its experience file
        # ä»criticè‡ªå·±çš„ç»éªŒæ–‡ä»¶åŠ è½½ç»éªŒ
        experiences_str = self._load_agent_experiences('critic')

        # Prepare proposals for prompt (pad with empty if less than 3)
        proposal_jsons = []
        for i in range(3):
            if i < len(proposals):
                proposal_jsons.append(
                    json.dumps(proposals[i]['scaffold'], indent=2, ensure_ascii=False)
                )
            else:
                proposal_jsons.append("{}")  # Empty placeholder

        # Construct critic prompt with knowledge and experiences
        prompt = self.critic_prompt.format(
            problem_text=problem_text,
            retrieved_knowledge=knowledge_str,
            prior_experiences=experiences_str,
            proposal_1=proposal_jsons[0],
            proposal_2=proposal_jsons[1],
            proposal_3=proposal_jsons[2]
        )

        # Retry loop for critic
        for attempt in range(1, self.max_retries + 1):
            try:
                if attempt > 1:
                    print(f"  ğŸ”„ Critic retry {attempt}/{self.max_retries}")
                    time.sleep(self.retry_delay)

                print(f"  ğŸ“ Critic processing (attempt {attempt})...")
                print(f"  ğŸ“ æ‰¹åˆ¤è€…å¤„ç†ä¸­ï¼ˆç¬¬ {attempt} æ¬¡å°è¯•ï¼‰...")

                # Call LLM with critic-specific client / ä½¿ç”¨æ‰¹åˆ¤è€…ç‰¹å®šçš„å®¢æˆ·ç«¯è°ƒç”¨LLM
                response = self.critic_client.complete(
                    prompt,
                    temperature=self.critic_temperature
                )

                print(f"  âœ“ Critic response received ({len(response)} chars)")

                # Extract JSON
                refined = self._extract_json(response)

                if refined:
                    if self._validate_scaffold(refined):
                        print(f"  âœ… Critic produced valid refined scaffold")
                        print(f"  âœ… æ‰¹åˆ¤è€…ç”Ÿæˆäº†æœ‰æ•ˆçš„ç²¾ç‚¼è„šæ‰‹æ¶")

                        # Print critic analysis if available
                        critic_analysis = refined.get('critic_analysis')
                        if critic_analysis:
                            print("\n" + "="*80)
                            print("ğŸ” CRITIC ANALYSIS (æ‰¹åˆ¤è€…åˆ†æ)")
                            print("="*80)
                            print(critic_analysis)
                            print("="*80 + "\n")

                        # Log fusion
                        self.fusion_log.append({
                            'num_proposals': len(proposals),
                            'attempt': attempt,
                            'success': True,
                            'target_variable': refined.get('target_variable'),
                            'critic_analysis': critic_analysis
                        })

                        return refined

                # Failed to parse or validate
                if attempt < self.max_retries:
                    print(f"  âš  Critic failed attempt {attempt}, retrying...")

            except Exception as e:
                print(f"  âœ— Critic error on attempt {attempt}: {e}")
                if attempt >= self.max_retries:
                    self.fusion_log.append({
                        'num_proposals': len(proposals),
                        'attempt': attempt,
                        'success': False,
                        'error': str(e)
                    })

        print("  âŒ Critic failed to produce valid output, using best generator proposal")
        print("  âŒ æ‰¹åˆ¤è€…æœªèƒ½ç”Ÿæˆæœ‰æ•ˆè¾“å‡ºï¼Œä½¿ç”¨æœ€ä½³ç”Ÿæˆå™¨ææ¡ˆ")

        # Fallback: return first valid proposal
        return proposals[0]['scaffold'] if len(proposals) > 0 else None

    def _extract_json(self, text: str) -> Optional[Dict[str, Any]]:
        """Extract JSON from LLM response."""
        import re

        # Try to find JSON code block
        json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)

        if json_match:
            json_str = json_match.group(1)
        else:
            # Try to find raw JSON object
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                return None

        # Preprocess: Fix Python-style fractions to string format
        # é¢„å¤„ç†ï¼šå°†Pythoné£æ ¼çš„åˆ†æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼ˆä¿ç•™ç²¾åº¦ï¼‰
        # Convert patterns like `: 1/3,` to `: "1/3",` to keep precision
        json_str = re.sub(r':\s*(\d+)/(\d+)(\s*[,\}])', r': "\1/\2"\3', json_str)
        json_str = re.sub(r'\[\s*(\d+)/(\d+)(\s*[,\]])', r'["\1/\2"\3', json_str)
        json_str = re.sub(r',\s*(\d+)/(\d+)(\s*[,\]\}])', r', "\1/\2"\3', json_str)

        # Parse JSON
        try:
            result = json.loads(json_str)

            # Unwrap if needed
            if isinstance(result, dict) and "problem_analysis" in result:
                result = result["problem_analysis"]

            return result
        except json.JSONDecodeError as e:
            # Enhanced error logging
            if hasattr(self, 'verbose') and self.verbose:
                print(f"  âš  JSON parse error: {e}")
                print(f"  First 200 chars: {json_str[:200]}")
            return None

    def _validate_scaffold(self, scaffold: Dict[str, Any]) -> bool:
        """Validate scaffold structure (internal)."""
        required_keys = ["target_variable", "knowns", "causal_graph", "computation_plan"]

        if not all(key in scaffold for key in required_keys):
            return False

        # Validate causal_graph
        for link in scaffold.get("causal_graph", []):
            if not all(key in link for key in ["cause", "effect", "rule"]):
                return False

        # Validate computation_plan
        for step in scaffold.get("computation_plan", []):
            required_step_keys = ["id", "target", "inputs", "description"]
            if not all(key in step for key in required_step_keys):
                return False

        return True

    def validate_scaffold(self, scaffold: Dict[str, Any]) -> bool:
        """
        Validate scaffold structure (public API for compatibility).
        éªŒè¯è„šæ‰‹æ¶ç»“æ„ï¼ˆå…¬å…±APIï¼Œç”¨äºå…¼å®¹æ€§ï¼‰

        Args:
            scaffold: The scaffold dictionary to validate

        Returns:
            True if valid, False otherwise
        """
        return self._validate_scaffold(scaffold)

    def get_logs(self) -> Dict[str, Any]:
        """
        Get generation and fusion logs.
        è·å–ç”Ÿæˆå’Œèåˆæ—¥å¿—
        """
        return {
            'generation_log': self.generation_log,
            'fusion_log': self.fusion_log
        }

    def print_summary(self):
        """Print summary of multi-agent execution."""
        print("\n" + "="*80)
        print("ğŸ“Š MULTI-AGENT EXECUTION SUMMARY")
        print("ğŸ“Š å¤šæ™ºèƒ½ä½“æ‰§è¡Œæ‘˜è¦")
        print("="*80)

        # Generation stats
        total_generations = len(self.generation_log)
        successful_generations = sum(1 for log in self.generation_log if log.get('success'))

        print(f"\nğŸ¤– Generator Agents:")
        print(f"   Total attempts: {total_generations}")
        print(f"   Successful: {successful_generations}")
        print(f"   Failed: {total_generations - successful_generations}")

        print(f"\nğŸ¤– ç”Ÿæˆå™¨æ™ºèƒ½ä½“:")
        print(f"   æ€»å°è¯•æ¬¡æ•°: {total_generations}")
        print(f"   æˆåŠŸ: {successful_generations}")
        print(f"   å¤±è´¥: {total_generations - successful_generations}")

        # Fusion stats
        total_fusions = len(self.fusion_log)
        successful_fusions = sum(1 for log in self.fusion_log if log.get('success'))

        print(f"\nğŸ§  Critic Agent:")
        print(f"   Total fusion attempts: {total_fusions}")
        print(f"   Successful: {successful_fusions}")
        print(f"   Failed: {total_fusions - successful_fusions}")

        print(f"\nğŸ§  æ‰¹åˆ¤è€…æ™ºèƒ½ä½“:")
        print(f"   æ€»èåˆå°è¯•: {total_fusions}")
        print(f"   æˆåŠŸ: {successful_fusions}")
        print(f"   å¤±è´¥: {total_fusions - successful_fusions}")

        print("="*80 + "\n")


# Example usage
if __name__ == "__main__":
    # Initialize multi-agent scaffolder
    ma_scaffolder = MultiAgentScaffolder(
        num_generators=3,
        generator_temperature=0.3,
        critic_temperature=0.0
    )

    # Test problem
    problem = """
    An object with a mass of 10 kg is initially at rest.
    A constant force of 50 Newtons is applied to it for 5 seconds.
    What is its final velocity?
    """

    # Mock retrieved knowledge
    knowledge = [
        "Newton's Second Law: Force equals mass times acceleration (F=ma).",
        "Kinematic Equation: Final velocity equals initial velocity plus acceleration multiplied by time (v_f = v_i + a*t)."
    ]

    # Generate scaffold using multi-agent system
    scaffold = ma_scaffolder.generate_scaffold_parallel(problem, knowledge)

    if scaffold:
        print("\n--- REFINED SCAFFOLD ---")
        print(json.dumps(scaffold, indent=2, ensure_ascii=False))

        # Print summary
        ma_scaffolder.print_summary()
