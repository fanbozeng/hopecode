"""
Training-Free GRPO Trainer (ç”¨æˆ·æ¶æ„ç‰ˆæœ¬)
è®­ç»ƒè‡ªç”±GRPOè®­ç»ƒå™¨

Implements per-generator experience learning:
å®ç°æ¯ä¸ªç”Ÿæˆå™¨ç‹¬ç«‹çš„ç»éªŒå­¦ä¹ ï¼š

Question â†’ Generator 1 â†’ [R1.1, R1.2, R1.3] â†’ Critic fusion â†’ Scaffold 1 â†’ Answer 1 â†’ Reward 1
Question â†’ Generator 2 â†’ [R2.1, R2.2, R2.3] â†’ Critic fusion â†’ Scaffold 2 â†’ Answer 2 â†’ Reward 2  
Question â†’ Generator 3 â†’ [R3.1, R3.2, R3.3] â†’ Critic fusion â†’ Scaffold 3 â†’ Answer 3 â†’ Reward 3

Then update each generator's and critic's experience based on their performance.
ç„¶åæ ¹æ®å„è‡ªçš„è¡¨ç°æ›´æ–°æ¯ä¸ªç”Ÿæˆå™¨å’Œæ‰¹åˆ¤è€…çš„ç»éªŒã€‚
"""

import json
import os
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
from datetime import datetime

# Import LLM client and computer
from engine.scaffolder import LLMClient
from engine.llm_computer import LLMComputer


class TrainingFreeGRPOTrainer:
    """
    Training-Free GRPO Trainer - Per-Generator Architecture.
    è®­ç»ƒè‡ªç”±GRPOè®­ç»ƒå™¨ - æ¯ç”Ÿæˆå™¨æ¶æ„
    
    Key features:
    - Each generator produces multiple rollouts (default: 3)
    - Critic fuses each generator's rollouts separately (not mixing all rollouts)
    - Get 3 final answers (one per generator)
    - Update each LLM's experience individually based on its own performance
    
    ä¸»è¦ç‰¹ç‚¹ï¼š
    - æ¯ä¸ªç”Ÿæˆå™¨äº§ç”Ÿå¤šä¸ªrolloutsï¼ˆé»˜è®¤ï¼š3ï¼‰
    - Criticåˆ†åˆ«èåˆæ¯ä¸ªç”Ÿæˆå™¨çš„rolloutsï¼ˆä¸æ··åˆæ‰€æœ‰rolloutsï¼‰
    - å¾—åˆ°3ä¸ªæœ€ç»ˆç­”æ¡ˆï¼ˆæ¯ä¸ªç”Ÿæˆå™¨ä¸€ä¸ªï¼‰
    - æ ¹æ®å„è‡ªçš„è¡¨ç°å•ç‹¬æ›´æ–°æ¯ä¸ªLLMçš„ç»éªŒ
    """
    
    def __init__(
        self,
        causal_engine,  # CausalReasoningEngine instance with MultiAgentScaffolder
        experience_manager,  # GRPOExperienceManager instance
        llm_client: Optional[LLMClient] = None,
        rollouts_per_generator: int = 3,  # Each generator produces 3 rollouts
        num_epochs: int = 3,
        verbose: bool = True
    ):
        """
        Initialize Training-Free GRPO Trainer.
        åˆå§‹åŒ–è®­ç»ƒè‡ªç”±GRPOè®­ç»ƒå™¨
        
        Args:
            causal_engine: CausalReasoningEngine with MultiAgentScaffolder
            experience_manager: GRPOExperienceManager
            llm_client: LLM for semantic advantage extraction
            rollouts_per_generator: Rollouts per generator (default: 3)
            num_epochs: Training epochs (default: 3)
            verbose: Print detailed info
        """
        self.engine = causal_engine
        self.experience_manager = experience_manager
        self.llm_client = llm_client or LLMClient()
        self.llm_computer = LLMComputer(verbose=False)  # For executing scaffolds
        self.rollouts_per_generator = rollouts_per_generator
        self.num_epochs = num_epochs
        self.verbose = verbose
        
        # Load answer comparison prompt for accurate evaluation
        # åŠ è½½ç­”æ¡ˆæ¯”è¾ƒæç¤ºè¯ä»¥å®ç°å‡†ç¡®è¯„ä¼°
        self.answer_comparison_prompt = self._load_answer_comparison_prompt()
        
        # Configure scaffolder for GRPO training
        # é…ç½®scaffolderç”¨äºGRPOè®­ç»ƒ
        if hasattr(self.engine, 'scaffolder'):
            # Ensure experience_manager is injected (failsafe mechanism)
            # ç¡®ä¿ç»éªŒç®¡ç†å™¨å·²æ³¨å…¥ï¼ˆä¿é™©æœºåˆ¶ï¼‰
            if not hasattr(self.engine.scaffolder, 'experience_manager') or \
               self.engine.scaffolder.experience_manager is None:
                self.engine.scaffolder.experience_manager = self.experience_manager
                self._print("âœ“ Experience manager auto-injected to scaffolder")
                self._print("âœ“ ç»éªŒç®¡ç†å™¨å·²è‡ªåŠ¨æ³¨å…¥åˆ°scaffolder")
            
            self.engine.scaffolder.rollouts_per_generator = rollouts_per_generator
            self._print(f"âœ“ Configured scaffolder: {rollouts_per_generator} rollouts per generator")
        
        # Training log
        self.training_log = []
        
        # Load prompts
        self._load_prompts()
        
        self._print("ğŸš€ Training-Free GRPO Trainer initialized")
        self._print(f"   - Rollouts per generator: {rollouts_per_generator}")
        self._print(f"   - Epochs: {num_epochs}")
        self._print("ğŸš€ è®­ç»ƒè‡ªç”±GRPOè®­ç»ƒå™¨å·²åˆå§‹åŒ–")
        self._print(f"   - æ¯ä¸ªç”Ÿæˆå™¨çš„rolloutæ•°: {rollouts_per_generator}")
        self._print(f"   - Epochæ•°: {num_epochs}")
    
    def _print(self, message: str):
        """Print if verbose mode is enabled."""
        if self.verbose:
            print(message)
    
    def _load_prompts(self):
        """Load prompts for experience extraction."""
        # Simplified prompts for generator-specific experience extraction
        self.generator_advantage_prompt = """
You are analyzing the rollouts from Generator {generator_id} for a single problem.

**Problem:**
{problem}

**Ground Truth:**
{ground_truth}

**Generator {generator_id}'s Performance:**
- Generated {num_rollouts} rollouts
- After critic fusion: Final answer = {final_answer}
- Result: {result} (Correct/Incorrect)

**Current Experiences for Generator {generator_id}:**
{current_experiences}

Based on this generator's performance, what experience should be added/modified/deleted?

Focus on:
1. What mistakes did this generator make in its causal graph construction?
2. What patterns should this generator learn for similar problems?
3. Are existing experiences being applied correctly?

Provide recommendations in JSON format:
```json
{{
    "operations": [
        {{
            "action": "add",
            "content": "New experience (â‰¤32 words)",
            "category": "causal_graph|validation|problem_solving",
            "reason": "Why this helps Generator {generator_id}"
        }},
        {{
            "action": "modify",
            "experience_id": "G{generator_id}-001",
            "new_content": "Modified experience",
            "reason": "Improvement reason"
        }},
        {{
            "action": "delete",
            "experience_id": "G{generator_id}-003",
            "reason": "Why remove"
        }}
    ],
    "summary": "Overall analysis for Generator {generator_id}"
}}
```
"""

        self.critic_advantage_prompt = """
You are analyzing the Critic's fusion performance across multiple generators.

**Problem:**
{problem}

**Ground Truth:**
{ground_truth}

**Critic's Performance:**
{critic_performance}

**Current Critic Experiences:**
{current_experiences}

Analyze:
1. Did the critic successfully fuse rollouts from each generator?
2. What fusion strategies worked well?
3. How can the critic better identify high-quality proposals?

Provide recommendations in JSON format:
```json
{{
    "operations": [
        {{
            "action": "add",
            "content": "New fusion experience (â‰¤32 words)",
            "category": "fusion_strategy|validation|conflict_resolution",
            "reason": "Why this helps fusion"
        }}
    ],
    "summary": "Critic fusion improvement insights"
}}
```
"""
    
    def train(
        self,
        training_problems: List[Dict[str, Any]],
        save_checkpoint: bool = True
    ):
        """
        Train using Training-Free GRPO.
        ä½¿ç”¨è®­ç»ƒè‡ªç”±GRPOè®­ç»ƒ
        
        Args:
            training_problems: Problems with ground truth
            save_checkpoint: Save after each epoch
        """
        self._print("\n" + "="*80)
        self._print("ğŸ“ Training-Free GRPO Training (Per-Generator Architecture)")
        self._print("ğŸ“ è®­ç»ƒè‡ªç”±GRPOè®­ç»ƒï¼ˆæ¯ç”Ÿæˆå™¨æ¶æ„ï¼‰")
        self._print("="*80)
        self._print(f"\nğŸ“Š Training Set: {len(training_problems)} problems")
        self._print(f"ğŸ”„ Epochs: {self.num_epochs}")
        self._print(f"ğŸ‘¥ Rollouts per generator: {self.rollouts_per_generator}\n")
        
        for epoch in range(1, self.num_epochs + 1):
            self._print("\n" + "â”€"*80)
            self._print(f"ğŸ“š EPOCH {epoch}/{self.num_epochs}")
            self._print("â”€"*80 + "\n")
            
            epoch_start_time = datetime.now()
            
            for idx, problem_data in enumerate(training_problems, 1):
                self._print(f"\n{'='*60}")
                self._print(f"Problem {idx}/{len(training_problems)} (Epoch {epoch})")
                self._print('='*60)
                
                self._train_on_problem(problem_data, epoch, idx)
                
                self.experience_manager.training_stats['total_problems'] += 1
            
            self.experience_manager.training_stats['epochs_completed'] = epoch
            
            if save_checkpoint:
                self._save_checkpoint(epoch)
            
            epoch_duration = (datetime.now() - epoch_start_time).total_seconds()
            self._print(f"\nâœ… Epoch {epoch} completed in {epoch_duration:.1f}s")
            
            self.experience_manager.print_summary()
        
        self.experience_manager.save_all()
        
        self._print("\n" + "="*80)
        self._print("ğŸ‰ Training Complete!")
        self._print("="*80)
        
        self._print_training_summary()
    
    def _train_on_problem(
        self,
        problem_data: Dict[str, Any],
        epoch: int,
        problem_idx: int
    ):
        """
        Train on single problem using per-generator architecture.
        ä½¿ç”¨æ¯ç”Ÿæˆå™¨æ¶æ„åœ¨å•ä¸ªé—®é¢˜ä¸Šè®­ç»ƒ
        
        Architecture:
        Question â†’ 3 generators (each 3 rollouts) â†’ Critic fuses each â†’ 3 answers â†’ 3 rewards
        """
        problem_text = problem_data['problem']
        ground_truth = problem_data.get('answer', '')
        
        self._print(f"\nğŸ“– Problem: {problem_text[:100]}...")
        self._print(f"âœ“ Ground Truth: {ground_truth}")
        
        # Step 1: Generate rollouts using GRPO method
        # æ­¥éª¤1ï¼šä½¿ç”¨GRPOæ–¹æ³•ç”Ÿæˆrollouts
        self._print(f"\nğŸ”„ Generating rollouts ({self.rollouts_per_generator} per generator)...")
        
        try:
            # Use the GRPO training method
            # ä½¿ç”¨GRPOè®­ç»ƒæ–¹æ³•
            results = self.engine.scaffolder.generate_scaffold_for_grpo_training(
                problem_text=problem_text,
                retrieved_knowledge=[]  # Or get from retriever
            )
            
            if not results:
                self._print("âš ï¸ No valid results, skipping...")
                return
        
            self._print(f"\nâœ“ Got {len(results)} fused scaffolds (one per generator)")
            
            # Step 2: Execute and evaluate each result
            # æ­¥éª¤2ï¼šæ‰§è¡Œå¹¶è¯„ä¼°æ¯ä¸ªç»“æœ
            self._print(f"\nğŸ“Š Evaluating answers...")
            
            evaluations = []
            for result in results:
                agent_id = result['agent_id']
                scaffold = result['scaffold']
                
                # Execute scaffold using LLM Computer to get actual answer
                # ä½¿ç”¨LLMè®¡ç®—å™¨æ‰§è¡Œscaffoldè·å–å®é™…ç­”æ¡ˆ
                try:
                    computation_result = self.llm_computer.compute_from_scaffold(
                        causal_scaffold=scaffold,
                        problem_text=problem_text
                    )
                    
                    if computation_result['success']:
                        answer = computation_result['result']
                    else:
                        answer = None
                        self._print(f"  âš ï¸ Generator {agent_id}: Computation failed - {computation_result.get('error', 'Unknown error')}")
                except Exception as e:
                    answer = None
                    self._print(f"  âš ï¸ Generator {agent_id}: Execution error - {e}")
                
                # Evaluate with problem context for accurate comparison
                # ä½¿ç”¨é—®é¢˜ä¸Šä¸‹æ–‡è¿›è¡Œå‡†ç¡®æ¯”è¾ƒ
                is_correct = self._compare_answers(answer, ground_truth, problem_text) if answer is not None else False
                
                evaluations.append({
                    'agent_id': agent_id,
                    'scaffold': scaffold,
                    'answer': answer,
                    'is_correct': is_correct,
                    'num_rollouts': result['num_rollouts']
                })
                
                status = "âœ… Correct" if is_correct else "âŒ Incorrect"
                self._print(f"  Generator {agent_id}: {status} (Answer: {answer})")
            
            # Step 3: Extract and update experiences for each generator
            # æ­¥éª¤3ï¼šä¸ºæ¯ä¸ªç”Ÿæˆå™¨æå–å¹¶æ›´æ–°ç»éªŒ
            # Always extract experiences regardless of success/failure distribution
            # æ— è®ºæˆåŠŸ/å¤±è´¥åˆ†å¸ƒå¦‚ä½•ï¼Œæ€»æ˜¯æå–ç»éªŒ
            
            correct_count = sum(1 for e in evaluations if e['is_correct'])
            total_count = len(evaluations)
            
            if correct_count == 0:
                self._print(f"\nğŸ§  Extracting experiences (All failed: 0/{total_count})...")
            elif correct_count == total_count:
                self._print(f"\nğŸ§  Extracting experiences (All correct: {total_count}/{total_count})...")
            else:
                self._print(f"\nğŸ§  Extracting experiences (Mixed: {correct_count}/{total_count} correct)...")
            
            self._extract_and_update_experiences(
                problem_data,
                evaluations,
                epoch,
                problem_idx
            )
    
        except Exception as e:
            self._print(f"âŒ Error during training: {e}")
            import traceback
            traceback.print_exc()
    
    def _load_answer_comparison_prompt(self) -> str:
        """Load answer comparison prompt from file or use default."""
        from pathlib import Path
        prompt_path = Path("prompts/answer_comparison_prompt.txt")
        if prompt_path.exists():
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            # Fallback to default prompt
            return """You are a scientific answer verification expert. Determine if two answers are equivalent.

PROBLEM CONTEXT:
{problem_text}

EXPECTED ANSWER: {expected_answer}
PREDICTED ANSWER: {predicted_answer}

Respond with exactly: YES or NO
Then provide a brief reason.

YOUR RESPONSE:"""

    def _compare_answers(self, predicted: str, expected: str, problem_text: str = "") -> bool:
        """
        Compare expected and predicted answers using LLM with problem context.
        ä½¿ç”¨ LLM æ¯”è¾ƒé¢„æœŸç­”æ¡ˆå’Œé¢„æµ‹ç­”æ¡ˆï¼ˆå¸¦é—®é¢˜ä¸Šä¸‹æ–‡ï¼‰
        
        This method uses the same robust comparison logic as evaluate_framework.py:
        æ­¤æ–¹æ³•ä½¿ç”¨ä¸ evaluate_framework.py ç›¸åŒçš„é²æ£’æ¯”è¾ƒé€»è¾‘ï¼š
        - LLM-based comparison with problem context / åŸºäºLLMçš„æ¯”è¾ƒï¼ˆå¸¦é—®é¢˜ä¸Šä¸‹æ–‡ï¼‰
        - Fallback to rule-based comparison / é™çº§åˆ°åŸºäºè§„åˆ™çš„æ¯”è¾ƒ
        - Unit conversion and scientific notation support / å•ä½è½¬æ¢å’Œç§‘å­¦è®¡æ•°æ³•æ”¯æŒ
        
        Args:
            predicted: Predicted answer / é¢„æµ‹ç­”æ¡ˆ
            expected: Expected answer / é¢„æœŸç­”æ¡ˆ
            problem_text: The original problem text for context / é—®é¢˜åŸæ–‡ï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
        
        Returns:
            True if answers match, False otherwise / å¦‚æœç­”æ¡ˆåŒ¹é…è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        import re
        
        # False
        if predicted is None:
            return False

        # Use LLM to compare answers with problem context
        try:
            prompt = self.answer_comparison_prompt.format(
                problem_text=problem_text if problem_text else "No context provided",
                expected_answer=expected,
                predicted_answer=predicted
            )

            response = self.llm_client.complete(prompt, temperature=0.0)

            # Parse response - look for YES or NO
            response_upper = response.strip().upper()

            if response_upper.startswith("YES"):
                if self.verbose:
                    self._print(f"  âœ“ LLM Answer Comparison: YES")
                return True
            elif response_upper.startswith("NO"):
                if self.verbose:
                    self._print(f"  âœ— LLM Answer Comparison: NO")
                return False
            else:
                # If LLM response is unclear, fallback to string matching
                if self.verbose:
                    self._print(f"  âš  LLM response unclear, using fallback comparison")
                return self._fallback_compare(expected, predicted)
                
        except Exception as e:
            # If LLM fails, use fallback comparison
            if self.verbose:
                self._print(f"  âš  LLM comparison failed: {e}, using fallback")
            return self._fallback_compare(expected, predicted)
    
    def _fallback_compare(self, expected: str, predicted: Any) -> bool:
        """
        Fallback comparison method with enhanced unit and scientific notation handling.
        å¸¦å¢å¼ºå•ä½å’Œç§‘å­¦è®¡æ•°æ³•å¤„ç†çš„å¤‡ç”¨æ¯”è¾ƒæ–¹æ³•ã€‚
        
        This is the same robust fallback used in evaluate_framework.py.
        è¿™æ˜¯ evaluate_framework.py ä¸­ä½¿ç”¨çš„ç›¸åŒé²æ£’å¤‡ç”¨æ–¹æ³•ã€‚
        """
        import re
        
        expected_str = str(expected).strip().lower()
        predicted_str = str(predicted).strip().lower()

        # Remove LaTeX, brackets, quotes
        expected_str = re.sub(r'[\$\\{}\[\]\'\"]', '', expected_str)
        predicted_str = re.sub(r'[\$\\{}\[\]\'\"]', '', predicted_str)

        # Exact match (after basic cleanup)
        if expected_str == predicted_str:
            return True

        # Extract numerical values (handles scientific notation and units)
        def extract_number_and_unit(s):
            """Extract numerical value and unit from string, handling scientific notation"""
            s = s.strip()
            
            # Handle scientific notation: 2Ã—10^5, 2e5, 2*10^5
            scientific_patterns = [
                r'([\d.]+)\s*[Ã—x*]\s*10\s*\^\s*([+-]?\d+)\s*([a-zA-Z/Â°Â²Â³]+)?',  # 2Ã—10^5 or 2*10^5 with optional unit
                r'([\d.]+)\s*[eE]\s*([+-]?\d+)\s*([a-zA-Z/Â°Â²Â³]+)?',              # 2e5 or 2E5 with optional unit
            ]
            
            for pattern in scientific_patterns:
                match = re.search(pattern, s)
                if match:
                    base = float(match.group(1))
                    exponent = float(match.group(2))
                    unit = match.group(3) if len(match.groups()) >= 3 else None
                    value = base * (10 ** exponent)
                    return (value, unit)
            
            # Extract number and unit: "30", "30 m/s", "30m/s", "30.5 kg", "6 kW"
            num_unit_match = re.search(r'^([+-]?[\d.]+)\s*([a-zA-Z/Â°Â²Â³]+)?', s)
            if num_unit_match:
                value = float(num_unit_match.group(1))
                unit = num_unit_match.group(2)
                return (value, unit)
            
            return (None, None)
        
        def normalize_unit_value(value, unit):
            """Convert to base units (e.g., kW -> W, km -> m)"""
            if value is None:
                return None
            
            if unit is None:
                return value
            
            unit_lower = unit.lower()
            
            # Power conversions
            if unit_lower in ['kw', 'kilowatt']:
                return value * 1000  # kW to W
            elif unit_lower in ['mw', 'megawatt']:
                return value * 1000000  # MW to W
            
            # Energy conversions
            elif unit_lower in ['kj', 'kilojoule']:
                return value * 1000  # kJ to J
            elif unit_lower in ['mj', 'megajoule']:
                return value * 1000000  # MJ to J
            
            # Distance conversions
            elif unit_lower in ['km', 'kilometer']:
                return value * 1000  # km to m
            elif unit_lower in ['cm', 'centimeter']:
                return value / 100  # cm to m
            elif unit_lower in ['mm', 'millimeter']:
                return value / 1000  # mm to m
            
            # Mass conversions
            elif unit_lower in ['g', 'gram']:
                return value / 1000  # g to kg
            elif unit_lower in ['ton', 'tonne']:
                return value * 1000  # ton to kg
            
            # Time conversions
            elif unit_lower in ['min', 'minute']:
                return value * 60  # min to s
            elif unit_lower in ['h', 'hour', 'hr']:
                return value * 3600  # hour to s
            
            # Pressure conversions
            elif unit_lower in ['kpa', 'kilopascal']:
                return value * 1000  # kPa to Pa
            elif unit_lower in ['mpa', 'megapascal']:
                return value * 1000000  # MPa to Pa
            
            # If no conversion needed, return original value
            return value

        # Try numerical comparison with unit conversion
        try:
            expected_num, expected_unit = extract_number_and_unit(expected_str)
            predicted_num, predicted_unit = extract_number_and_unit(predicted_str)
            
            if expected_num is not None and predicted_num is not None:
                # Normalize units to base units (e.g., kW -> W, km -> m)
                expected_normalized = normalize_unit_value(expected_num, expected_unit)
                predicted_normalized = normalize_unit_value(predicted_num, predicted_unit)
                
                # Compare normalized values
                if expected_normalized is not None and predicted_normalized is not None:
                    # Use relative tolerance for large numbers, absolute for small
                    if abs(expected_normalized) > 1e-6:
                        relative_diff = abs(expected_normalized - predicted_normalized) / abs(expected_normalized)
                        if relative_diff < 1e-4:  # 0.01% relative tolerance
                            return True
                    
                    # Absolute tolerance
                    if abs(expected_normalized - predicted_normalized) < 1e-6:
                        return True
        except Exception as e:
            if self.verbose:
                self._print(f"    âš  Fallback comparison error: {e}")
            pass

        # Remove all spaces and try exact match again
        expected_clean = re.sub(r'\s+', '', expected_str)
        predicted_clean = re.sub(r'\s+', '', predicted_str)
        
        if expected_clean == predicted_clean:
            return True

        return False
    
    def _extract_and_update_experiences(
        self,
        problem_data: Dict[str, Any],
        evaluations: List[Dict[str, Any]],
        epoch: int,
        problem_idx: int
    ):
        """
        Extract experiences for each generator individually.
        ä¸ºæ¯ä¸ªç”Ÿæˆå™¨å•ç‹¬æå–ç»éªŒ
        
        Core principle: we update each generator's experience based on 
        its own performance, not mixing all together.
        
        æ ¸å¿ƒåŸåˆ™ï¼šæˆ‘ä»¬æ ¹æ®æ¯ä¸ªç”Ÿæˆå™¨è‡ªå·±çš„è¡¨ç°æ›´æ–°å…¶ç»éªŒï¼Œ
        è€Œä¸æ˜¯æ··åœ¨ä¸€èµ·ã€‚
        """
        problem_text = problem_data['problem']
        ground_truth = problem_data.get('answer', '')
        
        # Update experience for each generator
        # ä¸ºæ¯ä¸ªç”Ÿæˆå™¨æ›´æ–°ç»éªŒ
        for eval_result in evaluations:
            agent_id = eval_result['agent_id']
            is_correct = eval_result['is_correct']
            answer = eval_result['answer']
            num_rollouts = eval_result['num_rollouts']
            
            # Get current experiences for this generator
            # è·å–è¿™ä¸ªç”Ÿæˆå™¨çš„å½“å‰ç»éªŒ
            agent_type = f'generator_{agent_id}'
            current_exp = self.experience_manager.get_experiences_for_agent(
                agent_type,
                include_shared=False,
                format_as_prompt=False
            )
            
            current_exp_str = "\n".join([
                f"{exp.id}: {exp.content}" for exp in current_exp
            ]) if current_exp else "No experiences yet"
            
            # Construct prompt for this generator
            # ä¸ºè¿™ä¸ªç”Ÿæˆå™¨æ„é€ æç¤º
            result_str = "Correct âœ“" if is_correct else "Incorrect âœ—"
            
            prompt = self.generator_advantage_prompt.format(
                generator_id=agent_id,
                problem=problem_text,
                ground_truth=ground_truth,
                num_rollouts=num_rollouts,
                final_answer=answer,
                result=result_str,
                current_experiences=current_exp_str
            )
            
            # Extract experiences for this generator
            # ä¸ºè¿™ä¸ªç”Ÿæˆå™¨æå–ç»éªŒ
            try:
                self._print(f"\n  ğŸ“ Extracting experiences for Generator {agent_id}...")
                
                response = self.llm_client.complete(prompt, temperature=0.3)
                operations = self._parse_operations(response)
                
                if operations:
                    self._apply_operations(operations, agent_type)
                    self._print(f"    âœ“ Updated Generator {agent_id}'s experiences")
                else:
                    self._print(f"    â„¹ No updates for Generator {agent_id}")
                    
            except Exception as e:
                self._print(f"    âš ï¸ Error updating Generator {agent_id}: {e}")
        
        # Also update critic experience (based on fusion success rate)
        # åŒæ—¶æ›´æ–°criticç»éªŒï¼ˆåŸºäºèåˆæˆåŠŸç‡ï¼‰
        self._update_critic_experience(problem_data, evaluations)
    
    def _update_critic_experience(
        self,
        problem_data: Dict[str, Any],
        evaluations: List[Dict[str, Any]]
    ):
        """
        Update critic's experience based on fusion performance.
        æ ¹æ®èåˆè¡¨ç°æ›´æ–°criticç»éªŒ
        """
        self._print(f"\n  ğŸ§  Analyzing Critic's fusion performance...")
        
        # Build critic performance summary
        # æ„å»ºcriticè¡¨ç°æ€»ç»“
        critic_perf_lines = []
        for eval_result in evaluations:
            agent_id = eval_result['agent_id']
            is_correct = eval_result['is_correct']
            status = "Successful" if is_correct else "Failed"
            critic_perf_lines.append(
                f"- Generator {agent_id}: {eval_result['num_rollouts']} rollouts â†’ {status}"
            )
        
        critic_performance = "\n".join(critic_perf_lines)
        
        # Get current critic experiences
        # è·å–å½“å‰criticç»éªŒ
        current_exp = self.experience_manager.get_experiences_for_agent(
            'critic',
            include_shared=False,
            format_as_prompt=False
        )
        
        current_exp_str = "\n".join([
            f"{exp.id}: {exp.content}" for exp in current_exp
        ]) if current_exp else "No experiences yet"
        
        prompt = self.critic_advantage_prompt.format(
            problem=problem_data['problem'],
            ground_truth=problem_data.get('answer', ''),
            critic_performance=critic_performance,
            current_experiences=current_exp_str
        )
        
        try:
            response = self.llm_client.complete(prompt, temperature=0.3)
            operations = self._parse_operations(response)
            
            if operations:
                self._apply_operations(operations, 'critic')
                self._print(f"    âœ“ Updated Critic's experiences")
            else:
                self._print(f"    â„¹ No updates for Critic")
            
        except Exception as e:
            self._print(f"    âš ï¸ Error updating Critic: {e}")
    
    def _parse_operations(self, response: str) -> List[Dict[str, Any]]:
        """Parse operations from LLM response."""
        try:
            start = response.find('{')
            end = response.rfind('}') + 1
            
            if start >= 0 and end > start:
                json_str = response[start:end]
                data = json.loads(json_str)
                return data.get('operations', [])
        except Exception as e:
            self._print(f"âš ï¸ Error parsing operations: {e}")
        
        return []
    
    def _apply_operations(
        self,
        operations: List[Dict[str, Any]],
        agent_type: str
    ):
        """Apply experience operations."""
        for op in operations:
            action = op.get('action', '').lower()
            
            if action == 'add':
                content = op.get('content', '')
                category = op.get('category', 'general')
                
                if content:
                    self.experience_manager.add_experience(
                        agent_type=agent_type,
                        content=content,
                        category=category,
                        save=False
                    )
            
            elif action == 'modify':
                exp_id = op.get('experience_id', '')
                new_content = op.get('new_content', '')
                
                if exp_id and new_content:
                    self.experience_manager.modify_experience(
                        exp_id=exp_id,
                        new_content=new_content,
                        save=False
                    )
            
            elif action == 'delete':
                exp_id = op.get('experience_id', '')
                
                if exp_id:
                    self.experience_manager.delete_experience(
                        exp_id=exp_id,
                        save=False
                    )
        
        self.experience_manager.save_all()
    
    def _save_checkpoint(self, epoch: int):
        """Save training checkpoint."""
        checkpoint_dir = Path("checkpoints/grpo")
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        checkpoint_path = checkpoint_dir / f"epoch_{epoch}.json"
        
        self.experience_manager.export_for_deployment(str(checkpoint_path))
        
        self._print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
    
    def _print_training_summary(self):
        """Print training summary."""
        stats = self.experience_manager.get_statistics()
        
        print("\n" + "="*80)
        print("ğŸ“Š TRAINING SUMMARY")
        print("="*80)
        
        print(f"\nâœ… Problems processed: {stats['training_stats']['total_problems']}")
        print(f"âœ… Experiences added: {stats['training_stats']['total_experiences_added']}")
        print(f"âœï¸ Experiences modified: {stats['training_stats']['total_experiences_modified']}")
        print(f"ğŸ—‘ï¸ Experiences deleted: {stats['training_stats']['total_experiences_deleted']}")
        print(f"ğŸ”„ Epochs completed: {stats['training_stats']['epochs_completed']}")
        
        print(f"\nğŸ“š Final Experience Counts:")
        for agent_type, count in stats['experience_counts'].items():
            print(f"   - {agent_type}: {count} experiences")
        
        print("\n" + "="*80)


# Example usage
if __name__ == "__main__":
    print("Training-Free GRPO Trainer (Per-Generator Architecture)")
    print("è®­ç»ƒè‡ªç”±GRPOè®­ç»ƒå™¨ï¼ˆæ¯ç”Ÿæˆå™¨æ¶æ„ï¼‰")
    print("\nKey features:")
    print("- Each generator produces multiple rollouts")
    print("- Critic fuses each generator's rollouts separately")
    print("- Get 3 final answers (one per generator)")
    print("- Update each LLM's experience individually")

