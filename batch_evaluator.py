"""
Batch Parallel Evaluator for Causal Reasoning Framework
æ‰¹é‡å¹¶è¡Œè¯„ä¼°å™¨

This module provides batch processing capabilities similar to deep learning batch_size,
allowing multiple problems to be evaluated concurrently using asyncio or threading.

æœ¬æ¨¡å—æä¾›ç±»ä¼¼æ·±åº¦å­¦ä¹  batch_size çš„æ‰¹é‡å¤„ç†èƒ½åŠ›ï¼Œ
å…è®¸ä½¿ç”¨ asyncio æˆ–çº¿ç¨‹å¹¶å‘è¯„ä¼°å¤šä¸ªé—®é¢˜ã€‚

Usage:
    python batch_evaluator.py --dataset gsm8k --limit 20 --batch-size 5 --methods baselines
"""

import asyncio
import json
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from dataclasses import asdict

# Import existing evaluation framework components
# å¯¼å…¥ç°æœ‰çš„è¯„ä¼°æ¡†æ¶ç»„ä»¶ï¼ˆä¸ä¿®æ”¹åŸä»£ç ï¼‰
from evaluate_framework import (
    EvaluationMethod,
    EvaluationResult,
    DatasetLoader,
    FrameworkEvaluator
)


class BatchParallelEvaluator:
    """
    Batch parallel evaluator with concurrent processing support
    æ”¯æŒå¹¶å‘å¤„ç†çš„æ‰¹é‡å¹¶è¡Œè¯„ä¼°å™¨

    This class wraps the existing FrameworkEvaluator and adds batch processing
    capabilities without modifying the original code.

    æ­¤ç±»åŒ…è£…ç°æœ‰çš„ FrameworkEvaluator å¹¶æ·»åŠ æ‰¹å¤„ç†èƒ½åŠ›ï¼Œæ— éœ€ä¿®æ”¹åŸå§‹ä»£ç ã€‚
    """

    def __init__(self, batch_size: int = 1, max_workers: Optional[int] = None, verbose: bool = False, enable_visualization: bool = False):
        """
        Initialize batch evaluator
        åˆå§‹åŒ–æ‰¹é‡è¯„ä¼°å™¨

        Args:
            batch_size: Number of problems to process concurrently (ç±»ä¼¼æ·±åº¦å­¦ä¹ çš„ batch_size)
                       å¹¶å‘å¤„ç†çš„é—®é¢˜æ•°é‡ï¼ˆç±»ä¼¼æ·±åº¦å­¦ä¹ çš„ batch_sizeï¼‰
            max_workers: Maximum number of worker threads (é»˜è®¤ä¸º batch_size)
                        æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ä¸º batch_sizeï¼‰
            verbose: Verbose output
                    è¯¦ç»†è¾“å‡º
            enable_visualization: Enable causal graph visualization (NEW!)
                                 å¯ç”¨å› æœå›¾å¯è§†åŒ–ï¼ˆæ–°åŠŸèƒ½ï¼ï¼‰
        """
        self.batch_size = batch_size
        self.max_workers = max_workers or batch_size
        self.verbose = verbose
        self.enable_visualization = enable_visualization

        # Create a single FrameworkEvaluator instance
        # åˆ›å»ºå•ä¸ª FrameworkEvaluator å®ä¾‹
        self.evaluator = FrameworkEvaluator(verbose=verbose)

        print(f"\n{'='*80}")
        print(f"Batch Parallel Evaluator Initialized")
        print(f"æ‰¹é‡å¹¶è¡Œè¯„ä¼°å™¨å·²åˆå§‹åŒ–")
        print(f"  Batch Size: {self.batch_size}")
        print(f"  æ‰¹é‡å¤§å°: {self.batch_size}")
        print(f"  Max Workers: {self.max_workers}")
        print(f"  æœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
        print(f"{'='*80}\n")

    def evaluate_single_wrapper(
        self,
        problem: Dict[str, Any],
        method: EvaluationMethod,
        index: int,
        total: int
    ) -> EvaluationResult:
        """
        Wrapper for single evaluation with progress tracking
        å•ä¸ªè¯„ä¼°çš„åŒ…è£…å™¨ï¼Œå¸¦è¿›åº¦è·Ÿè¸ª

        Args:
            problem: Problem to evaluate
            method: Evaluation method
            index: Problem index (1-based)
            total: Total number of problems

        Returns:
            EvaluationResult
        """
        if self.verbose:
            print(f"[{index}/{total}] Starting: {problem['id']}")

        # Call the original evaluator's evaluate_single method
        # è°ƒç”¨åŸå§‹è¯„ä¼°å™¨çš„ evaluate_single æ–¹æ³•ï¼ˆä¸ä¿®æ”¹åŸä»£ç ï¼‰
        result = self.evaluator.evaluate_single(problem, method)

        # Generate visualization if enabled and scaffold is available
        # å¦‚æœå¯ç”¨å¯è§†åŒ–ä¸”è„šæ‰‹æ¶å¯ç”¨ï¼Œåˆ™ç”Ÿæˆå¯è§†åŒ–
        if self.enable_visualization and hasattr(result, 'causal_scaffold') and result.causal_scaffold:
            try:
                from engine.causal_graph_visualizer import visualize_causal_graph
                viz_dir = Path("batch_visualizations") / method.value
                viz_dir.mkdir(parents=True, exist_ok=True)
                viz_path = viz_dir / f"{problem['id']}.png"
                visualize_causal_graph(result.causal_scaffold, str(viz_path))
                if self.verbose:
                    print(f"  ğŸ“Š Visualization: {viz_path}")
            except Exception as e:
                if self.verbose:
                    print(f"  âš ï¸ Visualization failed: {e}")

        # Print result
        # æ‰“å°ç»“æœ
        status = "âœ“" if result.is_correct else ("âš " if result.error else "âœ—")
        print(f"[{index}/{total}] {status} {problem['id']} ({result.execution_time:.2f}s)")

        return result

    def evaluate_batch_threading(
        self,
        problems: List[Dict[str, Any]],
        method: EvaluationMethod
    ) -> List[EvaluationResult]:
        """
        Evaluate a batch of problems using threading
        ä½¿ç”¨çº¿ç¨‹è¯„ä¼°ä¸€æ‰¹é—®é¢˜

        This method processes multiple problems concurrently using ThreadPoolExecutor.
        æ­¤æ–¹æ³•ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç†å¤šä¸ªé—®é¢˜ã€‚

        Args:
            problems: List of problems to evaluate
            method: Evaluation method

        Returns:
            List of EvaluationResults
        """
        results = [None] * len(problems)  # Pre-allocate results list / é¢„åˆ†é…ç»“æœåˆ—è¡¨

        # Use ThreadPoolExecutor for concurrent execution
        # ä½¿ç”¨ ThreadPoolExecutor è¿›è¡Œå¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {
                executor.submit(
                    self.evaluate_single_wrapper,
                    problem,
                    method,
                    i + 1,
                    len(problems)
                ): i
                for i, problem in enumerate(problems)
            }

            # Collect results as they complete
            # æ”¶é›†å®Œæˆçš„ç»“æœ
            for future in as_completed(future_to_index):
                index = future_to_index[future]
                try:
                    result = future.result()
                    results[index] = result
                except Exception as e:
                    print(f"  Error in problem {index + 1}: {e}")
                    # Create error result
                    # åˆ›å»ºé”™è¯¯ç»“æœ
                    problem = problems[index]
                    results[index] = EvaluationResult(
                        problem_id=problem['id'],
                        method=method.value,
                        problem_text=problem['question'],
                        expected_answer=problem['answer'],
                        predicted_answer=None,
                        is_correct=False,
                        execution_time=0.0,
                        error=str(e)
                    )

        return results

    def evaluate_single_method(
        self,
        problems: List[Dict[str, Any]],
        method: EvaluationMethod,
        method_idx: int,
        total_methods: int
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Evaluate all problems using a single method (runs in parallel with other methods)
        ä½¿ç”¨å•ä¸ªæ–¹æ³•è¯„ä¼°æ‰€æœ‰é—®é¢˜ï¼ˆä¸å…¶ä»–æ–¹æ³•å¹¶è¡Œè¿è¡Œï¼‰

        Args:
            problems: List of problems to evaluate
            method: Evaluation method
            method_idx: Method index for progress display
            total_methods: Total number of methods being evaluated

        Returns:
            Tuple of (method_name, method_results_dict)
        """
        print(f"\n{'-'*80}")
        print(f"[Method {method_idx}/{total_methods}] Starting: {method.value}")
        print(f"[æ–¹æ³• {method_idx}/{total_methods}] å¼€å§‹: {method.value}")
        print(f"{'-'*80}")

        method_start_time = time.time()

        # Process problems in batches
        # åˆ†æ‰¹å¤„ç†é—®é¢˜
        all_method_results = []

        # Split problems into batches
        # å°†é—®é¢˜åˆ†æˆæ‰¹æ¬¡
        num_batches = (len(problems) + self.batch_size - 1) // self.batch_size

        for batch_idx in range(num_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(problems))
            batch_problems = problems[start_idx:end_idx]

            print(f"  [{method.value}] Batch {batch_idx + 1}/{num_batches} (Problems {start_idx + 1}-{end_idx})")
            print(f"  [{method.value}] æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}ï¼ˆé—®é¢˜ {start_idx + 1}-{end_idx}ï¼‰")

            batch_start_time = time.time()

            # Evaluate batch using threading
            # ä½¿ç”¨çº¿ç¨‹è¯„ä¼°æ‰¹æ¬¡
            batch_results = self.evaluate_batch_threading(batch_problems, method)
            all_method_results.extend(batch_results)

            batch_time = time.time() - batch_start_time
            print(f"  [{method.value}] Batch completed in {batch_time:.2f}s")
            print(f"  [{method.value}] æ‰¹æ¬¡å®Œæˆï¼Œè€—æ—¶ {batch_time:.2f}s")

        # Calculate statistics
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        method_time = time.time() - method_start_time
        correct_count = sum(1 for r in all_method_results if r.is_correct)
        error_count = sum(1 for r in all_method_results if r.error)
        accuracy = correct_count / len(problems) if problems else 0
        avg_time = method_time / len(problems) if problems else 0

        # Print summary
        # æ‰“å°æ‘˜è¦
        print(f"\n  [{method.value}] âœ“ Accuracy: {accuracy*100:.2f}% ({correct_count}/{len(problems)})")
        print(f"  [{method.value}] âœ“ å‡†ç¡®ç‡: {accuracy*100:.2f}% ({correct_count}/{len(problems)})")
        print(f"  [{method.value}] â± Total Time: {method_time:.2f}s (Avg: {avg_time:.2f}s per problem)")
        print(f"  [{method.value}] â± æ€»æ—¶é—´: {method_time:.2f}sï¼ˆå¹³å‡: {avg_time:.2f}s æ¯é¢˜ï¼‰")

        # Return method results
        # è¿”å›æ–¹æ³•ç»“æœ
        method_results = {
            'results': all_method_results,
            'statistics': {
                'total': len(problems),
                'correct': correct_count,
                'wrong': len(problems) - correct_count - error_count,
                'errors': error_count,
                'accuracy': accuracy,
                'total_time': method_time,
                'avg_time': avg_time
            }
        }

        return (method.value, method_results)

    def evaluate_dataset_batch(
        self,
        problems: List[Dict[str, Any]],
        methods: List[EvaluationMethod],
        dataset_name: str
    ) -> Dict[str, Any]:
        """
        Evaluate dataset with multiple methods using batch processing
        ä½¿ç”¨æ‰¹å¤„ç†è¯„ä¼°å¤šä¸ªæ–¹æ³•çš„æ•°æ®é›†

        **NEW: All methods run concurrently in parallel!**
        **æ–°åŠŸèƒ½ï¼šæ‰€æœ‰æ–¹æ³•å¹¶å‘å¹¶è¡Œè¿è¡Œï¼**

        This is the main entry point that processes problems in batches.
        Each method runs in its own thread, and within each method,
        problems are processed in batches with multi-threading.

        è¿™æ˜¯ä»¥æ‰¹æ¬¡å¤„ç†é—®é¢˜çš„ä¸»è¦å…¥å£ç‚¹ã€‚
        æ¯ä¸ªæ–¹æ³•åœ¨è‡ªå·±çš„çº¿ç¨‹ä¸­è¿è¡Œï¼Œåœ¨æ¯ä¸ªæ–¹æ³•å†…éƒ¨ï¼Œ
        é—®é¢˜ä»¥æ‰¹æ¬¡å¹¶å‘å¤„ç†ã€‚

        Args:
            problems: List of problems to evaluate
            methods: List of evaluation methods
            dataset_name: Name of the dataset

        Returns:
            Evaluation results dictionary
        """
        print(f"\n{'='*80}")
        print(f"Batch Evaluating {dataset_name} with {len(methods)} methods on {len(problems)} problems")
        print(f"æ‰¹é‡è¯„ä¼° {dataset_name}ï¼Œ{len(methods)} ä¸ªæ–¹æ³•ï¼Œ{len(problems)} ä¸ªé—®é¢˜")
        print(f"Batch Size: {self.batch_size}")
        print(f"æ‰¹é‡å¤§å°: {self.batch_size}")
        print(f"**ALL METHODS WILL RUN CONCURRENTLY**")
        print(f"**æ‰€æœ‰æ–¹æ³•å°†å¹¶å‘è¿è¡Œ**")
        print(f"{'='*80}\n")

        all_results = {}  # æ‰€æœ‰ç»“æœ
        overall_start_time = time.time()

        # Run all methods concurrently using ThreadPoolExecutor
        # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘è¿è¡Œæ‰€æœ‰æ–¹æ³•
        with ThreadPoolExecutor(max_workers=len(methods)) as method_executor:
            # Submit all method evaluation tasks
            # æäº¤æ‰€æœ‰æ–¹æ³•è¯„ä¼°ä»»åŠ¡
            future_to_method = {
                method_executor.submit(
                    self.evaluate_single_method,
                    problems,
                    method,
                    idx + 1,
                    len(methods)
                ): method
                for idx, method in enumerate(methods)
            }

            # Collect results as they complete
            # æ”¶é›†å®Œæˆçš„ç»“æœ
            for future in as_completed(future_to_method):
                method = future_to_method[future]
                try:
                    method_name, method_results = future.result()
                    all_results[method_name] = method_results
                    print(f"\nâœ“ Method '{method_name}' completed!")
                    print(f"âœ“ æ–¹æ³• '{method_name}' å®Œæˆï¼")
                except Exception as e:
                    print(f"\nâŒ Error in method {method.value}: {e}")
                    print(f"âŒ æ–¹æ³• {method.value} å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()

        overall_time = time.time() - overall_start_time
        print(f"\n{'='*80}")
        print(f"âœ“ All methods completed in {overall_time:.2f}s")
        print(f"âœ“ æ‰€æœ‰æ–¹æ³•åœ¨ {overall_time:.2f}s å†…å®Œæˆ")
        print(f"{'='*80}\n")

        # Return results in the same format as FrameworkEvaluator
        # ä»¥ä¸ FrameworkEvaluator ç›¸åŒçš„æ ¼å¼è¿”å›ç»“æœ
        return {
            'dataset_name': dataset_name,
            'total_problems': len(problems),
            'methods': all_results,
            'evaluation_time': datetime.now().isoformat(),
            'batch_config': {
                'batch_size': self.batch_size,
                'max_workers': self.max_workers,
                'concurrent_methods': True  # NEW: Indicate methods run concurrently
            }
        }

    def save_results(self, results: Dict[str, Any], output_path: str):
        """
        Save results to JSON (reuses FrameworkEvaluator's save logic)
        ä¿å­˜ç»“æœåˆ° JSONï¼ˆå¤ç”¨ FrameworkEvaluator çš„ä¿å­˜é€»è¾‘ï¼‰
        """
        # Convert results to serializable format
        # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_results = {
            'dataset_name': results['dataset_name'],
            'total_problems': results['total_problems'],
            'evaluation_time': results['evaluation_time'],
            'batch_config': results.get('batch_config', {}),
            'methods': {}
        }

        # Convert dataclass results to dicts
        # è½¬æ¢ dataclass ç»“æœä¸ºå­—å…¸
        for method_name, method_data in results['methods'].items():
            serializable_results['methods'][method_name] = {
                'statistics': method_data['statistics'],
                'results': [asdict(r) for r in method_data['results']]
            }

        # Save to file
        # ä¿å­˜åˆ°æ–‡ä»¶
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        print(f"\nâœ“ Results saved to: {output_file}")
        print(f"âœ“ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    def print_comparison_table(self, results: Dict[str, Any]):
        """
        Print comparison table (reuses FrameworkEvaluator's print logic)
        æ‰“å°å¯¹æ¯”è¡¨ï¼ˆå¤ç”¨ FrameworkEvaluator çš„æ‰“å°é€»è¾‘ï¼‰
        """
        print(f"\n{'='*80}")
        print(f"COMPARISON TABLE / å¯¹æ¯”è¡¨")
        print(f"{'='*80}")
        print(f"Dataset: {results['dataset_name']}")
        print(f"æ•°æ®é›†: {results['dataset_name']}")

        if 'batch_config' in results:
            print(f"Batch Size: {results['batch_config']['batch_size']}")
            print(f"æ‰¹é‡å¤§å°: {results['batch_config']['batch_size']}")

        print()

        # Print table header
        # æ‰“å°è¡¨å¤´
        print(f"{'Method':<30} {'Accuracy':<15} {'Avg Time':<15} {'Total Time':<15}")
        print(f"{'æ–¹æ³•':<30} {'å‡†ç¡®ç‡':<15} {'å¹³å‡æ—¶é—´':<15} {'æ€»æ—¶é—´':<15}")
        print(f"{'-'*80}")

        # Print results for each method
        # æ‰“å°æ¯ä¸ªæ–¹æ³•çš„ç»“æœ
        for method_name, method_data in results['methods'].items():
            stats = method_data['statistics']
            acc_str = f"{stats['accuracy']*100:.2f}%"
            avg_time_str = f"{stats['avg_time']:.2f}s"
            total_time_str = f"{stats['total_time']:.2f}s"
            print(f"{method_name:<30} {acc_str:<15} {avg_time_str:<15} {total_time_str:<15}")

        print(f"{'='*80}\n")


def main():
    """Main function for batch evaluation / æ‰¹é‡è¯„ä¼°çš„ä¸»å‡½æ•°"""
    import argparse

    # Parse command line arguments
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="Batch Parallel Evaluation for Causal Reasoning Framework\n"
                    "æ‰¹é‡å¹¶è¡Œè¯„ä¼°å› æœæ¨ç†æ¡†æ¶"
    )

    # Dataset selection / æ•°æ®é›†é€‰æ‹©
    parser.add_argument(
        '--dataset',
        type=str,
        choices=['gsm8k', 'math', 'mydata', 'omnimath', 'olympiad'],  # æ–°å¢ / Added
        default='omnimath',
        help='Dataset to evaluate / è¦è¯„ä¼°çš„æ•°æ®é›†'
    )

    # Problem limit / é—®é¢˜æ•°é‡é™åˆ¶
    parser.add_argument(
        '--limit',
        type=int,
        default=2,
        help='Limit number of problems / é™åˆ¶é—®é¢˜æ•°é‡'
    )

    # Batch size (NEW FEATURE!)
    # æ‰¹é‡å¤§å°ï¼ˆæ–°åŠŸèƒ½ï¼ï¼‰
    parser.add_argument(
        '--batch-size',
        type=int,
        default=3,
        help='Number of problems to process concurrently (like batch_size in deep learning) / '
             'å¹¶å‘å¤„ç†çš„é—®é¢˜æ•°é‡ï¼ˆç±»ä¼¼æ·±åº¦å­¦ä¹ ä¸­çš„ batch_sizeï¼‰'
    )

    # Max workers / æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    parser.add_argument(
        '--max-workers',
        type=int,
        default=None,
        help='Maximum number of worker threads (defaults to batch_size) / '
             'æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ä¸º batch_sizeï¼‰'
    )

    # Methods to evaluate / è¯„ä¼°æ–¹æ³•
    parser.add_argument(
        '--methods',
        type=str,
        nargs='+',
        choices=['baselines', 'ablations', 'all'],
        default=['baselines'],
        help='Evaluation methods / è¯„ä¼°æ–¹æ³•'
    )

    # Output directory / è¾“å‡ºç›®å½•
    parser.add_argument(
        '--output',
        type=str,
        default='evaluation_results',
        help='Output directory / è¾“å‡ºç›®å½•'
    )

    # Verbose output / è¯¦ç»†è¾“å‡º
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Verbose output / è¯¦ç»†è¾“å‡º'
    )

    # Enable visualization / å¯ç”¨å¯è§†åŒ– (NEW!)
    parser.add_argument(
        '--enable-viz',
        action='store_true',
        help='Enable causal graph visualization for each problem / ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆå› æœå›¾å¯è§†åŒ–'
    )

    args = parser.parse_args()

    # Determine which methods to run
    # ç¡®å®šè¦è¿è¡Œçš„æ–¹æ³•
    methods_to_run = []

    if 'baselines' in args.methods or 'all' in args.methods:
        methods_to_run.extend([
            EvaluationMethod.DIRECT_LLM,
            EvaluationMethod.ZERO_SHOT_COT,
            # EvaluationMethod.FEW_SHOT_COT,
            EvaluationMethod.FULL_FRAMEWORK
        ])

    if 'ablations' in args.methods or 'all' in args.methods:
        methods_to_run.extend([
            EvaluationMethod.NO_RETRIEVER,
            EvaluationMethod.NO_AI_RETRIEVER,
            EvaluationMethod.NO_SYMBOLIC_EXECUTION
        ])

    # Load dataset
    # åŠ è½½æ•°æ®é›†
    loader = DatasetLoader()

    if args.dataset == 'gsm8k':
        dataset_path = "dataset/GSM8K/grade_school_math/data/test.jsonl"
        problems = loader.load_gsm8k(dataset_path, limit=args.limit)
        dataset_name = "GSM8K"
    elif args.dataset == 'math':
        dataset_path = "dataset/Math/test-00000-of-00001.parquet.json"
        problems = loader.load_math(dataset_path, limit=args.limit)
        dataset_name = "MATH"
    elif args.dataset == 'mydata':
        dataset_path = "dataset/mydata/data/2024A.json"
        problems = loader.load_mydata(dataset_path, limit=args.limit)
        dataset_name = "MyData_2024A"
    elif args.dataset == 'omnimath':
        # Omni-MATHï¼ˆæ–°å¢ / NEW!ï¼‰
        dataset_path = "dataset/Omni-MATH/archive/main_test.jsonl"
        problems = loader.load_omnimath(dataset_path, limit=args.limit)
        dataset_name = "Omni-MATH"
    elif args.dataset == 'olympiad':
        # OlympiadBenchï¼ˆæ–°å¢ï¼Œå¤šæ¨¡æ€æ”¯æŒ / NEW! Multi-Modalï¼‰
        dataset_path = "dataset/OlympiadBench_Dataset/OlympiadBench_Dataset/data/OE_TO_physics_zh_CEE.json"
        problems = loader.load_olympiadbench(dataset_path, limit=args.limit)
        dataset_name = "OlympiadBench"
        print("\nğŸ’¡ Tip: Olympiad problems are very challenging!")
        print("ğŸ’¡ æç¤ºï¼šå¥¥æ—åŒ¹å…‹é—®é¢˜éå¸¸æœ‰æŒ‘æˆ˜æ€§ï¼\n")
    else:
        print(f"âŒ Unknown dataset: {args.dataset}")
        return 1

    # Check if dataset exists
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    if not Path(dataset_path).exists():
        print(f"âŒ Dataset not found: {dataset_path}")
        return 1

    # Create batch evaluator (NEW!)
    # åˆ›å»ºæ‰¹é‡è¯„ä¼°å™¨ï¼ˆæ–°åŠŸèƒ½ï¼ï¼‰
    evaluator = BatchParallelEvaluator(
        batch_size=args.batch_size,
        max_workers=args.max_workers,
        verbose=args.verbose,
        enable_visualization=args.enable_viz  # NEW: Enable visualization / æ–°å¢ï¼šå¯ç”¨å¯è§†åŒ–
    )

    # Run batch evaluation
    # è¿è¡Œæ‰¹é‡è¯„ä¼°
    results = evaluator.evaluate_dataset_batch(problems, methods_to_run, dataset_name)

    # Print comparison table
    # æ‰“å°å¯¹æ¯”è¡¨
    evaluator.print_comparison_table(results)

    # Save results
    # ä¿å­˜ç»“æœ
    output_path = f"{args.output}/{dataset_name}_batch_comparison.json"
    evaluator.save_results(results, output_path)

    print(f"\n{'='*80}")
    print(f"âœ“ Batch evaluation completed successfully!")
    print(f"âœ“ æ‰¹é‡è¯„ä¼°æˆåŠŸå®Œæˆï¼")
    print(f"{'='*80}\n")

    return 0


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš  Evaluation interrupted by user.")
        print("âš  ç”¨æˆ·ä¸­æ–­è¯„ä¼°ã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
