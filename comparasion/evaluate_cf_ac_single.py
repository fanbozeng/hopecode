"""
å•æ–‡ä»¶CF/ACè¯„ä¼°å·¥å…·
Single File CF/AC Evaluation Tool

åŠŸèƒ½ / Features:
- å¯¹æŒ‡å®šçš„å•ä¸ªç»“æœæ–‡ä»¶è®¡ç®—CFå’ŒACåˆ†æ•°
- Evaluate CF and AC scores for a single result file
- æ€»æ˜¯é‡æ–°è¯„ä¼°ï¼Œä¸æ£€æŸ¥ç¼“å­˜
- Always re-evaluate, no cache checking

ä½¿ç”¨æ–¹æ³• / Usage:
    # è¯„ä¼°æŒ‡å®šæ–‡ä»¶
    python comparasion/evaluate_cf_ac_single.py <æ–‡ä»¶è·¯å¾„>
    
    # ç¤ºä¾‹
    python comparasion/evaluate_cf_ac_single.py comparasion/results/zero_shot_cot/zero_shot_cot_olympiad_physics_20251119_004656.json
    
    # é™é»˜æ¨¡å¼
    python comparasion/evaluate_cf_ac_single.py <æ–‡ä»¶è·¯å¾„> --quiet
    
    # ç”Ÿæˆæ–°æ–‡ä»¶è€Œéè¦†ç›–åŸæ–‡ä»¶
    python comparasion/evaluate_cf_ac_single.py <æ–‡ä»¶è·¯å¾„> --output-mode separate
"""

import json
import sys
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥è¯„ä¼°æ¨¡å—
from causal_evaluation import (
    CausalInterventionEvaluator,
    AbductiveReasoningEvaluator
)
from engine.scaffolder import LLMClient


def evaluate_file(file_path: Path, output_mode: str = "append", verbose: bool = True):
    """
    è¯„ä¼°å•ä¸ªç»“æœæ–‡ä»¶ / Evaluate single result file
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„ / JSON file path
        output_mode: è¾“å‡ºæ¨¡å¼ / Output mode
            - "append": åœ¨åŸJSONä¸­æ·»åŠ cf_scoreå’Œac_score
            - "separate": ç”Ÿæˆæ–°çš„ *_evaluated.json æ–‡ä»¶
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ / Whether to show detailed info
    """
    
    if verbose:
        print("="*80)
        print("ğŸ“Š CF/AC å•æ–‡ä»¶è¯„ä¼° / Single File CF/AC Evaluation")
        print("="*80)
        print(f"ğŸ“ File: {file_path}")
        print(f"ğŸ“ æ–‡ä»¶: {file_path}")
        print(f"ğŸ’¾ Output mode: {output_mode}")
        print(f"ğŸ’¾ è¾“å‡ºæ¨¡å¼: {output_mode}")
        print("="*80 + "\n")
    
    # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’Œè¯„ä¼°å™¨
    if verbose:
        print("ğŸ”§ Initializing LLM client and evaluators...")
        print("ğŸ”§ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’Œè¯„ä¼°å™¨...")
    
    try:
        llm_client = LLMClient()
        if verbose:
            print("âœ… LLM client initialized")
            print("âœ… LLMå®¢æˆ·ç«¯å·²åˆå§‹åŒ–")
    except Exception as e:
        if verbose:
            print(f"âš ï¸  Warning: Failed to initialize LLM client: {e}")
            print(f"âš ï¸  è­¦å‘Š: LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            print("âš ï¸  Using default scores (0.5)")
            print("âš ï¸  å°†ä½¿ç”¨é»˜è®¤åˆ†æ•°(0.5)")
        llm_client = None
    
    causal_evaluator = CausalInterventionEvaluator(llm_client=llm_client, verbose=verbose)
    abductive_evaluator = AbductiveReasoningEvaluator(llm_client=llm_client, verbose=verbose)
    
    if verbose:
        print("âœ… Evaluators ready!")
        print("âœ… è¯„ä¼°å™¨å°±ç»ªï¼\n")
    
    # åŠ è½½JSON
    if verbose:
        print(f"ğŸ“‚ Loading file...")
        print(f"ğŸ“‚ åŠ è½½æ–‡ä»¶...")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æå–ç»“æœ
    results = data.get('results', [])
    
    if not results:
        print("âŒ No results found in file!")
        print("âŒ æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç»“æœï¼")
        return
    
    if verbose:
        print(f"ğŸ“Š Found {len(results)} problem(s)")
        print(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªé—®é¢˜\n")
        print("ğŸ”„ Evaluating...")
        print("ğŸ”„ è¯„ä¼°ä¸­...\n")
    
    # è¯„ä¼°æ¯ä¸ªé—®é¢˜
    cf_scores = []
    ac_scores = []
    
    for i, result in enumerate(results):
        if verbose:
            print(f"  [{i+1}/{len(results)}] Evaluating problem {i+1}...")
        
        # åˆå§‹åŒ–é»˜è®¤åˆ†æ•°
        cf_score = 0.0
        ac_score = 0.0
        
        # æå–å¿…è¦å­—æ®µ
        dag = result.get('causal_dag') or result.get('causal_scaffold')
        problem_text = result.get('problem', '') or result.get('problem_text', '')
        
        if not dag:
            # æ²¡æœ‰DAGï¼Œåˆ†æ•°ä¸º0
            cf_scores.append(0.0)
            ac_scores.append(0.0)
            result['cf_score'] = 0.0
            result['ac_score'] = 0.0
            result['cf_details'] = "No DAG available"
            result['ac_details'] = "No DAG available"
            if verbose:
                print(f"      No DAG - CF=0.000, AC=0.000")
            continue
        
        # è®¡ç®—CFåˆ†æ•°
        try:
            cf_score, cf_details = causal_evaluator.evaluate_intervention(
                dag=dag,
                problem_text=problem_text
            )
            cf_scores.append(cf_score)
            result['cf_score'] = cf_score
            result['cf_details'] = cf_details
        except Exception as e:
            if verbose:
                print(f"      âš ï¸  CF evaluation error - {e}")
            cf_scores.append(0.0)
            result['cf_score'] = 0.0
            result['cf_details'] = f"Error: {str(e)}"
            cf_score = 0.0
        
        # è®¡ç®—ACåˆ†æ•°
        try:
            ac_score, ac_details = abductive_evaluator.evaluate_abductive(
                dag=dag,
                problem_text=problem_text
            )
            ac_scores.append(ac_score)
            result['ac_score'] = ac_score
            result['ac_details'] = ac_details
        except Exception as e:
            if verbose:
                print(f"      âš ï¸  AC evaluation error - {e}")
            ac_scores.append(0.0)
            result['ac_score'] = 0.0
            result['ac_details'] = f"Error: {str(e)}"
            ac_score = 0.0
        
        if verbose:
            print(f"      Result: CF={cf_score:.3f}, AC={ac_score:.3f}")
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_cf = sum(cf_scores) / len(cf_scores) if cf_scores else 0.0
    avg_ac = sum(ac_scores) / len(ac_scores) if ac_scores else 0.0
    
    # æ›´æ–°statistics
    if 'statistics' not in data:
        data['statistics'] = {}
    
    data['statistics']['cf_score'] = avg_cf
    data['statistics']['ac_score'] = avg_ac
    data['statistics']['cf_scores_per_problem'] = cf_scores
    data['statistics']['ac_scores_per_problem'] = ac_scores
    data['statistics']['cf_ac_evaluation_time'] = datetime.now().isoformat()
    
    # ä¿å­˜æ–‡ä»¶
    if output_mode == "append":
        output_path = file_path
    else:
        output_path = file_path.parent / f"{file_path.stem}_evaluated.json"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    # è¾“å‡ºç»“æœ
    if verbose:
        print("\n" + "="*80)
        print("ğŸ“Š Evaluation Complete / è¯„ä¼°å®Œæˆ")
        print("="*80)
        print(f"âœ… Average CF Score: {avg_cf:.3f}")
        print(f"âœ… å¹³å‡CFåˆ†æ•°: {avg_cf:.3f}")
        print(f"âœ… Average AC Score: {avg_ac:.3f}")
        print(f"âœ… å¹³å‡ACåˆ†æ•°: {avg_ac:.3f}")
        print(f"ğŸ’¾ Output file: {output_path}")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print("="*80)
    else:
        print(f"âœ… CF={avg_cf:.3f}, AC={avg_ac:.3f} -> {output_path.name}")


def main():
    """å‘½ä»¤è¡Œå…¥å£ / CLI entry point"""
    parser = argparse.ArgumentParser(
        description="å•æ–‡ä»¶CF/ACè¯„ä¼° / Single File CF/AC Evaluation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples / ç¤ºä¾‹:
  # è¯„ä¼°æŒ‡å®šæ–‡ä»¶
  python comparasion/evaluate_cf_ac_single.py comparasion/results/zero_shot_cot/zero_shot_cot_olympiad_physics_20251119_004656.json
  
  # é™é»˜æ¨¡å¼
  python comparasion/evaluate_cf_ac_single.py comparasion/results/direct_llm/direct_llm_olympiad_physics_20251118_233335.json --quiet
  
  # ç”Ÿæˆæ–°æ–‡ä»¶
  python comparasion/evaluate_cf_ac_single.py comparasion/results/few_shot_cot/few_shot_cot_olympiad_physics_20251119_002347.json --output-mode separate
        """
    )
    
    parser.add_argument(
        'file',
        type=str,
        help='è¦è¯„ä¼°çš„ç»“æœæ–‡ä»¶è·¯å¾„ / Path to result file to evaluate'
    )
    
    parser.add_argument(
        '--output-mode',
        type=str,
        choices=['append', 'separate'],
        default='append',
        help='è¾“å‡ºæ¨¡å¼ / Output mode: append=è¦†ç›–åŸæ–‡ä»¶, separate=ç”Ÿæˆæ–°æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='é™é»˜æ¨¡å¼ / Quiet mode'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    file_path = Path(args.file)
    
    # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ä¸”æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
    if not file_path.exists():
        # å°è¯•ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾
        project_root = Path(__file__).resolve().parent.parent
        alternative_path = project_root / args.file
        
        if alternative_path.exists():
            file_path = alternative_path
            if not args.quiet:
                print(f"â„¹ï¸  Using absolute path: {file_path}")
        else:
            print(f"âŒ File not found: {args.file}")
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.file}")
            print(f"\nğŸ’¡ Tried paths:")
            print(f"   - {Path(args.file).absolute()}")
            print(f"   - {alternative_path}")
            sys.exit(1)
    
    # è¯„ä¼°æ–‡ä»¶
    try:
        evaluate_file(
            file_path=file_path,
            output_mode=args.output_mode,
            verbose=not args.quiet
        )
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Evaluation interrupted by user.")
        print("âš ï¸  è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        sys.exit(1)