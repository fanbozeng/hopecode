"""
æ‰¹é‡CF/ACè¯„ä¼°å·¥å…·
Batch CF/AC Evaluation Tool

åŠŸèƒ½ / Features:
1. æ‰«æ comparasion/results/ ç›®å½•ä¸‹çš„æ‰€æœ‰ç»“æœJSON
   Scan all result JSON files in comparasion/results/
2. å¯¹æ¯ä¸ªç»“æœè°ƒç”¨ causal_evaluation.py è®¡ç®—CFå’ŒAC
   Call causal_evaluation.py to compute CF and AC for each result
3. å°†CF/ACåˆ†æ•°å†™å›JSONæ–‡ä»¶
   Write CF/AC scores back to JSON files
4. æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼ˆå·²è¯„ä¼°çš„è·³è¿‡ï¼‰
   Support resume (skip already evaluated files)
5. è¿›åº¦æ¡æ˜¾ç¤º
   Progress bar display

ä½¿ç”¨æ–¹æ³• / Usage:
    # è¯„ä¼°æ‰€æœ‰ç»“æœæ–‡ä»¶
    python comparasion/evaluate_cf_ac_batch.py

    # æŒ‡å®šç»“æœç›®å½•
    python comparasion/evaluate_cf_ac_batch.py --results-dir comparasion/results

    # é‡æ–°è¯„ä¼°æ‰€æœ‰æ–‡ä»¶ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
    python comparasion/evaluate_cf_ac_batch.py --no-cache

    # åªè¯„ä¼°æŒ‡å®šæ–¹æ³•
    python comparasion/evaluate_cf_ac_batch.py --methods direct_llm cfgo

    # é™é»˜æ¨¡å¼
    python comparasion/evaluate_cf_ac_batch.py --quiet
"""

import json
import sys
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥è¯„ä¼°æ¨¡å—
from causal_evaluation import (
    CausalInterventionEvaluator,
    AbductiveReasoningEvaluator,
    RewardEvaluator
)


class CFACBatchEvaluator:
    """æ‰¹é‡CF/ACè¯„ä¼°å™¨ / Batch CF/AC Evaluator"""
    
    def __init__(
        self,
        results_dir: str = "comparasion/results",
        output_mode: str = "append",
        use_cache: bool = True,
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–æ‰¹é‡è¯„ä¼°å™¨ / Initialize batch evaluator
        
        Args:
            results_dir: ç»“æœç›®å½• / Results directory
            output_mode: è¾“å‡ºæ¨¡å¼ / Output mode
                - "append": åœ¨åŸJSONä¸­æ·»åŠ cf_scoreå’Œac_score
                - "separate": ç”Ÿæˆæ–°çš„ *_evaluated.json æ–‡ä»¶
            use_cache: æ˜¯å¦è·³è¿‡å·²è¯„ä¼°çš„æ–‡ä»¶ / Whether to skip already evaluated files
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¿›åº¦ / Whether to show detailed progress
        """
        self.results_dir = Path(results_dir)
        self.output_mode = output_mode
        self.use_cache = use_cache
        self.verbose = verbose
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨ / Initialize evaluators
        if self.verbose:
            print("ğŸ”§ Initializing evaluators...")
            print("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
        
        # CFè¯„ä¼°å™¨ï¼šCausal Intervention + Logic Quality + Graph Quality
        self.causal_evaluator = CausalInterventionEvaluator(verbose=False)
        
        # ACè¯„ä¼°å™¨ï¼šAbductive Reasoning
        self.abductive_evaluator = AbductiveReasoningEvaluator(verbose=False)
        
        # å¥–åŠ±è¯„ä¼°å™¨ï¼ˆç”¨äºLogic Qualityå’ŒGraph Qualityï¼‰
        self.reward_evaluator = RewardEvaluator(verbose=False)
        
        if self.verbose:
            print("âœ… Evaluators initialized successfully!")
            print("âœ… è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸï¼\n")
    
    def evaluate_all(self, method_filter: Optional[List[str]] = None):
        """
        è¯„ä¼°æ‰€æœ‰ç»“æœæ–‡ä»¶ / Evaluate all result files
        
        Args:
            method_filter: åªè¯„ä¼°æŒ‡å®šæ–¹æ³•ï¼ˆå¦‚ ['direct_llm', 'cfgo']ï¼‰
                          Only evaluate specified methods
        """
        print("="*80)
        print("ğŸ“Š æ‰¹é‡CF/ACè¯„ä¼° / Batch CF/AC Evaluation")
        print("="*80)
        print(f"ğŸ“ Results directory: {self.results_dir}")
        print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        print(f"ğŸ’¾ Output mode: {self.output_mode}")
        print(f"ğŸ’¾ è¾“å‡ºæ¨¡å¼: {self.output_mode}")
        print(f"ğŸ”„ Use cache: {self.use_cache}")
        print(f"ğŸ”„ ä½¿ç”¨ç¼“å­˜: {self.use_cache}")
        print("="*80 + "\n")
        
        # 1. æ‰«ææ‰€æœ‰JSONæ–‡ä»¶ / Scan all JSON files
        json_files = self._scan_result_files(method_filter)
        
        if not json_files:
            print("âŒ No result files found!")
            print("âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶ï¼")
            return
        
        print(f"ğŸ“‚ Found {len(json_files)} result file(s)")
        print(f"ğŸ“‚ æ‰¾åˆ° {len(json_files)} ä¸ªç»“æœæ–‡ä»¶\n")
        
        # 2. è¿‡æ»¤å·²è¯„ä¼°çš„ï¼ˆå¦‚æœuse_cache=Trueï¼‰/ Filter evaluated files
        if self.use_cache:
            json_files = self._filter_unevaluated(json_files)
            print(f"ğŸ” After filtering: {len(json_files)} file(s) to evaluate")
            print(f"ğŸ” è¿‡æ»¤å: {len(json_files)} ä¸ªæ–‡ä»¶éœ€è¦è¯„ä¼°\n")
        
        if not json_files:
            print("âœ… All files already evaluated!")
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²è¯„ä¼°ï¼")
            return
        
        # 3. æ‰¹é‡è¯„ä¼° / Batch evaluation
        success_count = 0
        error_count = 0
        
        for json_file in tqdm(json_files, desc="Evaluating", disable=not self.verbose):
            try:
                self._evaluate_single_file(json_file)
                success_count += 1
            except Exception as e:
                error_count += 1
                print(f"\nâŒ Error evaluating {json_file.name}: {e}")
                if self.verbose:
                    import traceback
                    traceback.print_exc()
        
        # 4. æ€»ç»“ / Summary
        print("\n" + "="*80)
        print("ğŸ“Š Evaluation Summary / è¯„ä¼°æ€»ç»“")
        print("="*80)
        print(f"âœ… Success: {success_count} / æˆåŠŸ: {success_count}")
        print(f"âŒ Errors: {error_count} / é”™è¯¯: {error_count}")
        print(f"ğŸ“ Total: {len(json_files)} / æ€»è®¡: {len(json_files)}")
        print("="*80 + "\n")
    
    def _scan_result_files(self, method_filter: Optional[List[str]] = None) -> List[Path]:
        """
        æ‰«ææ‰€æœ‰ç»“æœJSONæ–‡ä»¶ / Scan all result JSON files
        
        Args:
            method_filter: åªæ‰«ææŒ‡å®šæ–¹æ³• / Only scan specified methods
        
        Returns:
            List of JSON file paths / JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        json_files = []
        
        # éå†resultsç›®å½• / Traverse results directory
        for json_path in self.results_dir.rglob("*.json"):
            # æ’é™¤å·²è¯„ä¼°çš„æ–‡ä»¶ / Exclude already evaluated files
            if "_evaluated.json" in str(json_path):
                continue
            
            # å¦‚æœæŒ‡å®šäº†æ–¹æ³•è¿‡æ»¤ / If method filter is specified
            if method_filter:
                # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ…å«æŒ‡å®šæ–¹æ³• / Check if path contains specified method
                matched = False
                for method in method_filter:
                    if method in str(json_path):
                        matched = True
                        break
                if not matched:
                    continue
            
            json_files.append(json_path)
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº / Sort by modification time
        json_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
        
        return json_files
    
    def _filter_unevaluated(self, json_files: List[Path]) -> List[Path]:
        """
        è¿‡æ»¤æ‰å·²è¯„ä¼°çš„æ–‡ä»¶ / Filter out already evaluated files
        
        Args:
            json_files: æ–‡ä»¶åˆ—è¡¨ / File list
        
        Returns:
            æœªè¯„ä¼°çš„æ–‡ä»¶åˆ—è¡¨ / List of unevaluated files
        """
        unevaluated = []
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æ£€æŸ¥statisticsä¸­æ˜¯å¦å·²æœ‰cf_scoreå’Œac_score
                # Check if cf_score and ac_score exist in statistics
                stats = data.get('statistics', {})
                if 'cf_score' in stats and 'ac_score' in stats:
                    if self.verbose:
                        print(f"â­ï¸  Skipping (already evaluated): {json_file.name}")
                    continue
                
                unevaluated.append(json_file)
            except Exception as e:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œä¹ŸåŠ å…¥å¾…è¯„ä¼°åˆ—è¡¨ / If read fails, add to evaluation list
                unevaluated.append(json_file)
        
        return unevaluated
    
    def _evaluate_single_file(self, json_path: Path):
        """
        è¯„ä¼°å•ä¸ªç»“æœæ–‡ä»¶ / Evaluate single result file
        
        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„ / JSON file path
        """
        # 1. åŠ è½½JSON / Load JSON
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # 2. æå–æ‰€æœ‰é—®é¢˜çš„ç»“æœ / Extract all problem results
        results = data.get('results', [])
        
        if not results:
            if self.verbose:
                print(f"âš ï¸  No results found in {json_path.name}")
            return
        
        # 3. å¯¹æ¯ä¸ªé—®é¢˜è®¡ç®—CFå’ŒAC / Compute CF and AC for each problem
        cf_scores = []
        ac_scores = []
        
        for i, result in enumerate(results):
            # æ£€æŸ¥æ˜¯å¦æœ‰DAG / Check if DAG exists
            dag = result.get('causal_dag') or result.get('causal_scaffold')
            problem_text = result.get('problem', '') or result.get('problem_text', '')
            reasoning = result.get('reasoning', '') or result.get('reasoning_steps', '')
            
            if not dag:
                # å¦‚æœæ²¡æœ‰DAGï¼Œåˆ†æ•°ä¸º0 / If no DAG, score is 0
                cf_scores.append(0.0)
                ac_scores.append(0.0)
                result['cf_score'] = 0.0
                result['ac_score'] = 0.0
                result['cf_details'] = "No DAG available"
                result['ac_details'] = "No DAG available"
                continue
            
            try:
                # è®¡ç®—CFï¼ˆä¸‰ä¸ªç»´åº¦çš„å¹³å‡ï¼‰/ Compute CF (average of 3 dimensions)
                # CF = (Causal Intervention + Logic Quality + Graph Quality) / 3
                
                # 1) Causal Intervention Score
                causal_score = self.causal_evaluator.evaluate_causal_intervention(
                    dag=dag,
                    problem_text=problem_text
                )
                
                # 2) Logic Quality Score
                logic_score = self.reward_evaluator.evaluate_logic_quality(
                    reasoning_text=reasoning,
                    problem_text=problem_text
                )
                
                # 3) Graph Quality Score
                graph_score = self.reward_evaluator.evaluate_graph_quality(dag)
                
                # CFç»¼åˆåˆ†æ•° / CF composite score
                cf_score = (causal_score + logic_score + graph_score) / 3.0
                cf_scores.append(cf_score)
                
                # ä¿å­˜è¯¦ç»†ä¿¡æ¯ / Save details
                result['cf_score'] = cf_score
                result['cf_details'] = {
                    'causal_intervention': causal_score,
                    'logic_quality': logic_score,
                    'graph_quality': graph_score
                }
                
            except Exception as e:
                if self.verbose:
                    print(f"âš ï¸  CF evaluation error for problem {i}: {e}")
                cf_scores.append(0.0)
                result['cf_score'] = 0.0
                result['cf_details'] = f"Error: {str(e)}"
            
            try:
                # è®¡ç®—AC / Compute AC
                ac_score = self.abductive_evaluator.evaluate_abductive_reasoning(
                    dag=dag,
                    problem_text=problem_text,
                    final_answer=result.get('answer', '') or result.get('predicted_answer', '')
                )
                ac_scores.append(ac_score)
                result['ac_score'] = ac_score
                
            except Exception as e:
                if self.verbose:
                    print(f"âš ï¸  AC evaluation error for problem {i}: {e}")
                ac_scores.append(0.0)
                result['ac_score'] = 0.0
                result['ac_details'] = f"Error: {str(e)}"
        
        # 4. è®¡ç®—å¹³å‡åˆ† / Compute average scores
        avg_cf = sum(cf_scores) / len(cf_scores) if cf_scores else 0.0
        avg_ac = sum(ac_scores) / len(ac_scores) if ac_scores else 0.0
        
        # 5. æ›´æ–°statistics / Update statistics
        if 'statistics' not in data:
            data['statistics'] = {}
        
        data['statistics']['cf_score'] = avg_cf
        data['statistics']['ac_score'] = avg_ac
        data['statistics']['cf_scores_per_problem'] = cf_scores
        data['statistics']['ac_scores_per_problem'] = ac_scores
        data['statistics']['cf_ac_evaluation_time'] = datetime.now().isoformat()
        
        # 6. ä¿å­˜ / Save
        if self.output_mode == "append":
            # è¦†ç›–åŸæ–‡ä»¶ / Overwrite original file
            output_path = json_path
        else:
            # ç”Ÿæˆæ–°æ–‡ä»¶ / Generate new file
            output_path = json_path.parent / f"{json_path.stem}_evaluated.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        if self.verbose:
            print(f"âœ… {json_path.name}: CF={avg_cf:.3f}, AC={avg_ac:.3f}")


def main():
    """å‘½ä»¤è¡Œå…¥å£ / CLI entry point"""
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡è¯„ä¼°CFå’ŒAC / Batch evaluate CF and AC",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples / ç¤ºä¾‹:
  # è¯„ä¼°æ‰€æœ‰ç»“æœæ–‡ä»¶
  python comparasion/evaluate_cf_ac_batch.py

  # æŒ‡å®šç»“æœç›®å½•
  python comparasion/evaluate_cf_ac_batch.py --results-dir comparasion/results

  # é‡æ–°è¯„ä¼°æ‰€æœ‰æ–‡ä»¶ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
  python comparasion/evaluate_cf_ac_batch.py --no-cache

  # åªè¯„ä¼°æŒ‡å®šæ–¹æ³•
  python comparasion/evaluate_cf_ac_batch.py --methods direct_llm cfgo

  # é™é»˜æ¨¡å¼
  python comparasion/evaluate_cf_ac_batch.py --quiet
        """
    )
    
    parser.add_argument(
        '--results-dir',
        type=str,
        default='comparasion/results',
        help='ç»“æœç›®å½•è·¯å¾„ / Results directory path'
    )
    
    parser.add_argument(
        '--output-mode',
        type=str,
        choices=['append', 'separate'],
        default='append',
        help='è¾“å‡ºæ¨¡å¼ / Output mode: append=è¿½åŠ åˆ°åŸæ–‡ä»¶, separate=ç”Ÿæˆæ–°æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--no-cache',
        action='store_true',
        help='é‡æ–°è¯„ä¼°æ‰€æœ‰æ–‡ä»¶ï¼ˆå¿½ç•¥å·²è¯„ä¼°çš„ï¼‰/ Re-evaluate all files (ignore cache)'
    )
    
    parser.add_argument(
        '--methods',
        type=str,
        nargs='+',
        help='åªè¯„ä¼°æŒ‡å®šæ–¹æ³• / Only evaluate specified methods (e.g., direct_llm cfgo)'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='é™é»˜æ¨¡å¼ / Quiet mode'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨ / Create evaluator
    evaluator = CFACBatchEvaluator(
        results_dir=args.results_dir,
        output_mode=args.output_mode,
        use_cache=not args.no_cache,
        verbose=not args.quiet
    )
    
    # æ‰§è¡Œè¯„ä¼° / Execute evaluation
    evaluator.evaluate_all(method_filter=args.methods)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Evaluation interrupted by user.")
        print("âš ï¸  è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

