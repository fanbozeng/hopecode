"""
ç»Ÿè®¡æ±‡æ€»å·¥å…·
Result Summarization Tool

åŠŸèƒ½ / Features:
1. è¯»å–æ‰€æœ‰å·²è¯„ä¼°çš„ç»“æœJSON
   Read all evaluated result JSON files
2. æå– Accuracyã€CFã€AC
   Extract Accuracy, CF, AC
3. æŒ‰æ–¹æ³•åˆ†ç»„
   Group by method
4. ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ï¼ˆMarkdownã€LaTeXã€CSVï¼‰
   Generate comparison tables (Markdown, LaTeX, CSV)
5. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆæŸ±çŠ¶å›¾ã€é›·è¾¾å›¾ï¼‰
   Generate visualization charts (bar chart, radar chart)
6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
   Generate statistical report

ä½¿ç”¨æ–¹æ³• / Usage:
    # æ±‡æ€»æ‰€æœ‰ç»“æœ
    python comparasion/summarize_results.py

    # æŒ‡å®šè¾“å‡ºç›®å½•
    python comparasion/summarize_results.py --output-dir comparasion/summary

    # åªç”Ÿæˆè¡¨æ ¼ï¼ˆä¸ç”Ÿæˆå›¾è¡¨ï¼‰
    python comparasion/summarize_results.py --no-charts

    # æŒ‡å®šæ•°æ®é›†
    python comparasion/summarize_results.py --dataset gsm8k
"""

import json
import sys
import argparse
import csv
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))


class ResultSummarizer:
    """ç»“æœæ±‡æ€»å™¨ / Result Summarizer"""
    
    def __init__(
        self,
        results_dir: str = "comparasion/results",
        output_dir: str = "comparasion/summary",
        verbose: bool = True
    ):
        """
        åˆå§‹åŒ–æ±‡æ€»å™¨ / Initialize summarizer
        
        Args:
            results_dir: ç»“æœç›®å½• / Results directory
            output_dir: è¾“å‡ºç›®å½• / Output directory
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ / Whether to show verbose info
        """
        self.results_dir = Path(results_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.verbose = verbose
    
    def summarize_all(self, generate_charts: bool = True, dataset_filter: Optional[str] = None):
        """
        æ±‡æ€»æ‰€æœ‰ç»“æœ / Summarize all results
        
        Args:
            generate_charts: æ˜¯å¦ç”Ÿæˆå›¾è¡¨ / Whether to generate charts
            dataset_filter: åªæ±‡æ€»æŒ‡å®šæ•°æ®é›† / Only summarize specified dataset
        """
        print("="*80)
        print("ğŸ“Š ç»Ÿè®¡æ±‡æ€» / Result Summarization")
        print("="*80)
        print(f"ğŸ“ Results directory: {self.results_dir}")
        print(f"ğŸ“ ç»“æœç›®å½•: {self.results_dir}")
        print(f"ğŸ“‚ Output directory: {self.output_dir}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*80 + "\n")
        
        # 1. æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„ç»Ÿè®¡æ•°æ® / Collect statistics for all methods
        print("ğŸ” Collecting statistics...")
        print("ğŸ” æ”¶é›†ç»Ÿè®¡æ•°æ®...")
        summary_data = self._collect_statistics(dataset_filter)
        
        if not summary_data['methods']:
            print("âŒ No evaluated results found!")
            print("âŒ æœªæ‰¾åˆ°å·²è¯„ä¼°çš„ç»“æœï¼")
            print("ğŸ’¡ Please run evaluate_cf_ac_batch.py first.")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œ evaluate_cf_ac_batch.pyã€‚")
            return
        
        print(f"âœ… Found {len(summary_data['methods'])} method(s)")
        print(f"âœ… æ‰¾åˆ° {len(summary_data['methods'])} ä¸ªæ–¹æ³•\n")
        
        # 2. ä¿å­˜åŸå§‹æ±‡æ€»æ•°æ® / Save raw summary data
        print("ğŸ’¾ Saving summary data...")
        print("ğŸ’¾ ä¿å­˜æ±‡æ€»æ•°æ®...")
        self._save_summary_json(summary_data)
        
        # 3. ç”Ÿæˆè¡¨æ ¼ / Generate tables
        print("ğŸ“ Generating tables...")
        print("ğŸ“ ç”Ÿæˆè¡¨æ ¼...")
        self._generate_markdown_table(summary_data)
        self._generate_latex_table(summary_data)
        self._generate_csv_table(summary_data)
        
        # 4. ç”Ÿæˆå›¾è¡¨ / Generate charts
        if generate_charts:
            print("ğŸ“Š Generating charts...")
            print("ğŸ“Š ç”Ÿæˆå›¾è¡¨...")
            try:
                self._generate_bar_chart(summary_data)
                self._generate_radar_chart(summary_data)
            except ImportError:
                print("âš ï¸  matplotlib not installed, skipping charts")
                print("âš ï¸  æœªå®‰è£…matplotlibï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        
        # 5. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š / Generate detailed report
        print("ğŸ“„ Generating detailed report...")
        print("ğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
        self._generate_detailed_report(summary_data)
        
        # 6. æ€»ç»“ / Summary
        print("\n" + "="*80)
        print("âœ… Summary completed! / æ±‡æ€»å®Œæˆï¼")
        print("="*80)
        print(f"ğŸ“‚ Output directory: {self.output_dir}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.output_dir}")
        print("\nğŸ“„ Generated files / ç”Ÿæˆçš„æ–‡ä»¶:")
        for file in sorted(self.output_dir.iterdir()):
            print(f"  - {file.name}")
        print("="*80 + "\n")
    
    def _collect_statistics(self, dataset_filter: Optional[str] = None) -> Dict[str, Any]:
        """
        æ”¶é›†æ‰€æœ‰æ–¹æ³•çš„ç»Ÿè®¡æ•°æ® / Collect statistics for all methods
        
        Args:
            dataset_filter: åªæ”¶é›†æŒ‡å®šæ•°æ®é›† / Only collect specified dataset
        
        Returns:
            æ±‡æ€»æ•°æ®å­—å…¸ / Summary data dictionary
        """
        summary = {
            'timestamp': datetime.now().isoformat(),
            'dataset_filter': dataset_filter,
            'methods': {}
        }
        
        # æ‰«ææ‰€æœ‰æ–¹æ³•ç›®å½• / Scan all method directories
        for method_dir in self.results_dir.iterdir():
            if not method_dir.is_dir():
                continue
            
            method_name = method_dir.name
            
            # å¦‚æœæ˜¯ablationç›®å½•ï¼Œé€’å½’å¤„ç†å­ç›®å½• / If ablation directory, process subdirectories
            if method_name == 'ablation':
                for ablation_dir in method_dir.iterdir():
                    if ablation_dir.is_dir():
                        ablation_name = f"cfgo_{ablation_dir.name}"
                        stats = self._extract_method_statistics(ablation_dir, dataset_filter)
                        if stats:
                            summary['methods'][ablation_name] = stats
            else:
                stats = self._extract_method_statistics(method_dir, dataset_filter)
                if stats:
                    summary['methods'][method_name] = stats
        
        return summary
    
    def _extract_method_statistics(
        self,
        method_dir: Path,
        dataset_filter: Optional[str] = None
    ) -> Optional[Dict]:
        """
        æå–å•ä¸ªæ–¹æ³•çš„ç»Ÿè®¡æ•°æ® / Extract statistics for a single method
        
        Args:
            method_dir: æ–¹æ³•ç›®å½• / Method directory
            dataset_filter: æ•°æ®é›†è¿‡æ»¤å™¨ / Dataset filter
        
        Returns:
            ç»Ÿè®¡æ•°æ®å­—å…¸ / Statistics dictionary
        """
        # æ‰¾åˆ°æ‰€æœ‰JSONæ–‡ä»¶ / Find all JSON files
        json_files = list(method_dir.glob("*.json"))
        if not json_files:
            return None
        
        # å¦‚æœæŒ‡å®šäº†æ•°æ®é›†è¿‡æ»¤ï¼Œåªä¿ç•™åŒ¹é…çš„æ–‡ä»¶ / Filter by dataset if specified
        if dataset_filter:
            json_files = [f for f in json_files if dataset_filter.lower() in f.name.lower()]
            if not json_files:
                return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„ / Sort by modification time, take latest
        latest_file = max(json_files, key=lambda p: p.stat().st_mtime)
        
        # åŠ è½½JSON / Load JSON
        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            if self.verbose:
                print(f"âš ï¸  Error loading {latest_file.name}: {e}")
            return None
        
        # æå–ç»Ÿè®¡æ•°æ® / Extract statistics
        stats = data.get('statistics', {})
        
        # æ£€æŸ¥æ˜¯å¦æœ‰CF/ACåˆ†æ•° / Check if CF/AC scores exist
        if 'cf_score' not in stats or 'ac_score' not in stats:
            if self.verbose:
                print(f"âš ï¸  {latest_file.name} missing CF/AC scores (run evaluate_cf_ac_batch.py first)")
            # ä»ç„¶è¿”å›æ•°æ®ï¼Œä½†CF/ACä¸ºNone / Still return data, but CF/AC are None
        
        return {
            'file': latest_file.name,
            'dataset': self._extract_dataset_name(latest_file.name),
            'total_problems': stats.get('total', 0),
            'correct': stats.get('correct', 0),
            'accuracy': stats.get('accuracy', 0.0),
            'cf_score': stats.get('cf_score', None),
            'ac_score': stats.get('ac_score', None),
            'avg_time': stats.get('avg_time', 0.0),
            'total_time': stats.get('total_time', 0.0),
            'errors': stats.get('errors', 0),
        }
    
    def _extract_dataset_name(self, filename: str) -> str:
        """
        ä»æ–‡ä»¶åæå–æ•°æ®é›†åç§° / Extract dataset name from filename
        
        Args:
            filename: æ–‡ä»¶å / Filename
        
        Returns:
            æ•°æ®é›†åç§° / Dataset name
        """
        # å¸¸è§æ•°æ®é›†åç§° / Common dataset names
        datasets = ['gsm8k', 'math', 'mydata', 'omnimath', 'olympiad']
        
        filename_lower = filename.lower()
        for dataset in datasets:
            if dataset in filename_lower:
                return dataset.upper()
        
        return "Unknown"
    
    def _save_summary_json(self, summary_data: Dict):
        """ä¿å­˜åŸå§‹æ±‡æ€»æ•°æ® / Save raw summary data"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.output_dir / f"summary_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        print(f"  âœ… {output_file.name}")
    
    def _generate_markdown_table(self, summary_data: Dict):
        """ç”ŸæˆMarkdownè¡¨æ ¼ / Generate Markdown table"""
        output_file = self.output_dir / "comparison_table.md"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# æ–¹æ³•å¯¹æ¯”è¡¨ / Method Comparison Table\n\n")
            f.write(f"**Generated at / ç”Ÿæˆæ—¶é—´**: {summary_data['timestamp']}\n\n")
            
            # åˆ†ç»„ï¼šåŸºçº¿æ–¹æ³•ã€CFGOã€æ¶ˆèå®éªŒ
            # Group: Baselines, CFGO, Ablations
            baselines = []
            cfgo_methods = []
            ablations = []
            
            for method_name in summary_data['methods'].keys():
                if method_name in ['direct_llm', 'zero_shot_cot', 'few_shot_cot']:
                    baselines.append(method_name)
                elif method_name == 'cfgo' or method_name == 'cfgo_full':
                    cfgo_methods.append(method_name)
                elif method_name.startswith('cfgo_'):
                    ablations.append(method_name)
                else:
                    cfgo_methods.append(method_name)
            
            # åŸºçº¿æ–¹æ³•è¡¨æ ¼ / Baseline methods table
            if baselines or cfgo_methods:
                f.write("## åŸºçº¿æ–¹æ³• vs CFGO / Baselines vs CFGO\n\n")
                f.write("| Method | Dataset | Accuracy | CF Score | AC Score | Avg Time (s) |\n")
                f.write("|--------|---------|----------|----------|----------|-------------|\n")
                
                for method in baselines:
                    if method in summary_data['methods']:
                        stats = summary_data['methods'][method]
                        f.write(self._format_table_row(method, stats))
                
                # CFGOå®Œæ•´ç‰ˆ / CFGO full version
                for method in cfgo_methods:
                    if method in summary_data['methods']:
                        stats = summary_data['methods'][method]
                        display_name = f"**{method.upper()}**"
                        f.write(self._format_table_row(display_name, stats))
            
            # æ¶ˆèå®éªŒè¡¨æ ¼ / Ablation experiments table
            if ablations:
                f.write("\n## æ¶ˆèå®éªŒ / Ablation Studies\n\n")
                f.write("| Ablation | Dataset | Accuracy | CF Score | AC Score | Avg Time (s) |\n")
                f.write("|----------|---------|----------|----------|----------|-------------|\n")
                
                # æ’åºï¼šfullåœ¨æœ€å‰é¢ / Sort: full first
                ablations_sorted = sorted(ablations, key=lambda x: (x != 'cfgo_full', x))
                
                for method in ablations_sorted:
                    if method in summary_data['methods']:
                        stats = summary_data['methods'][method]
                        display_name = method.replace('cfgo_', 'CFGO-')
                        f.write(self._format_table_row(display_name, stats))
            
            # æ·»åŠ è¯´æ˜ / Add notes
            f.write("\n## è¯´æ˜ / Notes\n\n")
            f.write("- **Accuracy**: ç­”æ¡ˆæ­£ç¡®ç‡ / Answer correctness rate\n")
            f.write("- **CF Score**: åäº‹å®å¿ å®åº¦ (Counterfactual Faithfulness)\n")
            f.write("  - ç»¼åˆè¯„åˆ† = (å› æœå¹²é¢„ + é€»è¾‘è´¨é‡ + å›¾è´¨é‡) / 3\n")
            f.write("  - Composite score = (Causal Intervention + Logic Quality + Graph Quality) / 3\n")
            f.write("- **AC Score**: æº¯å› ä¸€è‡´æ€§ (Abductive Consistency)\n")
            f.write("  - è¯„ä¼°ç­”æ¡ˆèƒ½å¦è¢«ä¸€è‡´åœ°åå‘æ¨å¯¼\n")
            f.write("  - Evaluates if answer can be consistently reverse-engineered\n")
            f.write("- **Avg Time**: å¹³å‡æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰/ Average execution time (seconds)\n")
        
        print(f"  âœ… {output_file.name}")
    
    def _format_table_row(self, method_name: str, stats: Dict) -> str:
        """
        æ ¼å¼åŒ–è¡¨æ ¼è¡Œ / Format table row
        
        Args:
            method_name: æ–¹æ³•åç§° / Method name
            stats: ç»Ÿè®¡æ•°æ® / Statistics
        
        Returns:
            è¡¨æ ¼è¡Œå­—ç¬¦ä¸² / Table row string
        """
        dataset = stats.get('dataset', 'N/A')
        accuracy = f"{stats['accuracy']*100:.2f}%" if stats.get('accuracy') is not None else "N/A"
        cf_score = f"{stats['cf_score']:.3f}" if stats.get('cf_score') is not None else "N/A"
        ac_score = f"{stats['ac_score']:.3f}" if stats.get('ac_score') is not None else "N/A"
        avg_time = f"{stats['avg_time']:.2f}" if stats.get('avg_time') is not None else "N/A"
        
        return f"| {method_name} | {dataset} | {accuracy} | {cf_score} | {ac_score} | {avg_time} |\n"
    
    def _generate_latex_table(self, summary_data: Dict):
        """ç”ŸæˆLaTeXè¡¨æ ¼ / Generate LaTeX table"""
        output_file = self.output_dir / "comparison_table.tex"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("% æ–¹æ³•å¯¹æ¯”è¡¨ / Method Comparison Table\n")
            f.write(f"% Generated at: {summary_data['timestamp']}\n\n")
            
            f.write("\\begin{table}[h]\n")
            f.write("\\centering\n")
            f.write("\\caption{æ–¹æ³•å¯¹æ¯”ç»“æœ / Method Comparison Results}\n")
            f.write("\\label{tab:method_comparison}\n")
            f.write("\\begin{tabular}{lcccc}\n")
            f.write("\\hline\n")
            f.write("Method & Accuracy & CF Score & AC Score & Avg Time (s) \\\\\n")
            f.write("\\hline\n")
            
            # åŸºçº¿æ–¹æ³• / Baseline methods
            baselines = ['direct_llm', 'zero_shot_cot', 'few_shot_cot']
            for method in baselines:
                if method in summary_data['methods']:
                    stats = summary_data['methods'][method]
                    f.write(self._format_latex_row(method.replace('_', ' ').title(), stats))
            
            f.write("\\hline\n")
            
            # CFGO / CFGO
            cfgo_methods = ['cfgo', 'cfgo_full']
            for method in cfgo_methods:
                if method in summary_data['methods']:
                    stats = summary_data['methods'][method]
                    f.write(self._format_latex_row('\\textbf{CFGO (Full)}', stats))
                    break
            
            f.write("\\hline\n")
            f.write("\\end{tabular}\n")
            f.write("\\end{table}\n")
        
        print(f"  âœ… {output_file.name}")
    
    def _format_latex_row(self, method_name: str, stats: Dict) -> str:
        """
        æ ¼å¼åŒ–LaTeXè¡¨æ ¼è¡Œ / Format LaTeX table row
        
        Args:
            method_name: æ–¹æ³•åç§° / Method name
            stats: ç»Ÿè®¡æ•°æ® / Statistics
        
        Returns:
            LaTeXè¡¨æ ¼è¡Œå­—ç¬¦ä¸² / LaTeX table row string
        """
        accuracy = f"{stats['accuracy']*100:.2f}\\%" if stats.get('accuracy') is not None else "N/A"
        cf_score = f"{stats['cf_score']:.3f}" if stats.get('cf_score') is not None else "N/A"
        ac_score = f"{stats['ac_score']:.3f}" if stats.get('ac_score') is not None else "N/A"
        avg_time = f"{stats['avg_time']:.2f}" if stats.get('avg_time') is not None else "N/A"
        
        return f"{method_name} & {accuracy} & {cf_score} & {ac_score} & {avg_time} \\\\\n"
    
    def _generate_csv_table(self, summary_data: Dict):
        """ç”ŸæˆCSVè¡¨æ ¼ / Generate CSV table"""
        output_file = self.output_dir / "comparison_table.csv"
        
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # è¡¨å¤´ / Header
            writer.writerow(['Method', 'Dataset', 'Total Problems', 'Correct', 'Accuracy', 'CF Score', 'AC Score', 'Avg Time (s)', 'Total Time (s)', 'Errors'])
            
            # æ•°æ®è¡Œ / Data rows
            for method_name, stats in sorted(summary_data['methods'].items()):
                writer.writerow([
                    method_name,
                    stats.get('dataset', 'N/A'),
                    stats.get('total_problems', 0),
                    stats.get('correct', 0),
                    f"{stats['accuracy']*100:.2f}" if stats.get('accuracy') is not None else "N/A",
                    f"{stats['cf_score']:.3f}" if stats.get('cf_score') is not None else "N/A",
                    f"{stats['ac_score']:.3f}" if stats.get('ac_score') is not None else "N/A",
                    f"{stats['avg_time']:.2f}" if stats.get('avg_time') is not None else "N/A",
                    f"{stats['total_time']:.2f}" if stats.get('total_time') is not None else "N/A",
                    stats.get('errors', 0)
                ])
        
        print(f"  âœ… {output_file.name}")
    
    def _generate_bar_chart(self, summary_data: Dict):
        """ç”ŸæˆæŸ±çŠ¶å›¾ / Generate bar chart"""
        try:
            import matplotlib.pyplot as plt
            import numpy as np
        except ImportError:
            print("  âš ï¸  matplotlib not installed, skipping bar chart")
            return
        
        # å‡†å¤‡æ•°æ® / Prepare data
        methods = []
        accuracies = []
        cf_scores = []
        ac_scores = []
        
        for method, stats in sorted(summary_data['methods'].items()):
            methods.append(method.replace('_', '\n'))  # æ¢è¡Œä»¥é€‚åº”å›¾è¡¨ / Line break for chart
            accuracies.append(stats['accuracy'] * 100 if stats.get('accuracy') is not None else 0)
            cf_scores.append(stats['cf_score'] * 100 if stats.get('cf_score') is not None else 0)
            ac_scores.append(stats['ac_score'] * 100 if stats.get('ac_score') is not None else 0)
        
        # åˆ›å»ºå›¾è¡¨ / Create chart
        x = np.arange(len(methods))
        width = 0.25
        
        fig, ax = plt.subplots(figsize=(14, 7))
        
        bars1 = ax.bar(x - width, accuracies, width, label='Accuracy', color='#3498db')
        bars2 = ax.bar(x, cf_scores, width, label='CF Score', color='#e74c3c')
        bars3 = ax.bar(x + width, ac_scores, width, label='AC Score', color='#2ecc71')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾ / Add value labels
        def autolabel(bars):
            for bar in bars:
                height = bar.get_height()
                if height > 0:
                    ax.annotate(f'{height:.1f}',
                                xy=(bar.get_x() + bar.get_width() / 2, height),
                                xytext=(0, 3),
                                textcoords="offset points",
                                ha='center', va='bottom',
                                fontsize=8)
        
        autolabel(bars1)
        autolabel(bars2)
        autolabel(bars3)
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜ / Set labels and title
        ax.set_xlabel('Method', fontsize=12, fontweight='bold')
        ax.set_ylabel('Score (%)', fontsize=12, fontweight='bold')
        ax.set_title('Method Comparison: Accuracy, CF Score, AC Score', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(methods, rotation=0, ha='center', fontsize=9)
        ax.legend(fontsize=10)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.set_ylim(0, 105)
        
        plt.tight_layout()
        output_file = self.output_dir / "comparison_chart.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… {output_file.name}")
    
    def _generate_radar_chart(self, summary_data: Dict):
        """ç”Ÿæˆé›·è¾¾å›¾ / Generate radar chart"""
        try:
            import matplotlib.pyplot as plt
            import numpy as np
        except ImportError:
            print("  âš ï¸  matplotlib not installed, skipping radar chart")
            return
        
        # é€‰æ‹©å…³é”®æ–¹æ³• / Select key methods
        key_methods = []
        method_priority = ['direct_llm', 'zero_shot_cot', 'few_shot_cot', 'cfgo', 'cfgo_full']
        
        for method in method_priority:
            if method in summary_data['methods']:
                key_methods.append(method)
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ–¹æ³•ï¼Œæ·»åŠ å…¶ä»–æ–¹æ³• / If not enough methods, add others
        for method in summary_data['methods'].keys():
            if method not in key_methods and len(key_methods) < 5:
                key_methods.append(method)
        
        if len(key_methods) < 2:
            print("  âš ï¸  Not enough methods for radar chart (need at least 2)")
            return
        
        # å‡†å¤‡æ•°æ® / Prepare data
        categories = ['Accuracy', 'CF Score', 'AC Score']
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]
        
        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6']
        
        for i, method in enumerate(key_methods):
            if method not in summary_data['methods']:
                continue
            
            stats = summary_data['methods'][method]
            values = [
                stats['accuracy'] * 100 if stats.get('accuracy') is not None else 0,
                stats['cf_score'] * 100 if stats.get('cf_score') is not None else 0,
                stats['ac_score'] * 100 if stats.get('ac_score') is not None else 0,
            ]
            values += values[:1]
            
            color = colors[i % len(colors)]
            ax.plot(angles, values, 'o-', linewidth=2, label=method, color=color)
            ax.fill(angles, values, alpha=0.15, color=color)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=12)
        ax.set_ylim(0, 100)
        ax.set_title('Method Comparison (Radar Chart)', fontsize=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=10)
        ax.grid(True, linestyle='--', alpha=0.5)
        
        plt.tight_layout()
        output_file = self.output_dir / "radar_chart.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ… {output_file.name}")
    
    def _generate_detailed_report(self, summary_data: Dict):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š / Generate detailed report"""
        output_file = self.output_dir / "detailed_report.md"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# è¯¦ç»†è¯„ä¼°æŠ¥å‘Š / Detailed Evaluation Report\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´ / Generated at**: {summary_data['timestamp']}\n\n")
            f.write("---\n\n")
            
            # æ€»ä½“ç»Ÿè®¡ / Overall statistics
            f.write("## æ€»ä½“ç»Ÿè®¡ / Overall Statistics\n\n")
            f.write(f"- **è¯„ä¼°æ–¹æ³•æ•° / Number of methods**: {len(summary_data['methods'])}\n")
            
            # æ‰¾å‡ºæœ€ä½³æ–¹æ³• / Find best methods
            best_accuracy_method = max(summary_data['methods'].items(), 
                                      key=lambda x: x[1].get('accuracy', 0) or 0)
            f.write(f"- **æœ€é«˜å‡†ç¡®ç‡ / Highest accuracy**: {best_accuracy_method[0]} ({best_accuracy_method[1]['accuracy']*100:.2f}%)\n")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰CF/ACåˆ†æ•° / Check if CF/AC scores exist
            methods_with_cf = {k: v for k, v in summary_data['methods'].items() 
                              if v.get('cf_score') is not None}
            
            if methods_with_cf:
                best_cf_method = max(methods_with_cf.items(), 
                                    key=lambda x: x[1].get('cf_score', 0) or 0)
                f.write(f"- **æœ€é«˜CFåˆ†æ•° / Highest CF score**: {best_cf_method[0]} ({best_cf_method[1]['cf_score']:.3f})\n")
                
                best_ac_method = max(methods_with_cf.items(), 
                                    key=lambda x: x[1].get('ac_score', 0) or 0)
                f.write(f"- **æœ€é«˜ACåˆ†æ•° / Highest AC score**: {best_ac_method[0]} ({best_ac_method[1]['ac_score']:.3f})\n")
            
            f.write("\n---\n\n")
            
            # å„æ–¹æ³•è¯¦ç»†ä¿¡æ¯ / Detailed information for each method
            f.write("## å„æ–¹æ³•è¯¦ç»†ä¿¡æ¯ / Detailed Method Information\n\n")
            
            for method_name, stats in sorted(summary_data['methods'].items()):
                f.write(f"### {method_name}\n\n")
                f.write(f"- **æ•°æ®é›† / Dataset**: {stats.get('dataset', 'N/A')}\n")
                f.write(f"- **æ¥æºæ–‡ä»¶ / Source file**: `{stats.get('file', 'N/A')}`\n")
                f.write(f"- **é—®é¢˜æ€»æ•° / Total problems**: {stats.get('total_problems', 0)}\n")
                f.write(f"- **æ­£ç¡®æ•° / Correct**: {stats.get('correct', 0)}\n")
                f.write(f"- **å‡†ç¡®ç‡ / Accuracy**: {stats['accuracy']*100:.2f}%\n" if stats.get('accuracy') is not None else "- **å‡†ç¡®ç‡ / Accuracy**: N/A\n")
                f.write(f"- **CFåˆ†æ•° / CF Score**: {stats['cf_score']:.3f}\n" if stats.get('cf_score') is not None else "- **CFåˆ†æ•° / CF Score**: N/A (éœ€è¦å…ˆè¿è¡Œevaluate_cf_ac_batch.py)\n")
                f.write(f"- **ACåˆ†æ•° / AC Score**: {stats['ac_score']:.3f}\n" if stats.get('ac_score') is not None else "- **ACåˆ†æ•° / AC Score**: N/A (éœ€è¦å…ˆè¿è¡Œevaluate_cf_ac_batch.py)\n")
                f.write(f"- **å¹³å‡æ—¶é—´ / Avg time**: {stats['avg_time']:.2f}s\n" if stats.get('avg_time') is not None else "- **å¹³å‡æ—¶é—´ / Avg time**: N/A\n")
                f.write(f"- **æ€»æ—¶é—´ / Total time**: {stats['total_time']:.2f}s\n" if stats.get('total_time') is not None else "- **æ€»æ—¶é—´ / Total time**: N/A\n")
                f.write(f"- **é”™è¯¯æ•° / Errors**: {stats.get('errors', 0)}\n")
                f.write("\n")
            
            f.write("---\n\n")
            
            # è¯„ä¼°æŒ‡æ ‡è¯´æ˜ / Evaluation metrics explanation
            f.write("## è¯„ä¼°æŒ‡æ ‡è¯´æ˜ / Evaluation Metrics Explanation\n\n")
            f.write("### Accuracy (å‡†ç¡®ç‡)\n")
            f.write("- ç­”æ¡ˆæ­£ç¡®çš„é—®é¢˜æ•° / æ€»é—®é¢˜æ•°\n")
            f.write("- Number of correct answers / Total number of problems\n\n")
            
            f.write("### CF Score (åäº‹å®å¿ å®åº¦ / Counterfactual Faithfulness)\n")
            f.write("- **ç»¼åˆè¯„åˆ†** = (å› æœå¹²é¢„åˆ†æ•° + é€»è¾‘è´¨é‡åˆ†æ•° + å›¾è´¨é‡åˆ†æ•°) / 3\n")
            f.write("- **Composite score** = (Causal Intervention + Logic Quality + Graph Quality) / 3\n")
            f.write("- **å› æœå¹²é¢„ / Causal Intervention**: ä½¿ç”¨do-calculusè¯„ä¼°èŠ‚ç‚¹é‡è¦æ€§\n")
            f.write("- **é€»è¾‘è´¨é‡ / Logic Quality**: LLMè¯„ä¼°æ¨ç†çš„é€»è¾‘è¿è´¯æ€§\n")
            f.write("- **å›¾è´¨é‡ / Graph Quality**: è¯„ä¼°DAGçš„ç»“æ„å®Œæ•´æ€§\n\n")
            
            f.write("### AC Score (æº¯å› ä¸€è‡´æ€§ / Abductive Consistency)\n")
            f.write("- è¯„ä¼°ç­”æ¡ˆèƒ½å¦è¢«ä¸€è‡´åœ°åå‘æ¨å¯¼\n")
            f.write("- Evaluates if the answer can be consistently reverse-engineered\n")
            f.write("- é€šè¿‡LLMä»ç­”æ¡ˆåæ¨é—®é¢˜ï¼Œæ£€æŸ¥ä¸€è‡´æ€§\n")
            f.write("- Uses LLM to reverse-engineer from answer to problem, checking consistency\n\n")
        
        print(f"  âœ… {output_file.name}")


def main():
    """å‘½ä»¤è¡Œå…¥å£ / CLI entry point"""
    parser = argparse.ArgumentParser(
        description="ç»Ÿè®¡æ±‡æ€»å·¥å…· / Result Summarization Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples / ç¤ºä¾‹:
  # æ±‡æ€»æ‰€æœ‰ç»“æœ
  python comparasion/summarize_results.py

  # æŒ‡å®šè¾“å‡ºç›®å½•
  python comparasion/summarize_results.py --output-dir comparasion/summary

  # åªç”Ÿæˆè¡¨æ ¼ï¼ˆä¸ç”Ÿæˆå›¾è¡¨ï¼‰
  python comparasion/summarize_results.py --no-charts

  # æŒ‡å®šæ•°æ®é›†
  python comparasion/summarize_results.py --dataset gsm8k
        """
    )
    
    parser.add_argument(
        '--results-dir',
        type=str,
        default='comparasion/results',
        help='ç»“æœç›®å½•è·¯å¾„ / Results directory path'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='comparasion/summary',
        help='è¾“å‡ºç›®å½•è·¯å¾„ / Output directory path'
    )
    
    parser.add_argument(
        '--no-charts',
        action='store_true',
        help='ä¸ç”Ÿæˆå›¾è¡¨ / Do not generate charts'
    )
    
    parser.add_argument(
        '--dataset',
        type=str,
        help='åªæ±‡æ€»æŒ‡å®šæ•°æ®é›† / Only summarize specified dataset (e.g., gsm8k)'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='é™é»˜æ¨¡å¼ / Quiet mode'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ±‡æ€»å™¨ / Create summarizer
    summarizer = ResultSummarizer(
        results_dir=args.results_dir,
        output_dir=args.output_dir,
        verbose=not args.quiet
    )
    
    # æ‰§è¡Œæ±‡æ€» / Execute summarization
    summarizer.summarize_all(
        generate_charts=not args.no_charts,
        dataset_filter=args.dataset
    )


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Summarization interrupted by user.")
        print("âš ï¸  æ±‡æ€»è¢«ç”¨æˆ·ä¸­æ–­ã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ Error: {e}")
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

