"""
è¯„ä¼°å·¥å…·æµ‹è¯•è„šæœ¬
Evaluation Tools Test Script

ç”¨äºæµ‹è¯•æ‰¹é‡CF/ACè¯„ä¼°å·¥å…·å’Œç»Ÿè®¡æ±‡æ€»å·¥å…·æ˜¯å¦æ­£å¸¸å·¥ä½œ
Tests if batch CF/AC evaluation tool and result summarization tool work correctly
"""

import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))


def test_imports():
    """æµ‹è¯•å¯¼å…¥ / Test imports"""
    print("="*80)
    print("ğŸ§ª æµ‹è¯•1: æ£€æŸ¥æ¨¡å—å¯¼å…¥ / Test 1: Check Module Imports")
    print("="*80)
    
    try:
        from comparasion.evaluate_cf_ac_batch import CFACBatchEvaluator
        print("âœ… evaluate_cf_ac_batch.py å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ evaluate_cf_ac_batch.py å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from comparasion.summarize_results import ResultSummarizer
        print("âœ… summarize_results.py å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ summarize_results.py å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    try:
        from causal_evaluation import (
            CausalInterventionEvaluator,
            AbductiveReasoningEvaluator,
            RewardEvaluator
        )
        print("âœ… causal_evaluation.py å¯¼å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ causal_evaluation.py å¯¼å…¥å¤±è´¥: {e}")
        return False
    
    print("\nâœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸï¼\n")
    return True


def test_directory_structure():
    """æµ‹è¯•ç›®å½•ç»“æ„ / Test directory structure"""
    print("="*80)
    print("ğŸ§ª æµ‹è¯•2: æ£€æŸ¥ç›®å½•ç»“æ„ / Test 2: Check Directory Structure")
    print("="*80)
    
    results_dir = Path("comparasion/results")
    
    if not results_dir.exists():
        print(f"âš ï¸  ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        print(f"âš ï¸  Results directory does not exist: {results_dir}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œå®éªŒç”Ÿæˆç»“æœæ–‡ä»¶")
        print("ğŸ’¡ Please run experiments first to generate result files")
        return False
    
    print(f"âœ… ç»“æœç›®å½•å­˜åœ¨: {results_dir}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœæ–‡ä»¶
    json_files = list(results_dir.rglob("*.json"))
    
    if not json_files:
        print(f"âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
        print(f"âš ï¸  No result files found")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œå®éªŒç”Ÿæˆç»“æœæ–‡ä»¶")
        print("ğŸ’¡ Please run experiments first to generate result files")
        return False
    
    print(f"âœ… æ‰¾åˆ° {len(json_files)} ä¸ªç»“æœæ–‡ä»¶")
    
    # åˆ—å‡ºå‰5ä¸ªæ–‡ä»¶
    print("\nğŸ“„ ç¤ºä¾‹æ–‡ä»¶ / Sample files:")
    for i, file in enumerate(json_files[:5], 1):
        print(f"  {i}. {file.relative_to(results_dir.parent)}")
    
    if len(json_files) > 5:
        print(f"  ... è¿˜æœ‰ {len(json_files) - 5} ä¸ªæ–‡ä»¶")
    
    print()
    return True


def test_result_file_format():
    """æµ‹è¯•ç»“æœæ–‡ä»¶æ ¼å¼ / Test result file format"""
    print("="*80)
    print("ğŸ§ª æµ‹è¯•3: æ£€æŸ¥ç»“æœæ–‡ä»¶æ ¼å¼ / Test 3: Check Result File Format")
    print("="*80)
    
    results_dir = Path("comparasion/results")
    json_files = list(results_dir.rglob("*.json"))
    
    if not json_files:
        print("âš ï¸  æ²¡æœ‰ç»“æœæ–‡ä»¶å¯æµ‹è¯•")
        return False
    
    # æµ‹è¯•ç¬¬ä¸€ä¸ªæ–‡ä»¶
    test_file = json_files[0]
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_file.name}")
    
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print("âœ… JSONæ ¼å¼æ­£ç¡®")
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['method', 'dataset', 'results', 'statistics']
        missing_fields = [f for f in required_fields if f not in data]
        
        if missing_fields:
            print(f"âš ï¸  ç¼ºå°‘å­—æ®µ: {missing_fields}")
        else:
            print("âœ… åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰CF/ACåˆ†æ•°
        stats = data.get('statistics', {})
        has_cf = 'cf_score' in stats
        has_ac = 'ac_score' in stats
        
        if has_cf and has_ac:
            print(f"âœ… å·²åŒ…å«CF/ACåˆ†æ•° (CF={stats['cf_score']:.3f}, AC={stats['ac_score']:.3f})")
        else:
            print("âš ï¸  å°šæœªè¯„ä¼°CF/ACåˆ†æ•°")
            print("ğŸ’¡ è¿è¡Œ: python comparasion/evaluate_cf_ac_batch.py")
        
        # æ£€æŸ¥resultsæ•°ç»„
        results = data.get('results', [])
        print(f"âœ… åŒ…å« {len(results)} ä¸ªé—®é¢˜ç»“æœ")
        
        print()
        return True
        
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return False


def test_evaluator_initialization():
    """æµ‹è¯•è¯„ä¼°å™¨åˆå§‹åŒ– / Test evaluator initialization"""
    print("="*80)
    print("ğŸ§ª æµ‹è¯•4: æµ‹è¯•è¯„ä¼°å™¨åˆå§‹åŒ– / Test 4: Test Evaluator Initialization")
    print("="*80)
    
    try:
        from comparasion.evaluate_cf_ac_batch import CFACBatchEvaluator
        
        print("ğŸ”§ åˆå§‹åŒ–CFACBatchEvaluator...")
        evaluator = CFACBatchEvaluator(
            results_dir="comparasion/results",
            verbose=False
        )
        print("âœ… CFACBatchEvaluator åˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ CFACBatchEvaluator åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    try:
        from comparasion.summarize_results import ResultSummarizer
        
        print("ğŸ”§ åˆå§‹åŒ–ResultSummarizer...")
        summarizer = ResultSummarizer(
            results_dir="comparasion/results",
            output_dir="comparasion/summary_test",
            verbose=False
        )
        print("âœ… ResultSummarizer åˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ ResultSummarizer åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    print("\nâœ… æ‰€æœ‰è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸï¼\n")
    return True


def test_causal_evaluation_modules():
    """æµ‹è¯•å› æœè¯„ä¼°æ¨¡å— / Test causal evaluation modules"""
    print("="*80)
    print("ğŸ§ª æµ‹è¯•5: æµ‹è¯•å› æœè¯„ä¼°æ¨¡å— / Test 5: Test Causal Evaluation Modules")
    print("="*80)
    
    try:
        from causal_evaluation import (
            CausalInterventionEvaluator,
            AbductiveReasoningEvaluator,
            RewardEvaluator
        )
        
        print("ğŸ”§ åˆå§‹åŒ–CausalInterventionEvaluator...")
        causal_eval = CausalInterventionEvaluator(verbose=False)
        print("âœ… CausalInterventionEvaluator åˆå§‹åŒ–æˆåŠŸ")
        
        print("ğŸ”§ åˆå§‹åŒ–AbductiveReasoningEvaluator...")
        abductive_eval = AbductiveReasoningEvaluator(verbose=False)
        print("âœ… AbductiveReasoningEvaluator åˆå§‹åŒ–æˆåŠŸ")
        
        print("ğŸ”§ åˆå§‹åŒ–RewardEvaluator...")
        reward_eval = RewardEvaluator(verbose=False)
        print("âœ… RewardEvaluator åˆå§‹åŒ–æˆåŠŸ")
        
        print("\nâœ… æ‰€æœ‰å› æœè¯„ä¼°æ¨¡å—åˆå§‹åŒ–æˆåŠŸï¼\n")
        return True
        
    except Exception as e:
        print(f"âŒ å› æœè¯„ä¼°æ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def print_summary(results):
    """æ‰“å°æµ‹è¯•æ€»ç»“ / Print test summary"""
    print("="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“ / Test Summary")
    print("="*80)
    
    total = len(results)
    passed = sum(results.values())
    failed = total - passed
    
    print(f"âœ… é€šè¿‡: {passed}/{total}")
    print(f"âŒ å¤±è´¥: {failed}/{total}")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¯„ä¼°å·¥å…·å·²å‡†å¤‡å°±ç»ªï¼")
        print("ğŸ‰ All tests passed! Evaluation tools are ready!")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("   1. è¿è¡Œ: python comparasion/evaluate_cf_ac_batch.py")
        print("   2. è¿è¡Œ: python comparasion/summarize_results.py")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        print("âš ï¸  Some tests failed, please check error messages")
    
    print("="*80)


def main():
    """ä¸»æµ‹è¯•å‡½æ•° / Main test function"""
    print("\n")
    print("â•”" + "="*78 + "â•—")
    print("â•‘" + " "*20 + "è¯„ä¼°å·¥å…·æµ‹è¯•è„šæœ¬" + " "*20 + "                  â•‘")
    print("â•‘" + " "*15 + "Evaluation Tools Test Script" + " "*15 + "           â•‘")
    print("â•š" + "="*78 + "â•")
    print("\n")
    
    results = {}
    
    # è¿è¡Œæµ‹è¯•
    results['test_imports'] = test_imports()
    results['test_directory_structure'] = test_directory_structure()
    results['test_result_file_format'] = test_result_file_format()
    results['test_evaluator_initialization'] = test_evaluator_initialization()
    results['test_causal_evaluation_modules'] = test_causal_evaluation_modules()
    
    # æ‰“å°æ€»ç»“
    print_summary(results)
    
    # è¿”å›é€€å‡ºç 
    return 0 if all(results.values()) else 1


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

