"""
Evaluation Framework / è¯„ä¼°æ¡†æ¶

Overview / æ¦‚è¿°
- Runs and compares multiple solving methods on several datasets.
  åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿è¡Œå¹¶æ¯”è¾ƒå¤šç§æ±‚è§£æ–¹æ³•ã€‚
- Methods include baselines (Direct LLM, Zeroâ€‘shot CoT, Fewâ€‘shot CoT),
  the full framework pipeline, and ablations (e.g., no retriever).
  æ–¹æ³•åŒ…æ‹¬åŸºçº¿ï¼ˆç›´æ¥LLMã€é›¶æ ·æœ¬CoTã€å°‘æ ·æœ¬CoTï¼‰ã€å®Œæ•´æ¡†æ¶ç®¡é“ä»¥åŠæ¶ˆèå®éªŒï¼ˆå¦‚ä¸ä½¿ç”¨æ£€ç´¢å™¨ï¼‰ã€‚
- Outputs perâ€‘problem results, method statistics, and a comparison table.
  è¾“å‡ºæ¯ä¸ªé—®é¢˜çš„ç»“æœã€æ–¹æ³•ç»Ÿè®¡å’Œå¯¹æ¯”è¡¨ã€‚

Key Components / æ ¸å¿ƒç»„ä»¶
1. DatasetLoader: Load multiple math datasets (GSM8K, MATH, OlympiadBench, etc.)
   æ•°æ®é›†åŠ è½½å™¨ï¼šåŠ è½½å¤šç§æ•°å­¦æ•°æ®é›†ï¼ˆGSM8Kã€MATHã€OlympiadBenchç­‰ï¼‰
2. BaselineEvaluator: Implement baseline solving methods
   åŸºçº¿è¯„ä¼°å™¨ï¼šå®ç°åŸºçº¿æ±‚è§£æ–¹æ³•
3. FrameworkEvaluator: Evaluate full framework and ablation variants
   æ¡†æ¶è¯„ä¼°å™¨ï¼šè¯„ä¼°å®Œæ•´æ¡†æ¶å’Œæ¶ˆèå˜ä½“
4. EvaluationMethod: Enumeration of all supported methods
   è¯„ä¼°æ–¹æ³•ï¼šæ‰€æœ‰æ”¯æŒæ–¹æ³•çš„æšä¸¾
5. EvaluationResult: Structured result for each problem
   è¯„ä¼°ç»“æœï¼šæ¯ä¸ªé—®é¢˜çš„ç»“æ„åŒ–ç»“æœ
"""

# æ ‡å‡†åº“å¯¼å…¥ / Standard library imports
import json          # JSON åºåˆ—åŒ–å’Œååºåˆ—åŒ– / JSON serialization and deserialization
import sys           # ç³»ç»Ÿç›¸å…³åŠŸèƒ½ï¼ˆé€€å‡ºç ç­‰ï¼‰/ System-specific functions (exit codes, etc.)
import time          # æ—¶é—´æµ‹é‡ï¼ˆæ‰§è¡Œæ—¶é—´ç»Ÿè®¡ï¼‰/ Time measurement (execution time tracking)
import re            # æ­£åˆ™è¡¨è¾¾å¼ï¼ˆç­”æ¡ˆæå–å’Œæ¯”è¾ƒï¼‰/ Regular expressions (answer extraction and comparison)
from pathlib import Path          # è·¯å¾„æ“ä½œ / Path operations
from typing import List, Dict, Any, Optional, Tuple  # ç±»å‹æ³¨è§£ / Type annotations
from dataclasses import dataclass, asdict  # æ•°æ®ç±»å’Œè½¬æ¢ / Data classes and conversion
from datetime import datetime     # æ—¥æœŸæ—¶é—´ï¼ˆæ—¶é—´æˆ³ï¼‰/ Date and time (timestamps)
from enum import Enum            # æšä¸¾ç±»å‹ / Enumeration types

# å¯¼å…¥åŸºçº¿æ–¹æ³•æ¨¡å— / Import baseline method modules
# è¿™äº›æ˜¯ç”¨äºå¯¹æ¯”çš„åŸºçº¿æ±‚è§£æ–¹æ³• / These are baseline solving methods for comparison
from baselines import DirectLLM, ZeroShotCoT, FewShotCoT


class EvaluationMethod(Enum):
    """Enumeration of supported evaluation methods.
    æ”¯æŒçš„è¯„ä¼°æ–¹æ³•æšä¸¾ã€‚

    Baselines / åŸºçº¿æ–¹æ³•
    - DIRECT_LLM: Ask the LLM to answer directly.
      ç›´æ¥LLMï¼šç›´æ¥è®©LLMå›ç­”é—®é¢˜ï¼Œæ— æ¨ç†è¿‡ç¨‹ã€‚
    - ZERO_SHOT_COT: Zeroâ€‘shot chainâ€‘ofâ€‘thought prompting.
      é›¶æ ·æœ¬CoTï¼šé›¶æ ·æœ¬æ€ç»´é“¾æç¤ºï¼Œè®©LLMé€æ­¥æ¨ç†ã€‚
    - FEW_SHOT_COT: Fewâ€‘shot chainâ€‘ofâ€‘thought prompting.
      å°‘æ ·æœ¬CoTï¼šå°‘æ ·æœ¬æ€ç»´é“¾æç¤ºï¼Œæä¾›ç¤ºä¾‹å¼•å¯¼æ¨ç†ã€‚

    Framework Variants / æ¡†æ¶å˜ä½“
    - FULL_FRAMEWORK: Full fourâ€‘stage pipeline (retrieval â†’ scaffold â†’ compute â†’ synthesize).
      å®Œæ•´æ¡†æ¶ï¼šå››é˜¶æ®µå®Œæ•´æµç¨‹ï¼ˆæ£€ç´¢ â†’ è„šæ‰‹æ¶ â†’ è®¡ç®— â†’ åˆæˆï¼‰ã€‚

    Ablations / æ¶ˆèå®éªŒ
    - NO_RETRIEVER: Disable knowledge retriever.
      æ— æ£€ç´¢å™¨ï¼šç¦ç”¨çŸ¥è¯†æ£€ç´¢å™¨ï¼ˆä¼ ç»Ÿå’ŒAIæ£€ç´¢å™¨éƒ½ç¦ç”¨ï¼‰ã€‚
    - NO_AI_RETRIEVER: Use only traditional retriever (no AI rule generation).
      æ— AIæ£€ç´¢å™¨ï¼šä»…ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢å™¨ï¼ˆä¸ä½¿ç”¨AIç”Ÿæˆè§„åˆ™ï¼‰ã€‚
    - NO_SYMBOLIC_EXECUTION: Use LLM computation instead of symbolic execution.
      æ— ç¬¦å·æ‰§è¡Œï¼šä½¿ç”¨LLMè®¡ç®—è€Œéç¬¦å·æ‰§è¡Œã€‚
    - NO_VALIDATION: Skip synthesis/validation.
      æ— éªŒè¯ï¼šè·³è¿‡åˆæˆ/éªŒè¯é˜¶æ®µã€‚
    """
    # åŸºçº¿æ–¹æ³• / Baselines
    DIRECT_LLM = "direct_llm"              # ç›´æ¥LLM / Direct LLM
    ZERO_SHOT_COT = "zero_shot_cot"        # é›¶æ ·æœ¬CoT / Zero-shot CoT
    FEW_SHOT_COT = "few_shot_cot"          # å°‘æ ·æœ¬CoT / Few-shot CoT

    # æˆ‘ä»¬çš„æ¡†æ¶ / Our Framework
    FULL_FRAMEWORK = "full_framework"      # å®Œæ•´æ¡†æ¶ / Full framework

    # æ¶ˆèå®éªŒ / Ablations
    NO_RETRIEVER = "no_retriever"                      # æ— æ£€ç´¢å™¨ / No retriever
    NO_AI_RETRIEVER = "no_ai_retriever"                # æ— AIæ£€ç´¢å™¨ / No AI retriever
    NO_SYMBOLIC_EXECUTION = "no_symbolic_execution"    # æ— ç¬¦å·æ‰§è¡Œ / No symbolic execution
    NO_VALIDATION = "no_validation"                    # æ— éªŒè¯ / No validation


@dataclass
class EvaluationResult:
    """Perâ€‘problem evaluation outcome.
    å•ä¸ªé—®é¢˜çš„è¯„ä¼°ç»“æœã€‚

    Fields / å­—æ®µ
    - problem_id: Unique identifier of the problem.
      é—®é¢˜IDï¼šé—®é¢˜çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
    - method: Evaluation method used (string value of EvaluationMethod).
      æ–¹æ³•ï¼šä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•ï¼ˆEvaluationMethodçš„å­—ç¬¦ä¸²å€¼ï¼‰ã€‚
    - problem_text: Original problem text.
      é—®é¢˜æ–‡æœ¬ï¼šåŸå§‹é—®é¢˜æ–‡æœ¬ã€‚
    - expected_answer: Groundâ€‘truth answer from dataset.
      é¢„æœŸç­”æ¡ˆï¼šæ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆã€‚
    - predicted_answer: Model/framework predicted answer (stringified).
      é¢„æµ‹ç­”æ¡ˆï¼šæ¨¡å‹/æ¡†æ¶é¢„æµ‹çš„ç­”æ¡ˆï¼ˆå­—ç¬¦ä¸²åŒ–ï¼‰ã€‚
    - is_correct: Whether predicted matches expected (via comparator).
      æ˜¯å¦æ­£ç¡®ï¼šé¢„æµ‹ç­”æ¡ˆæ˜¯å¦ä¸é¢„æœŸç­”æ¡ˆåŒ¹é…ï¼ˆé€šè¿‡æ¯”è¾ƒå™¨åˆ¤æ–­ï¼‰ã€‚
    - execution_time: Wallâ€‘clock time spent to produce prediction.
      æ‰§è¡Œæ—¶é—´ï¼šç”Ÿæˆé¢„æµ‹æ‰€èŠ±è´¹çš„å®é™…æ—¶é—´ï¼ˆç§’ï¼‰ã€‚
    - error: Error message if any stage failed.
      é”™è¯¯ä¿¡æ¯ï¼šå¦‚æœä»»ä½•é˜¶æ®µå¤±è´¥çš„é”™è¯¯æ¶ˆæ¯ã€‚
    - reasoning_steps: Optional reasoning text for CoT methods.
      æ¨ç†æ­¥éª¤ï¼šCoTæ–¹æ³•çš„æ¨ç†æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰ã€‚
    - causal_scaffold: Optional scaffold for visualization/debugging.
      å› æœè„šæ‰‹æ¶ï¼šç”¨äºå¯è§†åŒ–/è°ƒè¯•çš„è„šæ‰‹æ¶ç»“æ„ï¼ˆå¯é€‰ï¼‰ã€‚
    """
    problem_id: str                          # é—®é¢˜å”¯ä¸€æ ‡è¯†ç¬¦ / Problem unique identifier
    method: str                              # è¯„ä¼°æ–¹æ³•åç§° / Evaluation method name
    problem_text: str                        # åŸå§‹é—®é¢˜æ–‡æœ¬ / Original problem text
    expected_answer: str                     # æ ‡å‡†ç­”æ¡ˆ / Ground-truth answer
    predicted_answer: Optional[str]          # é¢„æµ‹ç­”æ¡ˆ / Predicted answer
    is_correct: bool                         # æ˜¯å¦æ­£ç¡® / Whether correct
    execution_time: float                    # æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰/ Execution time (seconds)
    error: Optional[str]                     # é”™è¯¯ä¿¡æ¯ / Error message
    reasoning_steps: Optional[str] = None    # æ¨ç†æ­¥éª¤ / Reasoning steps
    causal_scaffold: Optional[Dict[str, Any]] = None  # å› æœè„šæ‰‹æ¶ / Causal scaffold


class DatasetLoader:
    """
    Dataset Loader for multiple formats
    å¤šæ ¼å¼æ•°æ®é›†åŠ è½½å™¨
    
    This class provides static methods to load different math reasoning datasets.
    è¯¥ç±»æä¾›é™æ€æ–¹æ³•æ¥åŠ è½½ä¸åŒçš„æ•°å­¦æ¨ç†æ•°æ®é›†ã€‚
    
    Supported datasets / æ”¯æŒçš„æ•°æ®é›†ï¼š
    - GSM8K: Grade school math problems / å°å­¦æ•°å­¦é—®é¢˜
    - MATH: Competition-level math problems / ç«èµ›çº§æ•°å­¦é—®é¢˜
    - MyData: Custom dataset / è‡ªå®šä¹‰æ•°æ®é›†
    - Omni-MATH: Comprehensive math reasoning / ç»¼åˆæ•°å­¦æ¨ç†
    - OlympiadBench: Olympiad-level problems (multi-modal support) / å¥¥æ—åŒ¹å…‹çº§é—®é¢˜ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
    """

    @staticmethod
    def load_gsm8k(file_path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Load GSM8K dataset / åŠ è½½ GSM8K æ•°æ®é›†
        
        GSM8K is a dataset of grade school math problems in JSONL format.
        GSM8K æ˜¯å°å­¦æ•°å­¦é—®é¢˜æ•°æ®é›†ï¼ŒJSONL æ ¼å¼ã€‚
        
        Each line contains:
        æ¯è¡ŒåŒ…å«ï¼š
        - question: The math problem / æ•°å­¦é—®é¢˜
        - answer: Solution with final answer after '####' / è§£ç­”ï¼ˆ'####'åé¢æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
        
        Args:
            file_path: Path to JSONL file / JSONL æ–‡ä»¶è·¯å¾„
            limit: Maximum number of problems to load / æœ€å¤šåŠ è½½çš„é—®é¢˜æ•°é‡
            
        Returns:
            List of problem dictionaries / é—®é¢˜å­—å…¸åˆ—è¡¨
        """
        problems = []  # å­˜å‚¨é—®é¢˜åˆ—è¡¨ / Store problem list
        with open(file_path, 'r', encoding='utf-8') as f:
            # é€è¡Œè¯»å–JSONLæ–‡ä»¶ / Read JSONL file line by line
            for i, line in enumerate(f):
                # å¦‚æœè¾¾åˆ°é™åˆ¶æ•°é‡åˆ™åœæ­¢ / Stop if limit reached
                if limit and i >= limit:
                    break
                # è§£æJSONè¡Œ / Parse JSON line
                data = json.loads(line.strip())

                # æå–æœ€ç»ˆç­”æ¡ˆï¼ˆåœ¨ #### ä¹‹åï¼‰/ Extract final answer (after ####)
                answer_text = data['answer']
                # åˆ†å‰²å¹¶è·å– #### åçš„ç­”æ¡ˆ / Split and get answer after ####
                final_answer = answer_text.split('####')[-1].strip() if '####' in answer_text else answer_text

                # æ·»åŠ åˆ°é—®é¢˜åˆ—è¡¨ / Add to problem list
                problems.append({
                    'id': f'gsm8k_{i}',           # é—®é¢˜ID / Problem ID
                    'question': data['question'],  # é—®é¢˜æ–‡æœ¬ / Question text
                    'answer': final_answer,        # æœ€ç»ˆç­”æ¡ˆ / Final answer
                    'full_solution': answer_text   # å®Œæ•´è§£ç­” / Full solution
                })

        return problems  # è¿”å›é—®é¢˜åˆ—è¡¨ / Return problem list 

    @staticmethod
    def load_math(file_path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Load MATH dataset / åŠ è½½ MATH æ•°æ®é›†
        
        MATH is a competition-level math problem dataset in JSON format.
        MATH æ˜¯ç«èµ›çº§æ•°å­¦é—®é¢˜æ•°æ®é›†ï¼ŒJSON æ ¼å¼ã€‚
        
        Each item contains:
        æ¯ä¸ªé¡¹ç›®åŒ…å«ï¼š
        - problem: The math problem / æ•°å­¦é—®é¢˜
        - answer: The final answer / æœ€ç»ˆç­”æ¡ˆ
        - solution: Step-by-step solution / é€æ­¥è§£ç­”
        - subject: Math subject (e.g., algebra, geometry) / æ•°å­¦ç§‘ç›®ï¼ˆå¦‚ä»£æ•°ã€å‡ ä½•ï¼‰
        - level: Difficulty level / éš¾åº¦ç­‰çº§
        
        Args:
            file_path: Path to JSON file / JSON æ–‡ä»¶è·¯å¾„
            limit: Maximum number of problems to load / æœ€å¤šåŠ è½½çš„é—®é¢˜æ•°é‡
            
        Returns:
            List of problem dictionaries / é—®é¢˜å­—å…¸åˆ—è¡¨
        """
        # åŠ è½½JSONæ–‡ä»¶ / Load JSON file
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # å¦‚æœæŒ‡å®šäº†é™åˆ¶ï¼Œåˆ™æˆªå– / Apply limit if specified
        if limit:
            data = data[:limit]

        problems = []  # å­˜å‚¨é—®é¢˜åˆ—è¡¨ / Store problem list
        # éå†æ¯ä¸ªé—®é¢˜ / Iterate through each problem
        for item in data:
            problems.append({
                'id': item.get('unique_id', f"math_{len(problems)}"),  # é—®é¢˜ID / Problem ID
                'question': item['problem'],      # é—®é¢˜æ–‡æœ¬ / Question text
                'answer': item['answer'],         # ç­”æ¡ˆ / Answer
                'solution': item.get('solution', ''),  # è§£ç­” / Solution
                'subject': item.get('subject', ''),    # ç§‘ç›® / Subject
                'level': item.get('level', '')         # éš¾åº¦ / Level
            })

        return problems  # è¿”å›é—®é¢˜åˆ—è¡¨ / Return problem list 

    @staticmethod
    def load_mydata(file_path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Load MyData dataset / åŠ è½½ MyData æ•°æ®é›†
        
        MyData is a custom dataset with flexible structure.
        MyData æ˜¯å…·æœ‰çµæ´»ç»“æ„çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚
        
        Features / ç‰¹ç‚¹ï¼š
        - Supports list or string for final_answer / æ”¯æŒåˆ—è¡¨æˆ–å­—ç¬¦ä¸²æ ¼å¼çš„æœ€ç»ˆç­”æ¡ˆ
        - Solution is stored as list of steps / è§£ç­”å­˜å‚¨ä¸ºæ­¥éª¤åˆ—è¡¨
        - Includes subfield and context metadata / åŒ…å«å­é¢†åŸŸå’Œä¸Šä¸‹æ–‡å…ƒæ•°æ®
        
        Args:
            file_path: Path to JSON file / JSON æ–‡ä»¶è·¯å¾„
            limit: Maximum number of problems to load / æœ€å¤šåŠ è½½çš„é—®é¢˜æ•°é‡
            
        Returns:
            List of problem dictionaries / é—®é¢˜å­—å…¸åˆ—è¡¨
        """
        # åŠ è½½JSONæ–‡ä»¶ / Load JSON file
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # å¦‚æœæŒ‡å®šäº†é™åˆ¶ï¼Œåˆ™æˆªå– / Apply limit if specified
        if limit:
            data = data[:limit]

        problems = []  # å­˜å‚¨é—®é¢˜åˆ—è¡¨ / Store problem list
        # éå†æ¯ä¸ªé—®é¢˜ / Iterate through each problem
        for item in data:
            # å¤„ç†æœ€ç»ˆç­”æ¡ˆï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²ï¼‰/ Handle final answer (may be list or string)
            final_answer = item['final_answer']
            if isinstance(final_answer, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´  / If list, take first element
                final_answer = final_answer[0] if final_answer else ""

            # å°†è§£ç­”åˆ—è¡¨åˆå¹¶ä¸ºæ–‡æœ¬ / Join solution list into text
            solution_text = '\n'.join(item.get('solution', []))

            problems.append({
                'id': f"mydata_{item['id']}",     # é—®é¢˜ID / Problem ID
                'question': item['question'],     # é—®é¢˜æ–‡æœ¬ / Question text
                'answer': final_answer,           # æœ€ç»ˆç­”æ¡ˆ / Final answer
                'solution': solution_text,        # è§£ç­”æ­¥éª¤ / Solution steps
                'subfield': item.get('subfield', ''),  # å­é¢†åŸŸ / Subfield
                'context': item.get('context', '')     # ä¸Šä¸‹æ–‡ä¿¡æ¯ / Context information
            })

        return problems  # è¿”å›é—®é¢˜åˆ—è¡¨ / Return problem list

    @staticmethod
    def load_omnimath(file_path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Load Omni-MATH dataset / åŠ è½½ Omni-MATH æ•°æ®é›†

        Omni-MATH is a comprehensive math reasoning dataset in JSONL format.
        Format is similar to GSM8K with 'question' and 'answer' fields.
        Omni-MATH æ˜¯ä¸€ä¸ªå…¨é¢çš„æ•°å­¦æ¨ç†æ•°æ®é›†ï¼ŒJSONL æ ¼å¼ã€‚
        æ ¼å¼ç±»ä¼¼ GSM8Kï¼ŒåŒ…å« 'question' å’Œ 'answer' å­—æ®µã€‚

        Features / ç‰¹ç‚¹ï¼š
        - Comprehensive coverage of math topics / å…¨é¢è¦†ç›–æ•°å­¦ä¸»é¢˜
        - Answer format uses '####' separator like GSM8K / ç­”æ¡ˆæ ¼å¼åƒGSM8Kä¸€æ ·ä½¿ç”¨'####'åˆ†éš”ç¬¦

        Args:
            file_path: Path to Omni-MATH JSONL file
                      Omni-MATH JSONL æ–‡ä»¶è·¯å¾„
            limit: Maximum number of problems to load
                   æœ€å¤šåŠ è½½çš„é—®é¢˜æ•°é‡

        Returns:
            List of problem dictionaries
            é—®é¢˜å­—å…¸åˆ—è¡¨
        """
        problems = []  # å­˜å‚¨é—®é¢˜åˆ—è¡¨ / Store problem list
        with open(file_path, 'r', encoding='utf-8') as f:
            # é€è¡Œè¯»å–JSONLæ–‡ä»¶ / Read JSONL file line by line
            for i, line in enumerate(f):
                # å¦‚æœè¾¾åˆ°é™åˆ¶æ•°é‡åˆ™åœæ­¢ / Stop if limit reached
                if limit and i >= limit:
                    break

                # è§£æJSONè¡Œ / Parse JSON line
                data = json.loads(line.strip())

                # Extract final answer from "#### answer" format
                # ä» "#### answer" æ ¼å¼ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
                answer_text = data.get('answer', '')
                final_answer = answer_text.split('####')[-1].strip() if '####' in answer_text else answer_text

                # æ·»åŠ åˆ°é—®é¢˜åˆ—è¡¨ / Add to problem list
                problems.append({
                    'id': f'omnimath_{i}',        # é—®é¢˜ID / Problem ID
                    'question': data['question'],  # é—®é¢˜æ–‡æœ¬ / Question text
                    'answer': final_answer,        # æœ€ç»ˆç­”æ¡ˆ / Final answer
                    'full_solution': answer_text   # å®Œæ•´è§£ç­” / Full solution
                })

        return problems  # è¿”å›é—®é¢˜åˆ—è¡¨ / Return problem list

    @staticmethod
    # OlympiadBench loader: supports text-only and multi-modal variants
    def load_olympiadbench(
        file_path: str,
        limit: Optional[int] = None,
        filter_multimodal: Optional[bool] = None
    ) -> List[Dict[str, Any]]:
        """
        Load OlympiadBench dataset / åŠ è½½ OlympiadBench æ•°æ®é›†

        OlympiadBench is an Olympiad-level math and physics dataset supporting multi-modal problems.
        Some problems contain images marked as <img_XXXX> in the question text.

        OlympiadBench æ˜¯å¥¥æ—åŒ¹å…‹çº§åˆ«çš„æ•°å­¦å’Œç‰©ç†æ•°æ®é›†ï¼Œæ”¯æŒå¤šæ¨¡æ€é—®é¢˜ã€‚
        éƒ¨åˆ†é—®é¢˜åœ¨é—®é¢˜æ–‡æœ¬ä¸­åŒ…å« <img_XXXX> æ ‡è®°çš„å›¾ç‰‡ã€‚

        File naming convention: {ProblemType}_{Modality}_{Subject}_{Language}_{Exam}.json
        - ProblemType: TP (Theorem Proving) or OE (Open-Ended)
        - Modality: TO (Text-Only) or MM (Multi-Modal)
        - Subject: maths or physics
        - Language: en or zh
        - Exam: COMP (Competition) or CEE (College Entrance Exam)

        æ–‡ä»¶å‘½åè§„åˆ™: {é—®é¢˜ç±»å‹}_{æ¨¡æ€}_{å­¦ç§‘}_{è¯­è¨€}_{è€ƒè¯•}.json
        - é—®é¢˜ç±»å‹: TP (å®šç†è¯æ˜) æˆ– OE (å¼€æ”¾å¼)
        - æ¨¡æ€: TO (çº¯æ–‡æœ¬) æˆ– MM (å¤šæ¨¡æ€)
        - å­¦ç§‘: maths (æ•°å­¦) æˆ– physics (ç‰©ç†)
        - è¯­è¨€: en (è‹±è¯­) æˆ– zh (ä¸­æ–‡)
        - è€ƒè¯•: COMP (ç«èµ›) æˆ– CEE (é«˜è€ƒ)

        Args:
            file_path: Path to OlympiadBench JSON file
                      OlympiadBench JSON æ–‡ä»¶è·¯å¾„
            limit: Maximum number of problems to load
                   æœ€å¤šåŠ è½½çš„é—®é¢˜æ•°é‡
            filter_multimodal: If True, only load multi-modal problems;
                             If False, only load text-only problems;
                             If None, load all problems
                             å¦‚æœä¸º Trueï¼ŒåªåŠ è½½å¤šæ¨¡æ€é—®é¢˜ï¼›
                             å¦‚æœä¸º Falseï¼ŒåªåŠ è½½çº¯æ–‡æœ¬é—®é¢˜ï¼›
                             å¦‚æœä¸º Noneï¼ŒåŠ è½½æ‰€æœ‰é—®é¢˜

        Returns:
            List of problem dictionaries with multi-modal metadata
            åŒ…å«å¤šæ¨¡æ€å…ƒæ•°æ®çš„é—®é¢˜å­—å…¸åˆ—è¡¨
        """
        # Load JSON data
        # åŠ è½½ JSON æ•°æ®
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Parse file name to extract metadata
        # è§£ææ–‡ä»¶åä»¥æå–å…ƒæ•°æ®ï¼ˆä»æ–‡ä»¶åä¸­æå–æ•°æ®é›†ç±»å‹ä¿¡æ¯ï¼‰
        file_name = Path(file_path).stem  # ä¾‹å¦‚ï¼š"TP_MM_maths_en_COMP" / e.g., "TP_MM_maths_en_COMP"
        parts = file_name.split('_')  # æŒ‰ä¸‹åˆ’çº¿åˆ†å‰² / Split by underscore
        if len(parts) >= 5:
            problem_type = parts[0]  # TPï¼ˆå®šç†è¯æ˜ï¼‰æˆ– OEï¼ˆå¼€æ”¾å¼ï¼‰/ TP (Theorem Proving) or OE (Open-Ended)
            modality = parts[1]      # TOï¼ˆçº¯æ–‡æœ¬ï¼‰æˆ– MMï¼ˆå¤šæ¨¡æ€ï¼‰/ TO (Text-Only) or MM (Multi-Modal)
            subject = parts[2]       # mathsï¼ˆæ•°å­¦ï¼‰æˆ– physicsï¼ˆç‰©ç†ï¼‰/ maths or physics
            language = parts[3]      # enï¼ˆè‹±è¯­ï¼‰æˆ– zhï¼ˆä¸­æ–‡ï¼‰/ en (English) or zh (Chinese)
            exam_type = parts[4]     # COMPï¼ˆç«èµ›ï¼‰æˆ– CEEï¼ˆé«˜è€ƒï¼‰/ COMP (Competition) or CEE (College Entrance Exam)
        else:
            # å¦‚æœæ–‡ä»¶åæ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œä½¿ç”¨é»˜è®¤å€¼ / If filename format is unexpected, use defaults
            problem_type = modality = subject = language = exam_type = "unknown"

        problems = []  # å­˜å‚¨é—®é¢˜åˆ—è¡¨ / Store problem list
        for item in data:
            # å¦‚æœè¾¾åˆ°é™åˆ¶æ•°é‡åˆ™åœæ­¢ / Stop if limit reached
            if limit and len(problems) >= limit:
                break

            # Check for images in question text
            # æ£€æŸ¥é—®é¢˜æ–‡æœ¬ä¸­æ˜¯å¦æœ‰å›¾ç‰‡ï¼ˆå¤šæ¨¡æ€é—®é¢˜æ£€æµ‹ï¼‰
            question_text = item.get('question', '')
            image_pattern = r'<img_(\d+)>'  # å›¾ç‰‡æ ‡è®°æ¨¡å¼ / Image marker pattern
            image_matches = re.findall(image_pattern, question_text)  # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡è®° / Find all image markers
            has_images = len(image_matches) > 0  # æ˜¯å¦åŒ…å«å›¾ç‰‡ / Whether contains images

            # Apply multi-modal filter if specified
            # åº”ç”¨å¤šæ¨¡æ€è¿‡æ»¤å™¨ï¼ˆå¦‚æœæŒ‡å®šï¼‰
            if filter_multimodal is not None:
                if filter_multimodal and not has_images:
                    continue  # è·³è¿‡çº¯æ–‡æœ¬é—®é¢˜ / Skip text-only problems
                if not filter_multimodal and has_images:
                    continue  # è·³è¿‡å¤šæ¨¡æ€é—®é¢˜ / Skip multi-modal problems

            # Extract solution text
            # æå–è§£ç­”æ–‡æœ¬ï¼ˆå°†åˆ—è¡¨æ ¼å¼çš„è§£ç­”åˆå¹¶ä¸ºå­—ç¬¦ä¸²ï¼‰
            solution_list = item.get('solution', [])
            if isinstance(solution_list, list):
                solution_text = '\n\n'.join(solution_list)  # ç”¨åŒæ¢è¡Œè¿æ¥æ­¥éª¤ / Join steps with double newline
            else:
                solution_text = str(solution_list)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸² / Convert to string

            # Extract final answer (may be None for proof problems)
            # æå–æœ€ç»ˆç­”æ¡ˆï¼ˆè¯æ˜é¢˜å¯èƒ½ä¸º Noneï¼‰
            final_answer = item.get('final_answer', None)
            if final_answer is None or final_answer == "":
                # For proof problems, use a placeholder
                # å¯¹äºè¯æ˜é¢˜ï¼Œä½¿ç”¨å ä½ç¬¦ï¼ˆå› ä¸ºè¯æ˜é¢˜æ²¡æœ‰æ•°å€¼ç­”æ¡ˆï¼‰
                final_answer = "[PROOF_REQUIRED]"

            # æ·»åŠ åˆ°é—®é¢˜åˆ—è¡¨ï¼ˆåŒ…å«ä¸°å¯Œçš„å…ƒæ•°æ®ï¼‰/ Add to problem list (with rich metadata)
            problems.append({
                'id': f"olympiad_{item.get('id', len(problems))}",  # é—®é¢˜ID / Problem ID
                'question': question_text,        # é—®é¢˜æ–‡æœ¬ / Question text
                'answer': str(final_answer),      # æœ€ç»ˆç­”æ¡ˆ / Final answer
                'solution': solution_text,        # è§£ç­”æ­¥éª¤ / Solution steps
                'subfield': item.get('subfield', ''),  # å­é¢†åŸŸ / Subfield
                'context': item.get('context', ''),     # ä¸Šä¸‹æ–‡ / Context
                # Multi-modal metadata / å¤šæ¨¡æ€å…ƒæ•°æ®
                'has_images': has_images,         # æ˜¯å¦åŒ…å«å›¾ç‰‡ / Whether contains images
                'image_ids': image_matches if has_images else [],  # å›¾ç‰‡IDåˆ—è¡¨ / List of image IDs
                'image_count': len(image_matches),  # å›¾ç‰‡æ•°é‡ / Number of images
                # Dataset metadata / æ•°æ®é›†å…ƒæ•°æ®
                'problem_type': problem_type,     # é—®é¢˜ç±»å‹ï¼ˆTP/OEï¼‰/ Problem type (TP/OE)
                'modality': modality,             # æ¨¡æ€ï¼ˆTO/MMï¼‰/ Modality (TO/MM)
                'subject': subject,               # å­¦ç§‘ï¼ˆæ•°å­¦/ç‰©ç†ï¼‰/ Subject (maths/physics)
                'language': language,             # è¯­è¨€ï¼ˆè‹±è¯­/ä¸­æ–‡ï¼‰/ Language (en/zh)
                'exam_type': exam_type,           # è€ƒè¯•ç±»å‹ï¼ˆç«èµ›/é«˜è€ƒï¼‰/ Exam type (COMP/CEE)
                'is_multiple_answer': item.get('is_multiple_answer', False),  # æ˜¯å¦å¤šç­”æ¡ˆ / Whether multiple answers
                'answer_type': item.get('answer_type', None),  # ç­”æ¡ˆç±»å‹ / Answer type
                'unit': item.get('unit', None)    # å•ä½ / Unit
            })

        # Print summary / æ‰“å°æ‘˜è¦ï¼ˆæ˜¾ç¤ºåŠ è½½çš„æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ï¼‰
        if problems:
            mm_count = sum(1 for p in problems if p['has_images'])  # ç»Ÿè®¡å¤šæ¨¡æ€é—®é¢˜æ•° / Count multi-modal problems
            to_count = len(problems) - mm_count  # ç»Ÿè®¡çº¯æ–‡æœ¬é—®é¢˜æ•° / Count text-only problems
            print(f"\nğŸ“Š OlympiadBench Dataset Loaded / OlympiadBench æ•°æ®é›†å·²åŠ è½½:")
            print(f"  Total problems: {len(problems)} / æ€»é—®é¢˜æ•°: {len(problems)}")
            print(f"  Multi-modal (with images): {mm_count} / å¤šæ¨¡æ€ï¼ˆå«å›¾ç‰‡ï¼‰: {mm_count}")
            print(f"  Text-only: {to_count} / çº¯æ–‡æœ¬: {to_count}")
            print(f"  Subject: {subject} | Language: {language} | Type: {problem_type}\n")

        return problems  # è¿”å›é—®é¢˜åˆ—è¡¨ / Return problem list


class BaselineEvaluator:
    """
    Baseline methods evaluator
    åŸºçº¿æ–¹æ³•è¯„ä¼°å™¨

    This class now uses modular baseline implementations from the baselines package.
    è¯¥ç±»ä½¿ç”¨ baselines åŒ…ä¸­çš„æ¨¡å—åŒ–åŸºçº¿å®ç°ã€‚
    
    Purpose / ç›®çš„ï¼š
    - Provides baseline solving methods for comparison / æä¾›ç”¨äºå¯¹æ¯”çš„åŸºçº¿æ±‚è§£æ–¹æ³•
    - Wraps three standard approaches: Direct LLM, Zero-shot CoT, Few-shot CoT
      å°è£…ä¸‰ç§æ ‡å‡†æ–¹æ³•ï¼šç›´æ¥LLMã€é›¶æ ·æœ¬CoTã€å°‘æ ·æœ¬CoT
    - Extracts final answers from LLM responses / ä»LLMå“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
    """

    def __init__(self, llm_client=None):
        """Initialize baseline evaluator / åˆå§‹åŒ–åŸºçº¿è¯„ä¼°å™¨
        
        Args:
            llm_client: Optional LLM client instance. If not provided, creates a default one.
                       å¯é€‰çš„LLMå®¢æˆ·ç«¯å®ä¾‹ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™åˆ›å»ºé»˜è®¤å®ä¾‹ã€‚
        """
        # åˆå§‹åŒ–æˆ–ä½¿ç”¨æä¾›çš„ LLM å®¢æˆ·ç«¯ / Initialize or use provided LLM client
        if llm_client is None:
            from engine.scaffolder import LLMClient
            llm_client = LLMClient()

        # ä¿å­˜ LLM å®¢æˆ·ç«¯å¼•ç”¨ / Save LLM client reference
        self.llm_client = llm_client

        # Initialize baseline solvers / åˆå§‹åŒ–åŸºçº¿æ±‚è§£å™¨
        # è¿™äº›æ±‚è§£å™¨éƒ½ä½¿ç”¨ç›¸åŒçš„ LLM å®¢æˆ·ç«¯ / These solvers all use the same LLM client
        self.direct_llm_solver = DirectLLM(llm_client=llm_client)        # ç›´æ¥LLMæ±‚è§£å™¨ / Direct LLM solver
        self.zero_shot_cot_solver = ZeroShotCoT(llm_client=llm_client)   # é›¶æ ·æœ¬CoTæ±‚è§£å™¨ / Zero-shot CoT solver
        self.few_shot_cot_solver = FewShotCoT(llm_client=llm_client)     # å°‘æ ·æœ¬CoTæ±‚è§£å™¨ / Few-shot CoT solver 

    def direct_llm(self, problem: str) -> str:
        """
        Direct LLM answer / ç›´æ¥LLMå›ç­”

        Uses DirectLLM baseline from baselines/direct_llm.py
        ä½¿ç”¨ baselines/direct_llm.py ä¸­çš„ DirectLLM åŸºçº¿æ–¹æ³•
        
        Approach / æ–¹æ³•ï¼š
        - Directly asks LLM to solve the problem without reasoning steps
          ç›´æ¥è®©LLMè§£å†³é—®é¢˜ï¼Œä¸æä¾›æ¨ç†æ­¥éª¤
        - Fastest but least accurate baseline / æœ€å¿«ä½†æœ€ä¸å‡†ç¡®çš„åŸºçº¿
        
        Args:
            problem: The math problem text / æ•°å­¦é—®é¢˜æ–‡æœ¬
            
        Returns:
            The predicted answer / é¢„æµ‹çš„ç­”æ¡ˆ
        """
        return self.direct_llm_solver.solve(problem)

    def zero_shot_cot(self, problem: str) -> Tuple[str, str]:
        """
        Zero-shot Chain of Thought / é›¶æ ·æœ¬æ€ç»´é“¾

        Uses ZeroShotCoT baseline from baselines/zero_shot_cot.py
        ä½¿ç”¨ baselines/zero_shot_cot.py ä¸­çš„ ZeroShotCoT åŸºçº¿æ–¹æ³•
        
        Approach / æ–¹æ³•ï¼š
        - Prompts LLM to "think step by step" without examples
          æç¤ºLLM"é€æ­¥æ€è€ƒ"ï¼Œä¸æä¾›ç¤ºä¾‹
        - Generates reasoning before answering / åœ¨å›ç­”å‰ç”Ÿæˆæ¨ç†è¿‡ç¨‹
        
        Args:
            problem: The math problem text / æ•°å­¦é—®é¢˜æ–‡æœ¬
            
        Returns:
            Tuple of (predicted_answer, reasoning_steps)
            è¿”å›å…ƒç»„ï¼š(é¢„æµ‹ç­”æ¡ˆ, æ¨ç†æ­¥éª¤)
        """
        return self.zero_shot_cot_solver.solve(problem)

    def few_shot_cot(self, problem: str) -> Tuple[str, str]:
        """
        Few-shot Chain of Thought / å°‘æ ·æœ¬æ€ç»´é“¾

        Uses FewShotCoT baseline from baselines/few_shot_cot.py
        ä½¿ç”¨ baselines/few_shot_cot.py ä¸­çš„ FewShotCoT åŸºçº¿æ–¹æ³•
        
        Approach / æ–¹æ³•ï¼š
        - Provides few examples of step-by-step reasoning
          æä¾›å°‘é‡é€æ­¥æ¨ç†çš„ç¤ºä¾‹
        - LLM follows the pattern to solve new problems / LLMéµå¾ªæ¨¡å¼è§£å†³æ–°é—®é¢˜
        - Most accurate baseline but requires examples / æœ€å‡†ç¡®çš„åŸºçº¿ä½†éœ€è¦ç¤ºä¾‹
        
        Args:
            problem: The math problem text / æ•°å­¦é—®é¢˜æ–‡æœ¬
            
        Returns:
            Tuple of (predicted_answer, reasoning_steps)
            è¿”å›å…ƒç»„ï¼š(é¢„æµ‹ç­”æ¡ˆ, æ¨ç†æ­¥éª¤)
        """
        return self.few_shot_cot_solver.solve(problem)

    def _extract_answer(self, response: str) -> str:
        """
        Extract final answer from LLM response / ä»LLMå“åº”ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
        
        This method tries multiple patterns to extract numerical or text answers.
        è¯¥æ–¹æ³•å°è¯•å¤šç§æ¨¡å¼æ¥æå–æ•°å€¼æˆ–æ–‡æœ¬ç­”æ¡ˆã€‚
        
        Patterns tried / å°è¯•çš„æ¨¡å¼ï¼š
        1. "ç­”æ¡ˆï¼š" or "Answer:" followed by answer / "ç­”æ¡ˆï¼š"æˆ–"Answer:"åè·Ÿç­”æ¡ˆ
        2. "=" followed by number / "="åè·Ÿæ•°å­—
        3. Number at end of line / è¡Œå°¾çš„æ•°å­—
        4. Last line of response (fallback) / å“åº”çš„æœ€åä¸€è¡Œï¼ˆå¤‡ç”¨ï¼‰
        
        Args:
            response: LLM response text / LLMå“åº”æ–‡æœ¬
            
        Returns:
            Extracted answer / æå–çš„ç­”æ¡ˆ
        """
        import re
        
        # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…ç­”æ¡ˆ / Try multiple patterns to match answer
        patterns = [
            r'(?:ç­”æ¡ˆ|Answer|Final answer)[:ï¼š]\s*([^\n]+)',  # "ç­”æ¡ˆ:" or "Answer:" pattern
            r'=\s*([0-9\.]+)',           # "= number" pattern / "= æ•°å­—" æ¨¡å¼
            r'([0-9\.]+)\s*$'            # è¡Œå°¾æ•°å­— / Number at end of line
        ]
        
        for pattern in patterns:
            match = re.search(pattern, response, re.IGNORECASE)
            if match:
                return match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè¿”å›æœ€åä¸€è¡Œä½œä¸ºç­”æ¡ˆ / If no match, return last line as answer
        lines = response.strip().split('\n')
        return lines[-1].strip() if lines else response.strip()


class FrameworkEvaluator:
    """Evaluator for baselines, full framework, and ablations.
    åŸºçº¿ã€å®Œæ•´æ¡†æ¶å’Œæ¶ˆèå®éªŒçš„è¯„ä¼°å™¨ã€‚

    Responsibilities / èŒè´£ï¼š
    - Route a single problem to the requested method
      å°†å•ä¸ªé—®é¢˜è·¯ç”±åˆ°è¯·æ±‚çš„æ–¹æ³•
    - Compare predicted vs expected answers (LLM aided fallback)
      æ¯”è¾ƒé¢„æµ‹ç­”æ¡ˆå’Œé¢„æœŸç­”æ¡ˆï¼ˆLLMè¾…åŠ©å¤‡ç”¨ï¼‰
    - Aggregate perâ€‘method statistics across a dataset
      æ±‡æ€»æ•°æ®é›†ä¸­æ¯ç§æ–¹æ³•çš„ç»Ÿè®¡ä¿¡æ¯
    
    Key Features / ä¸»è¦ç‰¹æ€§ï¼š
    - Supports multiple evaluation methods (baselines + framework + ablations)
      æ”¯æŒå¤šç§è¯„ä¼°æ–¹æ³•ï¼ˆåŸºçº¿+æ¡†æ¶+æ¶ˆèï¼‰
    - Uses LLM-based answer comparison for flexible matching
      ä½¿ç”¨åŸºäºLLMçš„ç­”æ¡ˆæ¯”è¾ƒå®ç°çµæ´»åŒ¹é…
    - Tracks execution time and error handling
      è·Ÿè¸ªæ‰§è¡Œæ—¶é—´å’Œé”™è¯¯å¤„ç†
    """

    def __init__(self, verbose: bool = False):
        """Initialize evaluator, baselines, and LLM comparator.
        åˆå§‹åŒ–è¯„ä¼°å™¨ã€åŸºçº¿å’ŒLLMæ¯”è¾ƒå™¨ã€‚

        Args:
            verbose: Print detailed progress if True.
                    å¦‚æœä¸ºTrueï¼Œæ‰“å°è¯¦ç»†è¿›åº¦ä¿¡æ¯ã€‚
        """
        self.verbose = verbose  # æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯ / Whether to print verbose info
        
        # Baseline wrappers (direct/zero-shot/few-shot CoT)
        # åŸºçº¿åŒ…è£…å™¨ï¼ˆç›´æ¥LLM/é›¶æ ·æœ¬CoT/å°‘æ ·æœ¬CoTï¼‰
        self.baseline_evaluator = BaselineEvaluator()

        # LLM comparator (used to compare predicted vs expected answers)
        # LLMæ¯”è¾ƒå™¨ï¼ˆç”¨äºæ¯”è¾ƒé¢„æµ‹ç­”æ¡ˆå’Œé¢„æœŸç­”æ¡ˆï¼‰
        self.answer_comparison_prompt = self._load_answer_comparison_prompt()
        self.llm_client = self.baseline_evaluator.llm_client

    def evaluate_single(
        self,
        problem: Dict[str, Any],
        method: EvaluationMethod
    ) -> EvaluationResult:
        """Evaluate a single problem with a specified method.
        ä½¿ç”¨æŒ‡å®šæ–¹æ³•è¯„ä¼°å•ä¸ªé—®é¢˜ã€‚

        Steps / æ­¥éª¤ï¼š
        - Route to the corresponding solver/ablation by method
          æ ¹æ®æ–¹æ³•è·¯ç”±åˆ°ç›¸åº”çš„æ±‚è§£å™¨/æ¶ˆèå®éªŒ
        - Measure elapsed time for transparency
          æµ‹é‡æ‰§è¡Œæ—¶é—´ä»¥æä¾›é€æ˜åº¦
        - Compare predicted vs expected answers and build a result
          æ¯”è¾ƒé¢„æµ‹ç­”æ¡ˆå’Œé¢„æœŸç­”æ¡ˆå¹¶æ„å»ºç»“æœ

        Args:
            problem: Problem dictionary containing id, question, answer
                    é—®é¢˜å­—å…¸ï¼ŒåŒ…å«idã€é—®é¢˜æ–‡æœ¬ã€ç­”æ¡ˆ
            method: The evaluation method to use
                   è¦ä½¿ç”¨çš„è¯„ä¼°æ–¹æ³•

        Returns:
            EvaluationResult with prediction, correctness, timing, etc.
            åŒ…å«é¢„æµ‹ã€æ­£ç¡®æ€§ã€æ—¶é—´ç­‰çš„è¯„ä¼°ç»“æœ
        """
        # æå–é—®é¢˜ä¿¡æ¯ / Extract problem information
        problem_id = problem['id']        # é—®é¢˜ID / Problem ID
        question = problem['question']    # é—®é¢˜æ–‡æœ¬ / Question text
        expected_answer = problem['answer']  # é¢„æœŸç­”æ¡ˆ / Expected answer

        # åˆå§‹åŒ–å˜é‡ / Initialize variables
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´ / Record start time
        error = None           # é”™è¯¯ä¿¡æ¯ / Error message
        predicted_answer = None  # é¢„æµ‹ç­”æ¡ˆ / Predicted answer
        reasoning_steps = None   # æ¨ç†æ­¥éª¤ / Reasoning steps

        try:
            # Route based on selected method / æ ¹æ®é€‰æ‹©çš„æ–¹æ³•è·¯ç”±
            if method == EvaluationMethod.DIRECT_LLM:
                # ç›´æ¥LLMæ–¹æ³• / Direct LLM method
                predicted_answer = self.baseline_evaluator.direct_llm(question)

            elif method == EvaluationMethod.ZERO_SHOT_COT:
                # é›¶æ ·æœ¬æ€ç»´é“¾æ–¹æ³• / Zero-shot CoT method
                predicted_answer, reasoning_steps = self.baseline_evaluator.zero_shot_cot(question)

            elif method == EvaluationMethod.FEW_SHOT_COT:
                # å°‘æ ·æœ¬æ€ç»´é“¾æ–¹æ³• / Few-shot CoT method
                predicted_answer, reasoning_steps = self.baseline_evaluator.few_shot_cot(question)

            elif method == EvaluationMethod.FULL_FRAMEWORK:
                # å®Œæ•´æ¡†æ¶æ–¹æ³• / Full framework method
                predicted_answer = self._run_full_framework(question, problem_id, method.value)

            elif method == EvaluationMethod.NO_RETRIEVER:
                # æ— æ£€ç´¢å™¨æ¶ˆè / No retriever ablation
                predicted_answer = self._run_without_retriever(question, problem_id, method.value)

            elif method == EvaluationMethod.NO_AI_RETRIEVER:
                # æ— AIæ£€ç´¢å™¨æ¶ˆè / No AI retriever ablation
                predicted_answer = self._run_without_ai_retriever(question, problem_id, method.value)

            elif method == EvaluationMethod.NO_SYMBOLIC_EXECUTION:
                # æ— ç¬¦å·æ‰§è¡Œæ¶ˆèï¼ˆä½¿ç”¨LLMè®¡ç®—ï¼‰/ No symbolic execution ablation (use LLM computation)
                predicted_answer = self._run_without_symbolic_execution(question, problem_id, method.value)

            elif method == EvaluationMethod.NO_VALIDATION:
                # æ— éªŒè¯æ¶ˆè / No validation ablation
                predicted_answer = self._run_without_validation(question, problem_id, method.value)

        except Exception as e:
            # æ•è·ä»»ä½•å¼‚å¸¸ / Catch any exception
            error = str(e)

        # è®¡ç®—æ‰§è¡Œæ—¶é—´ / Calculate execution time
        execution_time = time.time() - start_time

        # Build result object / æ„å»ºç»“æœå¯¹è±¡
        # æ¯”è¾ƒç­”æ¡ˆï¼ˆå¦‚æœæœ‰é¢„æµ‹ç­”æ¡ˆï¼‰/ Compare answers (if predicted answer exists)
        is_correct = self._compare_answers(expected_answer, predicted_answer, question) if predicted_answer else False

        # è·å–ä¿å­˜çš„å› æœè„šæ‰‹æ¶ï¼ˆå¦‚æœæœ‰ï¼‰/ Get saved causal scaffold (if any)
        causal_scaffold = getattr(self, '_last_causal_scaffold', None)

        # åˆ›å»ºè¯„ä¼°ç»“æœå¯¹è±¡ / Create evaluation result object
        result = EvaluationResult(
            problem_id=problem_id,
            method=method.value,
            problem_text=question,
            expected_answer=expected_answer,
            predicted_answer=str(predicted_answer) if predicted_answer else None,
            is_correct=is_correct,
            execution_time=execution_time,
            error=error,
            reasoning_steps=reasoning_steps,
            causal_scaffold=causal_scaffold  # æ·»åŠ å› æœè„šæ‰‹æ¶ç”¨äºå¯è§†åŒ– / Add causal scaffold for visualization
        )

        # Clear temporary scaffold cache between problems
        # æ¸…é™¤ä¸´æ—¶è„šæ‰‹æ¶ç¼“å­˜ï¼ˆä¸ºä¸‹ä¸€ä¸ªé—®é¢˜åšå‡†å¤‡ï¼‰/ Clear temporary scaffold cache (prepare for next problem)
        self._last_causal_scaffold = None

        return result

    # Full pipeline: retrieval â†’ scaffold â†’ compute (LLM) â†’ synthesis
    def _run_full_framework(self, problem: str, problem_id: str = None, method: str = None) -> Any:
        """Run full causal reasoning framework with GRPO experiences"""
        try:
            from main import CausalReasoningEngine
            from engine import GRPOExperienceManager

            # Initialize engine
            engine = CausalReasoningEngine(
                knowledge_base_path="data/knowledge_base.json",
                verbose=self.verbose,
                use_ai_retriever=True,
                auto_enrich_kb=True,
                min_rules_threshold=5,
                use_multi_agent=True  # Enable multi-agent for experience injection
            )

            # Load and inject GRPO experiences
            try:
                experience_manager = GRPOExperienceManager(
                    experience_dir="data/grpo_experiences",
                    verbose=False
                )
                
                if hasattr(engine, 'scaffolder') and experience_manager:
                    engine.scaffolder.experience_manager = experience_manager
                    if self.verbose:
                        print(f"  âœ“ Loaded GRPO experiences for evaluation")
            except Exception as e:
                if self.verbose:
                    print(f"  âš ï¸ Could not load GRPO experiences: {e}")

            # Solve problem
            results = engine.solve_problem(
                problem,
                include_validation=False,
                problem_id=problem_id,
                method_name=method
            )

            # Save causal_scaffold for visualization
            self._last_causal_scaffold = results.get('causal_scaffold')

            if results.get('success'):
                return results.get('final_answer')
            else:
                error_msg = results.get('error', 'Unknown error')
                if self.verbose:
                    print(f"  Framework error: {error_msg}")
                raise Exception(error_msg)

        except Exception as e:
            raise Exception(f"Full framework failed: {e}")

    # Ablation: disable both traditional and AI retrievers
    def _run_without_retriever(self, problem: str, problem_id: str = None, method: str = None) -> Any:
        """Run without knowledge retriever /

        """
        try:
            #
            from main import CausalReasoningEngine

            # Mock
            class EmptyRetriever:
                def get_knowledge(self, problem_text):
                    #
                    return []

            #  AI
            engine = CausalReasoningEngine(
                verbose=self.verbose,
                use_ai_retriever=False,  #  AI
                auto_enrich_kb=False
            )
            #
            engine.retriever = EmptyRetriever()
            #  AI  None
            engine.ai_retriever = None

            #
            results = engine.solve_problem(
                problem,
                include_validation=False,
                problem_id=problem_id,
                method_name=method
            )

            # 
            if results.get('success'):
                return results.get('final_answer')
            else:
                # 
                raise Exception(results.get('error', 'Unknown error'))

        except Exception as e:
            # 
            raise Exception(f"No retriever ablation failed: {e}")

    # Ablation: use only traditional retriever (no AI rule generation)
    def _run_without_ai_retriever(self, problem: str, problem_id: str = None, method: str = None) -> Any:
        """Run with traditional retriever only (no AI rule generation)"""
        try:
            from main import CausalReasoningEngine
            from engine import GRPOExperienceManager

            # Initialize engine without AI retriever
            engine = CausalReasoningEngine(
                verbose=self.verbose,
                use_multi_agent=True
            )

            # Load and inject GRPO experiences (still useful even without AI retriever)
            try:
                experience_manager = GRPOExperienceManager(
                    experience_dir="data/grpo_experiences",
                    verbose=False
                )
                
                if hasattr(engine, 'scaffolder') and experience_manager:
                    engine.scaffolder.experience_manager = experience_manager
            except Exception:
                pass  # Silently continue without experiences

            # Solve problem
            results = engine.solve_problem(
                problem,
                include_validation=False,
                problem_id=problem_id,
                method_name=method
            )

            if results.get('success'):
                return results.get('final_answer')
            else:
                raise Exception(results.get('error', 'Unknown error'))

        except Exception as e:
            raise Exception(f"No AI retriever ablation failed: {e}")

    # Ablation: compute via LLM (no symbolic execution)
    def _run_without_symbolic_execution(self, problem: str, problem_id: str = None, method: str = None) -> Any:
        """
        Run without symbolic execution (use LLM for computation based on causal scaffold).
        ä¸ä½¿ç”¨ç¬¦å·æ‰§è¡Œè¿è¡Œï¼ˆåŸºäºå› æœè„šæ‰‹æ¶ä½¿ç”¨LLMè®¡ç®—ï¼‰ã€‚

        This is a proper ablation that:
        æ­¤æ¶ˆèå®éªŒï¼š
        - Still uses Knowledge Retrieval / ä»ä½¿ç”¨çŸ¥è¯†æ£€ç´¢
        - Still uses Causal Scaffolding / ä»ä½¿ç”¨å› æœè„šæ‰‹æ¶
        - **Uses LLM Computation** instead of Symbolic Execution / **ä½¿ç”¨LLMè®¡ç®—**è€Œéç¬¦å·æ‰§è¡Œ
        - Still uses Synthesis / ä»ä½¿ç”¨åˆæˆ

        This tests whether symbolic execution is necessary or LLM computation is sufficient.
        è¿™æµ‹è¯•ç¬¦å·æ‰§è¡Œæ˜¯å¦å¿…è¦ï¼Œæˆ–è€…LLMè®¡ç®—æ˜¯å¦è¶³å¤Ÿã€‚
        """
        try:
            from main import CausalReasoningEngine
            from engine import GRPOExperienceManager

            # Initialize engine with LLM computation mode
            engine = CausalReasoningEngine(
                knowledge_base_path="data/knowledge_base.json",
                verbose=self.verbose,
                use_ai_retriever=True,
                auto_enrich_kb=True,
                min_rules_threshold=2,
                computation_mode="llm",  # KEY: Use LLM computation instead of symbolic execution
                use_multi_agent=True
            )

            # Load and inject GRPO experiences
            try:
                experience_manager = GRPOExperienceManager(
                    experience_dir="data/grpo_experiences",
                    verbose=False
                )
                
                if hasattr(engine, 'scaffolder') and experience_manager:
                    engine.scaffolder.experience_manager = experience_manager
            except Exception:
                pass  # Silently continue without experiences

            # Solve problem using LLM computation
            results = engine.solve_problem(
                problem,
                include_validation=False,
                problem_id=problem_id,
                method_name=method
            )

            if results.get('success'):
                return results.get('final_answer')
            else:
                error_msg = results.get('error', 'Unknown error')
                if self.verbose:
                    print(f"  LLM computation error: {error_msg}")
                raise Exception(error_msg)

        except Exception as e:
            raise Exception(f"No symbolic execution ablation failed: {e}")

    # Ablation: skip synthesis/validation stage
    def _run_without_validation(self, problem: str, problem_id: str = None, method: str = None) -> Any:
        """Run without validation /

        """
        #
        return self._run_full_framework(problem, problem_id, method)

    def _load_answer_comparison_prompt(self) -> str:
        """Load answer comparison prompt from file"""
        prompt_path = Path("prompts/answer_comparison_prompt.txt")
        if prompt_path.exists():
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            # Fallback to default prompt
            return """You are a scientific answer verification expert. Determine if two answers are equivalent.

EXPECTED ANSWER: {expected_answer}
PREDICTED ANSWER: {predicted_answer}

Respond with exactly: YES or NO
Then provide a brief reason.

YOUR RESPONSE:"""

    def _compare_answers(self, expected: str, predicted: Any, problem_text: str = "") -> bool:
        """Compare expected and predicted answers using LLM with problem context
        ä½¿ç”¨ LLM æ¯”è¾ƒé¢„æœŸç­”æ¡ˆå’Œé¢„æµ‹ç­”æ¡ˆï¼ˆå¸¦é—®é¢˜ä¸Šä¸‹æ–‡ï¼‰
        
        Args:
            expected: Expected answer
            predicted: Predicted answer
            problem_text: The original problem text for context
        """
        # False
        if predicted is None:
            return False

        # Use LLM to compare answers with problem context
        try:
            prompt = self.answer_comparison_prompt.format(
                problem_text=problem_text if problem_text else "No context provided",
                expected_answer=expected,
                predicted_answer=predicted
            )

            response = self.llm_client.complete(prompt, temperature=0.0)

            # Parse response - look for YES or NO
            response_upper = response.strip().upper()

            if response_upper.startswith("YES"):
                if self.verbose:
                    print(f"  âœ“ LLM Answer Comparison: YES")
                    print(f"    Problem: {problem_text[:100]}..." if len(problem_text) > 100 else f"    Problem: {problem_text}")
                    print(f"    Expected: {expected}")
                    print(f"    Predicted: {predicted}")
                    # Extract and show reasoning if available
                    reasoning = response.strip().split('\n', 1)
                    if len(reasoning) > 1:
                        print(f"    Reasoning: {reasoning[1][:150]}..." if len(reasoning[1]) > 150 else f"    Reasoning: {reasoning[1]}")
                return True
            elif response_upper.startswith("NO"):
                if self.verbose:
                    print(f"  âœ— LLM Answer Comparison: NO")
                    print(f"    Problem: {problem_text[:100]}..." if len(problem_text) > 100 else f"    Problem: {problem_text}")
                    print(f"    Expected: {expected}")
                    print(f"    Predicted: {predicted}")
                    # Extract and show reasoning if available
                    reasoning = response.strip().split('\n', 1)
                    if len(reasoning) > 1:
                        print(f"    Reasoning: {reasoning[1][:150]}..." if len(reasoning[1]) > 150 else f"    Reasoning: {reasoning[1]}")
                return False
            else:
                # If LLM response is unclear, fallback to string matching
                print(f"  âš  LLM response unclear, using fallback comparison")
                return self._fallback_compare(expected, predicted)

        except Exception as e:
            # If LLM fails, use fallback comparison
            print(f"  âš  LLM comparison failed: {e}, using fallback")
            return self._fallback_compare(expected, predicted)

    def _fallback_compare(self, expected: str, predicted: Any) -> bool:
        """Fallback comparison method (simple rule-based) with enhanced unit and scientific notation handling"""
        expected_str = str(expected).strip().lower()
        predicted_str = str(predicted).strip().lower()

        # Remove LaTeX, brackets, quotes
        expected_str = re.sub(r'[\$\\{}\[\]\'\"]', '', expected_str)
        predicted_str = re.sub(r'[\$\\{}\[\]\'\"]', '', predicted_str)

        # Exact match (after basic cleanup)
        if expected_str == predicted_str:
            return True

        # Extract numerical values (handles scientific notation and units)
        def extract_number_and_unit(s):
            """Extract numerical value and unit from string, handling scientific notation"""
            s = s.strip()
            
            # Handle scientific notation: 2Ã—10^5, 2e5, 2*10^5
            scientific_patterns = [
                r'([\d.]+)\s*[Ã—x*]\s*10\s*\^\s*([+-]?\d+)\s*([a-zA-Z/Â°Â²Â³]+)?',  # 2Ã—10^5 or 2*10^5 with optional unit
                r'([\d.]+)\s*[eE]\s*([+-]?\d+)\s*([a-zA-Z/Â°Â²Â³]+)?',              # 2e5 or 2E5 with optional unit
            ]
            
            for pattern in scientific_patterns:
                match = re.search(pattern, s)
                if match:
                    base = float(match.group(1))
                    exponent = float(match.group(2))
                    unit = match.group(3) if len(match.groups()) >= 3 else None
                    value = base * (10 ** exponent)
                    return (value, unit)
            
            # Extract number and unit: "30", "30 m/s", "30m/s", "30.5 kg", "6 kW"
            num_unit_match = re.search(r'^([+-]?[\d.]+)\s*([a-zA-Z/Â°Â²Â³]+)?', s)
            if num_unit_match:
                value = float(num_unit_match.group(1))
                unit = num_unit_match.group(2)
                return (value, unit)
            
            return (None, None)
        
        def normalize_unit_value(value, unit):
            """Convert to base units (e.g., kW -> W, km -> m)"""
            if value is None:
                return None
            
            if unit is None:
                return value
            
            unit_lower = unit.lower()
            
            # Power conversions
            if unit_lower in ['kw', 'kilowatt']:
                return value * 1000  # kW to W
            elif unit_lower in ['mw', 'megawatt']:
                return value * 1000000  # MW to W
            
            # Energy conversions
            elif unit_lower in ['kj', 'kilojoule']:
                return value * 1000  # kJ to J
            elif unit_lower in ['mj', 'megajoule']:
                return value * 1000000  # MJ to J
            
            # Distance conversions
            elif unit_lower in ['km', 'kilometer']:
                return value * 1000  # km to m
            elif unit_lower in ['cm', 'centimeter']:
                return value / 100  # cm to m
            elif unit_lower in ['mm', 'millimeter']:
                return value / 1000  # mm to m
            
            # Mass conversions
            elif unit_lower in ['g', 'gram']:
                return value / 1000  # g to kg
            elif unit_lower in ['ton', 'tonne']:
                return value * 1000  # ton to kg
            
            # Time conversions
            elif unit_lower in ['min', 'minute']:
                return value * 60  # min to s
            elif unit_lower in ['h', 'hour', 'hr']:
                return value * 3600  # hour to s
            
            # Pressure conversions
            elif unit_lower in ['kpa', 'kilopascal']:
                return value * 1000  # kPa to Pa
            elif unit_lower in ['mpa', 'megapascal']:
                return value * 1000000  # MPa to Pa
            
            # If no conversion needed, return original value
            return value

        # Try numerical comparison with unit conversion
        try:
            expected_num, expected_unit = extract_number_and_unit(expected_str)
            predicted_num, predicted_unit = extract_number_and_unit(predicted_str)
            
            if expected_num is not None and predicted_num is not None:
                # Normalize units to base units (e.g., kW -> W, km -> m)
                expected_normalized = normalize_unit_value(expected_num, expected_unit)
                predicted_normalized = normalize_unit_value(predicted_num, predicted_unit)
                
                # Compare normalized values
                if expected_normalized is not None and predicted_normalized is not None:
                    # Use relative tolerance for large numbers, absolute for small
                    if abs(expected_normalized) > 1e-6:
                        relative_diff = abs(expected_normalized - predicted_normalized) / abs(expected_normalized)
                        if relative_diff < 1e-4:  # 0.01% relative tolerance
                            return True
                    
                    # Absolute tolerance
                    if abs(expected_normalized - predicted_normalized) < 1e-6:
                        return True
        except Exception as e:
            if self.verbose:
                print(f"    âš  Fallback comparison error: {e}")
            pass

        # Remove all spaces and try exact match again
        expected_clean = re.sub(r'\s+', '', expected_str)
        predicted_clean = re.sub(r'\s+', '', predicted_str)
        
        if expected_clean == predicted_clean:
            return True

        return False

    def evaluate_dataset(
        self,
        problems: List[Dict[str, Any]],
        methods: List[EvaluationMethod],
        dataset_name: str
    ) -> Dict[str, Any]:
        """
        Evaluate dataset with multiple methods
        
        
        """
        # 
        print(f"\n{'='*80}")
        print(f"Evaluating {dataset_name} with {len(methods)} methods on {len(problems)} problems")
        print(f" {dataset_name}  {len(methods)}  {len(problems)} ")
        print(f"{'='*80}\n")

        all_results = {}  # 

        # 
        for method in methods:
            print(f"\n{''*80}")
            print(f"Method: {method.value}")
            print(f": {method.value}")
            print(f"{''*80}")

            method_results = []  # 
            correct_count = 0    # 
            total_time = 0       # 
            error_count = 0      # 

            # Iterate all problems with simple progress output
            for i, problem in enumerate(problems, 1):
                print(f"[{i}/{len(problems)}] {problem['id']}", end=" ")

                # Evaluate one problem via selected method
                result = self.evaluate_single(problem, method)
                method_results.append(result)

                # Update counters and perâ€‘problem status symbol
                if result.is_correct:
                    correct_count += 1
                    print("âœ“", end="")  # Correct
                elif result.error:
                    error_count += 1
                    print("âŒ", end="")  # Error
                else:
                    print("âœ—", end="")  # Incorrect

                # Show time per problem
                print(f" ({result.execution_time:.2f}s)")
                
                # Show current accuracy in real-time
                current_accuracy = (correct_count / i) * 100
                print(f"    Current: {correct_count}/{i} correct ({current_accuracy:.1f}%)")

                # Aggregate total time for method stats
                total_time += result.execution_time

            # Compute methodâ€‘level statistics
            accuracy = correct_count / len(problems) if problems else 0
            avg_time = total_time / len(problems) if problems else 0

            # Collect results and stats for this method
            all_results[method.value] = {
                'results': method_results,  # 
                'statistics': {
                    'total': len(problems),           # 
                    'correct': correct_count,         # 
                    'wrong': len(problems) - correct_count - error_count,  # 
                    'errors': error_count,            # 
                    'accuracy': accuracy,             # 
                    'total_time': total_time,         # 
                    'avg_time': avg_time              # 
                }
            }

            # Method summary line
            print(f"\n  Accuracy: {accuracy*100:.2f}% ({correct_count}/{len(problems)})")
            print(f"  : {accuracy*100:.2f}% ({correct_count}/{len(problems)})")
            print(f"  Avg Time: {avg_time:.2f}s")
            print(f"  : {avg_time:.2f}s")

        # 
        return {
            'dataset_name': dataset_name,      # 
            'total_problems': len(problems),   # 
            'methods': all_results,            # 
            'evaluation_time': datetime.now().isoformat()  # 
        }

    def save_results(self, results: Dict[str, Any], output_path: str):
        """Serialize evaluation results to a JSON file.

        The JSON contains perâ€‘method statistics and perâ€‘problem records.
        """
        # 
        serializable_results = {
            'dataset_name': results['dataset_name'],
            'total_problems': results['total_problems'],
            'evaluation_time': results['evaluation_time'],
            'methods': {}
        }

        # 
        for method_name, method_data in results['methods'].items():
            serializable_results['methods'][method_name] = {
                'statistics': method_data['statistics'],
                'results': [asdict(r) for r in method_data['results']]  # asdict
            }

        # 
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)  # 

        # JSON
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

        print(f"\n Results saved to: {output_file}")  # 

    def print_comparison_table(self, results: Dict[str, Any]):
        """Prettyâ€‘print a compact comparison table across methods."""
        print(f"\n{'='*80}")
        print(f"COMPARISON TABLE / ")
        print(f"{'='*80}")
        print(f"Dataset: {results['dataset_name']}")
        print(f": {results['dataset_name']}\n")

        # 
        print(f"{'Method':<30} {'Accuracy':<15} {'Avg Time':<15}")
        print(f"{'':<30} {'':<15} {'':<15}")
        print(f"{'-'*80}")

        # 
        for method_name, method_data in results['methods'].items():
            stats = method_data['statistics']
            acc_str = f"{stats['accuracy']*100:.2f}%"  # 
            time_str = f"{stats['avg_time']:.2f}s"     # 
            print(f"{method_name:<30} {acc_str:<15} {time_str:<15}")

        print(f"{'='*80}\n")  # 


def main():
    """CLI entry for running dataset evaluation."""
    import argparse

    # Initialize CLI parser

    #  - 
    parser = argparse.ArgumentParser(
        description="Comprehensive Framework Evaluation\n"
    )

    parser.add_argument(
        '--dataset',
        type=str,
        choices=['gsm8k', 'math', 'mydata', 'omnimath', 'olympiad'],  # æ–°å¢ omnimath å’Œ olympiad / Added omnimath and olympiad
        default='gsm8k',
        help='Dataset to evaluate / '
    )

    #  - 
    parser.add_argument(
        '--limit',
        type=int,
        default=20,
        help='Limit number of problems / '
    )

    #  - 
    parser.add_argument(
        '--methods',
        type=str,
        nargs='+',
        choices=['baselines', 'ablations', 'all'],
        default=['baselines'],
        help='Evaluation methods / '
    )

    #  - 
    parser.add_argument(
        '--output',
        type=str,
        default='evaluation_results',
        help='Output directory / '
    )

    #  - 
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Verbose output / '
    )

    # 
    args = parser.parse_args()

    # Resolve which methods to evaluate
    methods_to_run = []

    # Baselines (and full framework)
    if 'baselines' in args.methods or 'all' in args.methods:
        methods_to_run.extend([
            # EvaluationMethod.DIRECT_LLM,      # LLM
            # EvaluationMethod.ZERO_SHOT_COT,   #
            # EvaluationMethod.FEW_SHOT_COT,    #
            EvaluationMethod.FULL_FRAMEWORK   # 
        ])

    # Ablation variants
    if 'ablations' in args.methods or 'all' in args.methods:
        methods_to_run.extend([
            EvaluationMethod.NO_RETRIEVER,       # 
            EvaluationMethod.NO_AI_RETRIEVER,    # AI
            EvaluationMethod.NO_SYMBOLIC_EXECUTION  # 
        ])

    # Load dataset problems (switch by name)
    loader = DatasetLoader()

    # åŠ è½½æ•°æ®é›† / Load dataset
    if args.dataset == 'gsm8k':
        # GSM8K
        dataset_path = "dataset/GSM8K/grade_school_math/data/test.jsonl"
        problems = loader.load_gsm8k(dataset_path, limit=args.limit)
        dataset_name = "GSM8K"
    elif args.dataset == 'math':
        # MATH
        dataset_path = "dataset/Math/test-00000-of-00001.parquet.json"
        problems = loader.load_math(dataset_path, limit=args.limit)
        dataset_name = "MATH"
    elif args.dataset == 'mydata':
        # MyData
        dataset_path = "dataset/mydata/data/2024A.json"
        problems = loader.load_mydata(dataset_path, limit=args.limit)
        dataset_name = "MyData_2024A"
    elif args.dataset == 'omnimath':
        # Omni-MATHï¼ˆæ–°å¢ / NEW!ï¼‰
        dataset_path = "dataset/Omni-MATH/archive/main_test.jsonl"
        problems = loader.load_omnimath(dataset_path, limit=args.limit)
        dataset_name = "Omni-MATH"
    elif args.dataset == 'olympiad':
        # OlympiadBenchï¼ˆæ–°å¢ï¼Œå¤šæ¨¡æ€æ”¯æŒ / NEW! Multi-Modalï¼‰
        # é»˜è®¤ä½¿ç”¨è‹±è¯­æ•°å­¦ç«èµ›çš„çº¯æ–‡æœ¬ç‰ˆæœ¬ / Default: English math competition text-only
        dataset_path = "dataset/OlympiadBench_Dataset/OlympiadBench_Dataset/data/TP_TO_maths_en_COMP.json"
        problems = loader.load_olympiadbench(dataset_path, limit=args.limit)
        dataset_name = "OlympiadBench"
        print("\nğŸ’¡ Tip: You can also try multi-modal versions like TP_MM_maths_en_COMP.json")
        print("ğŸ’¡ æç¤º: ä½ ä¹Ÿå¯ä»¥å°è¯•å¤šæ¨¡æ€ç‰ˆæœ¬ï¼Œå¦‚ TP_MM_maths_en_COMP.json\n")
    else:
        print(f"âŒ Unknown dataset: {args.dataset}")
        return 1

    # 
    if not Path(dataset_path).exists():
        print(f" Dataset not found: {dataset_path}")
        return 1

    # 
    evaluator = FrameworkEvaluator(verbose=args.verbose)

    #  - 
    results = evaluator.evaluate_dataset(problems, methods_to_run, dataset_name)

    #  - 
    evaluator.print_comparison_table(results)

    # 
    output_path = f"{args.output}/{dataset_name}_comparison.json"
    evaluator.save_results(results, output_path)

    return 0


if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\n  Evaluation interrupted by user.")
        sys.exit(1)
    except Exception as e:
        print(f"\n\n Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
