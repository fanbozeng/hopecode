### Free Train GRPO （Trainning stage）
![](https://cdn.nlark.com/yuque/0/2025/svg/27201149/1762354346351-abf60723-706d-43aa-8d0a-23e316f44b2c.svg)

背景：llm在构造SCM结构化因果模型的时候 经常出现失误。这种上游的失误往往会导致下游计算的失误和模型思维链错误，因此提出了 free train GRPO模块来加强llm在构造DAG的能力。由于GRPO预训练需要借助强大的显卡资源和资金花费，因此我们借助GRPO训练的思想，将训练的参数空间转移到上下文空间，我们发现同样只需要在少样本的数据集上训练 就能达到跟GRPO相接近的效果。

具体做法如下：

在我们的multi-agent generator SCM中会有多个llm的提取层 和一个critic层。数据集我们用的是aime2024和aime2025以及从30道物理竞赛题。具体训练的流程如下：

（这部分先前的代码已经有了 只是需要定义角色来做）

+ 第一步：在llm的提取层部分 我们会让LLM生成DAG图并且根据该DAG图得出它的一个计算结论。同理在critic层 我们会让critic融合前面不同llm生成得到的DAG图，同样计算得到answer
+ 第二步
    - 在llm提取层 我们的奖励函数 r= w*r_ans+j*r_log+i*r_dag
    - 在critic提取层 我们的奖励函数 r=w*r_ans+j*r_log+i*r_dag+g*r_fusion
+ 计算标准差和阀值 合适的话让更新提取层和critic 这里仿照KL散度的思想（有几种操作） 保证经验库不会特别大
+ epoch=3 温度0.7

### Reasoning Stage

推理阶段分为两个核心阶段：**提取阶段（Extraction）** 和 **增强阶段（Enhancement）**

#### Step1: 提取阶段 - Multi-agent Generator for DAG of SCM

**输入**：数学/物理问题

**过程**：
+ 3个经过GRPO训练的Generator（带有经验库）独立从问题文本中提取并构建初始DAG图
+ 每个Generator基于其积累的经验，生成自己的因果图结构
+ Critic层接收所有Generator的输出，进行融合和去偏见处理
+ **输出**：Fixed DAG（初步融合的因果图，但由于LLM本身知识缺陷，还不够完善）

#### Step2: 增强阶段 - Post-enhancement of the DAG

**输入**：Fixed DAG（来自Step1）

**增强流程**：

1. **领域专家Review**
   - 根据题目类型（数学/物理），调用对应的专家Agent（数学家/物理学家）
   - 专家审查推理链路和推理路径中使用的公式、定理、方法是否正确
   - 识别并修正推理缺陷

2. **RAG知识增强**
   - 从知识库中检索相关的领域知识、公式、定理
   - 补充DAG中缺失的关键知识节点和因果关系

3. **因果学家结构优化**
   - 因果学家利用因果推断的三种基本结构对DAG进行结构化调整：
     - **(a) Chain/链结构**：X → Z → Y（中介效应）
     - **(b) Fork/叉结构**：X ← Z → Y（共同原因）
     - **(c) Collider/对撞结构**：X → Z ← Y（共同结果）
   - 确保因果关系的方向性和逻辑一致性

**输出**：Enhanced DAG（经过多维度增强的、更鲁棒、更忠实的因果推理链路）

#### Step3: 因果评估 - Causal Evaluation

**两个维度的质量检验**：

1. **因果干预（Causal Intervention）**
   - 对DAG中的关键变量X进行干预操作 do(X)
   - 观察反事实结果："如果我们改变某个推理路径，结果会如何变化？"
   - **评估指标**：CF（Counterfactual Faithfulness，思维链的忠诚度）

2. **溯因推理（Abductive Reasoning）**
   - 基于观察到的结果，反向推理最可能的原因
   - 检查推理思维的质量和合理性
   - **评估指标**：AC（Abductive Consistency，逆因果忠诚度）

**输出**：Abductive DAG（最终经过因果评估验证的高质量推理图）

**最终评判指标**：
+ pass@1：答案准确率
+ CF（Counterfactual Faithfulness）：思维链的忠诚度（对应causal intervention）
+ AC（Abductive Consistency）：逆因果忠诚度（对应abductive DAG）

![](https://cdn.nlark.com/yuque/0/2025/svg/27201149/1762354890497-04294c78-97b3-452a-9874-d5b29a664299.svg)

### 实验设计

#### 数据集配置

**训练阶段数据集（Free-Train GRPO）**：
- **AIME 2024**：30题（美国数学邀请赛，高难度数学推理）
- **AIME 2025-I**：15题（2025年第一场）
- **AIME 2025-II**：15题（2025年第二场）
- **物理竞赛题**：30题（从mydata中选取，涵盖1981-2025年经典题目）
- **总计**：90题用于GRPO经验积累

**测试阶段数据集（Reasoning & Evaluation）**：
- **GSM8K**：小学数学应用题（测试基础推理能力）
- **MATH**：竞赛级数学题（测试高阶数学推理）
- **OlympiadBench**：奥林匹克数学物理综合题（测试跨领域推理）
- **Omni-MATH**：综合数学题库（测试泛化能力）
- **物理竞赛题（测试集）**：与训练集不重叠的题目

#### 实验配置

**模型参数**：
- Temperature = 0.0（评估阶段，确保结果稳定可复现）
- Temperature = 0.7（训练阶段GRPO，保持一定探索性）
- Rollouts per problem = 3（每个问题生成3个rollout）
- GRPO threshold τ = 0.05

**奖励权重**：
- Generator: α=0.6 (r_ans) + β=0.25 (r_logic) + γ=0.15 (r_graph)
- Critic: α=0.6 (r_ans) + β=0.25 (r_logic) + γ=0.15 (r_graph) + δ=0.1 (r_fusion)

---

#### 对比实验（Baseline Comparison）

| 方法 | GSM8K | MATH | OlympiadBench | Omni-MATH | Physics |
| --- | --- | --- | --- | --- | --- |
| **Zero-shot** | pass@1 / CF / AC | pass@1 / CF / AC | pass@1 / CF / AC | pass@1 / CF / AC | pass@1 / CF / AC |
| **Few-shot CoT** | | | | | |
| **Zero-shot CoT** | | | | | |
| **ToT** | | | | | |
| **GoT** | | | | | |
| **CFGO (Ours)** | | | | | |

**Baseline方法说明**：
- **Zero-shot**：直接让LLM回答，无任何提示
- **Few-shot CoT**：Chain-of-Thought，给3个示例
- **Zero-shot CoT**：使用"Let's think step by step"提示
- **ToT (Tree of Thoughts)**：树状搜索推理
- **GoT (Graph of Thoughts)**：图状搜索推理
- **CFGO (Causal Flow Graph Optimization)**：我们的方法（完整流程：GRPO训练 + 提取阶段 + 增强阶段 + 因果评估）

**评估指标说明**：
- **pass@1**：答案准确率（一次性正确率）
- **CF (Counterfactual Faithfulness)**：思维链忠诚度（0-1分数，衡量推理链与最终答案的因果一致性）
- **AC (Abductive Consistency)**：逆因果一致性（0-1分数，衡量从结果反推原因的合理性）

---

#### 消融实验（Ablation Study）

| 变体 | GSM8K | MATH | OlympiadBench | 说明 |
| --- | --- | --- | --- | --- |
| **CFGO (Full)** | pass@1 / CF / AC | pass@1 / CF / AC | pass@1 / CF / AC | 完整方法 |
| **CFGO-woGRPO** | | | | 移除GRPO训练（Generator无经验库） |
| **CFGO-woEnhancement** | | | | 移除增强阶段（无专家/RAG/因果学家） |
| **CFGO-woExpert** | | | | 仅移除领域专家 |
| **CFGO-woRAG** | | | | 仅移除RAG知识增强 |
| **CFGO-woCausal** | | | | 仅移除因果学家结构优化 |
| **CFGO-woEvaluation** | | | | 移除Step3因果评估 |

**消融实验目的**：
1. **CFGO-woGRPO**：验证GRPO训练的必要性
2. **CFGO-woEnhancement**：验证整个增强阶段的贡献
3. **CFGO-woExpert**：验证领域专家的作用
4. **CFGO-woRAG**：验证知识检索的重要性
5. **CFGO-woCausal**：验证因果结构优化的效果
6. **CFGO-woEvaluation**：验证因果评估的价值

---

#### 案例分析（Qualitative Analysis）

选取典型题目进行可视化分析：
1. **DAG演化过程**：Fixed DAG → Enhanced DAG → Abductive DAG
2. **因果干预示例**：展示do(X)操作前后的推理路径变化
3. **溯因推理示例**：展示从答案反推关键步骤的过程
4. **错误案例分析**：分析系统失败的原因（知识缺失？推理错误？因果关系错误？）

---

#### 统计显著性检验

- 使用配对t检验（paired t-test）比较CFGO与各baseline在相同测试集上的表现
- 显著性水平：p < 0.05
- 每个数据集至少包含100+测试样本以保证统计有效性
