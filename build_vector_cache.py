"""
Build Vector Embeddings Cache
æ„å»ºå‘é‡åµŒå…¥ç¼“å­˜

This script pre-computes embeddings for the knowledge base
to speed up first-time initialization.

æ­¤è„šæœ¬é¢„å…ˆè®¡ç®—çŸ¥è¯†åº“çš„åµŒå…¥ï¼Œä»¥åŠ å¿«é¦–æ¬¡åˆå§‹åŒ–é€Ÿåº¦ã€‚
"""

import time
from pathlib import Path
from engine.vector_retriever import VectorKnowledgeRetriever


def main():
    print("="*80)
    print(" Building Vector Embeddings Cache")
    print(" æ„å»ºå‘é‡åµŒå…¥ç¼“å­˜")
    print("="*80 + "\n")
    
    # Check if knowledge base exists
    kb_path = Path("data/knowledge_base.json")
    if not kb_path.exists():
        print("âŒ Error: knowledge_base.json not found in data/")
        print("âŒ é”™è¯¯ï¼šdata/ ç›®å½•ä¸­æœªæ‰¾åˆ° knowledge_base.json")
        print("\nPlease ensure the knowledge base file exists before running this script.")
        return 1
    
    # Check for local model
    model_path = Path("all-MiniLM-L6-v2")
    if model_path.exists():
        model_name = "all-MiniLM-L6-v2"
        print(f"âœ“ Found local model at: {model_path}")
        print(f"âœ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹: {model_path}")
    else:
        model_name = "sentence-transformers/all-MiniLM-L6-v2"
        print(f"âš  Local model not found, will download from HuggingFace")
        print(f"âš  æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†ä» HuggingFace ä¸‹è½½")
        print(f"Model: {model_name}\n")
    
    cache_path = Path("data/knowledge_embeddings.pkl")
    
    # Check if cache already exists
    if cache_path.exists():
        print(f"âš  Cache file already exists: {cache_path}")
        print(f"âš  ç¼“å­˜æ–‡ä»¶å·²å­˜åœ¨: {cache_path}")
        response = input("\nDo you want to rebuild it? (y/n): ")
        if response.lower() != 'y':
            print("Cancelled. Exiting...")
            print("å·²å–æ¶ˆã€‚é€€å‡º...")
            return 0
        
        # Delete old cache
        cache_path.unlink()
        print("âœ“ Deleted old cache file")
        print("âœ“ å·²åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶\n")
    
    # Build cache
    print("-"*80)
    print("Starting embedding computation...")
    print("å¼€å§‹è®¡ç®—åµŒå…¥...")
    print("-"*80 + "\n")
    
    start_time = time.time()
    
    try:
        # Initialize retriever (will compute and cache embeddings)
        retriever = VectorKnowledgeRetriever(
            knowledge_base_path=str(kb_path),
            model_name=model_name,
            cache_path=str(cache_path),
            use_cache=False  # Force computation
        )
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "="*80)
        print("âœ… SUCCESS / æˆåŠŸ")
        print("="*80)
        print(f"\nğŸ“Š Statistics:")
        print(f"  - Knowledge entries: {len(retriever.knowledge_entries)}")
        print(f"  - Embedding dimensions: {retriever.embeddings_matrix.shape[1]}")
        print(f"  - Total embeddings: {retriever.embeddings_matrix.shape[0]}")
        print(f"  - Computation time: {elapsed_time:.2f} seconds")
        print(f"  - Cache file: {cache_path}")
        print(f"  - Cache size: {cache_path.stat().st_size / 1024 / 1024:.2f} MB")
        
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - çŸ¥è¯†æ¡ç›®æ•°: {len(retriever.knowledge_entries)}")
        print(f"  - åµŒå…¥ç»´åº¦: {retriever.embeddings_matrix.shape[1]}")
        print(f"  - æ€»åµŒå…¥æ•°: {retriever.embeddings_matrix.shape[0]}")
        print(f"  - è®¡ç®—æ—¶é—´: {elapsed_time:.2f} ç§’")
        print(f"  - ç¼“å­˜æ–‡ä»¶: {cache_path}")
        print(f"  - ç¼“å­˜å¤§å°: {cache_path.stat().st_size / 1024 / 1024:.2f} MB")
        
        print("\n" + "="*80)
        print("âœ“ Cache built successfully! Future initializations will be much faster.")
        print("âœ“ ç¼“å­˜æ„å»ºæˆåŠŸï¼æœªæ¥çš„åˆå§‹åŒ–å°†æ›´å¿«ã€‚")
        print("="*80)
        
        # Test retrieval
        print("\nğŸ” Testing retrieval...")
        print("ğŸ” æµ‹è¯•æ£€ç´¢...")
        
        test_problem = "An object with mass 10 kg accelerates at 5 m/sÂ²"
        results = retriever.retrieve_with_scores(test_problem, top_k=3, similarity_threshold=0.2)
        
        if results:
            print(f"\nâœ“ Test successful! Retrieved {len(results)} rules:")
            print(f"âœ“ æµ‹è¯•æˆåŠŸï¼æ£€ç´¢åˆ° {len(results)} æ¡è§„åˆ™:")
            for i, (rule, score, category) in enumerate(results, 1):
                cat_str = f"[{category}]" if category else ""
                print(f"  {i}. {cat_str} Score: {score:.3f}")
                print(f"     {rule[:80]}...")
        else:
            print("âš  Warning: No results found in test query")
            print("âš  è­¦å‘Šï¼šæµ‹è¯•æŸ¥è¯¢æœªæ‰¾åˆ°ç»“æœ")
        
        return 0
        
    except Exception as e:
        print("\n" + "="*80)
        print("âŒ ERROR / é”™è¯¯")
        print("="*80)
        print(f"\nFailed to build cache: {e}")
        print(f"æ„å»ºç¼“å­˜å¤±è´¥: {e}")
        
        import traceback
        print("\nFull error traceback:")
        print("å®Œæ•´é”™è¯¯è·Ÿè¸ª:")
        traceback.print_exc()
        
        return 1


if __name__ == "__main__":
    exit(main())


